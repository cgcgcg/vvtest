#!/usr/bin/env python

import sys, os
import re, string
import exceptions
import stat
import time
import signal
import shutil
import types
import glob

version = '28'  # version number; used to check format and for information

usagepage = \
'''
  vvtest { -h | -H | --help | -v | --version }
  vvtest [OPTIONS] [directory ...]
  vvtest -i [OPTIONS]
  vvtest -g [kKpP] [directory ...]
  vvtest -b [OPTIONS]
  vvtest --extract <to_directory> [OPTIONS] [source_directory ...]

Use -H or --help to get the full man page.
'''

manpage = \
'''
NAME
      vvtest - generate and/or run a set of execution tests
      
      Version = ''' + version + '''

SYNOPSIS
      1.  vvtest { -h | -H | --help | -v | --version }
      2.  vvtest [OPTIONS] [directory ...]
      3.  vvtest -i [OPTIONS]
      4.  vvtest -g [kKpP] [directory ...]
      5.  vvtest -b [OPTIONS]
      6.  vvtest --extract <to_directory> [OPTIONS] [source_directory ...]

DESCRIPTION
      1. -h gives short usage help, while --help or -H gives long man page.
      Get the version number with -v or --version.

      2. Execute a set of tests.  The list of tests is determined by recursively
      scanning the given directories (or the current working directory if no
      directories are given).  If this is a repeat call and a test results
      directory already exists, then any new tests are merged into the
      existing list.  Also in this case, if no directory is given, a scan of
      the current working directory is not done.
      
      The tests are run in a directory whose name starts with TestResults
      and is appended with the platform (usually uname) and the -o and -O
      options.  Therefore, the -o and -O options must be specified for each
      invocation of vvtest (unless the current working directory is one of
      the TestResults directories).

      If the current working directory is a TestResults directory, then the
      original options are used.
      
      Only those tests are run that show "notrun" or "notdone" and that
      satisfy the filtering options.  However, if the -k or -K options are
      used, they take precedence.  Also, if the -R option is used, all the
      tests are run after being filtered by -kK keyword options.
      
      3. Display information on a previous test run.  The same on/off
      options must be given on the command line when the current working
      directory is not a TestResults directory.  Also, to get the results of
      a platform other than the current platform, the --plat option must be
      used (unless the current working directory is a TestResults directory).
      
      4. Generate a test list for later execution by recursively scanning the
      given directories the same as in number 2.  The test directory will be
      populated with the test scripts but no tests will be launched.

      5. Rebaseline generated files from a previous run by copying them over
      the top of the files in the directory which contain the XML test
      description.  Keywords and parameter options can be given to control
      which tests get rebaselined, including results keywords like "diff".
      If no keyword options are specified, then all tests that show a diff
      status are rebaselined, which would be equivalent to running the
      command "vvtest -b -k diff".
      
      The rebaseline option can be given when the current working directory is
      a TestResults directory with the same behavior.
      
      6. Extract tests from a test tree.  This mode copies the files from the
      test source directory into the 'to_directory' given as the argument to
      the --extract option.  Keyword filtering can be applied, and is often
      used to pull tests that can be distributed to various customers.

OPTIONS
      -o on_options
      -O off_options
            Options are arbitrary and may be used by the execution launch
            script.  Also, the TestResults directory contains the options.
      
      -k keyword
            Filter the set of tests by including those with keyword.  Multiple
            -k keyword options are logically ANDed together.  Keywords
            separated with a "/" are ORed together.  Eg, "-k key1/key2" means
            if key1 or key2 is contained in the test keyword list, then the
            test is included.
            
            Note that implicit results keywords are available:
            
            notrun  : test was in the test list but was not launched
            notdone : test was launched but did not finish
            fail    : test finished and returned a fail status
            diff    : test finished with a diff status
            pass    : test finished with a pass status
            timeout : test finished by running out of time
      
            For example, "vvtest -k diff" would run all tests that
            finished with a diff status.  If no keywords are specified,
            then all tests "notrun" or "notdone" will be run.  If a test times
            out, it will receive the "fail" and "timeout" implicit keywords.
      
      -K keyword
            Filter the set of tests by excluding those with keyword.  Multiple
            -K keyword options are logically ANDed together.  Keywords
            separated with a "/" are ORed together after applying the exclusion
            operator.  Eg, "-K key1/key2" means "(!key1) OR (!key2)" where
            "!key" means "key" is not in the test keyword list.
            
            Note that if both -k and -K options are given, the expressions are
            evaluated left to right.
      
      --keys
            Collect the tests in an existing TestResults directory if it exists,
            or scan the directories given on the command line.  Then gather all
            the keywords for each of the tests and write them to stdout.  Note
            that keyword filtering is applied before the keywords are gathered.
      
      --files
            Same as --keys except the collection of test file names is written
            instead.
      
      --sort <character(s)>
            When test results are printed to the screen, they are sorted by
            their test name by default.  Use this option to sort by other
            fields:
                    n : test name (the default)
                    x : execution directory name
                    t : test run time
                    d : execution date
                    s : test status (such as pass, fail, diff, etc)
                    r : reverse the order
            If more than one character is given, then ties on the first
            ordering are broken by subsquent orderings.  For example,
            "--sort txr" would sort first by runtime, then by execution
            directory name, and would reverse the order (so largest runtimes
            first, smallest runtimes last).

      -w    Wipe out previous test list and test results, if present.  By
            default, previous tests are only modified or appended to.
      
      -p param=value[/param2=value2...]
            Filter the set of tests by including those whose given parameter
            has the given value.  If a test does not know the parameter, it
            is filtered out.  If a value is not given, eg. "-p np=" or "-p np",
            then any test that does not have the parameter name, eg. "np", is
            filtered out.  Can also use "np<=4", "np<4", "np>4", "np!=4", etc.
            Specifications separated by '/' character are OR'ed together, and
            multiple -p/-P options are AND'ed together.
      
      -P param=value[/param2=value2...]
            Filter the set of tests by excluding those whose given parameter
            has the given value.  If a test does not know the parameter, it
            is not filtered out.  If a value is not given, eg. "-P np=" or
            "-P np", then any test that has the parameter name, eg. "np", is
            filtered out.  Can also use inequalities.  Specifications separated
            by '/' character are OR'ed together, and multiple -p/-P options
            are AND'ed together.
      
      -S param=value
            Set parameter to value.  More than one is accumulated and 'value'
            can be a space separated list.  Example: -S np=2 -S np="5 8".
      
      -x platform[/platform...]
            Include tests that would be included for the given platform name.
            Platform names separated by '/' mean OR.  This can be used to run
            the tests that would run on another platform.  Multiple -x and -X
            are allowed.  So "-x Linux -X SunOS" would run all tests that would
            normally run on Linux but not on SunOS.
      
      -X platform[/platform...]
            Exclude tests that would be included for the given platform name.
            Platform names separated by '/' mean OR.  This can be used to run
            the tests that are NOT run on another platform.
      
      -s <regular expression>, --search <regular expression>
            Search for the regular expression in the input file(s).  The files
            specified for soft link or copy are searched for the given regular
            expression.  May be repeated.  If any regular expressions match
            in any of the input files, then the test is included; otherwise it
            is filtered out.  This search is case insensitive.
      
      -j directory
            Passed along to the test launch scripts for use in finding the
            executables to run.  This defines the PROJECT variable available
            in the test scripts.

      -m    Do not overwrite existing scripts and no clean out.  By default,
            the individual test scripts are always overritten and all files
            in the test directory are removed.  Turning this option on
            will prevent a test script that exists from being overwritten
            and the test directory will not be cleaned out.
      
      --perms <permissions>
            Apply permission settings to files and directories in the test
            execution area.  Multiple --perms options can be used and/or the
            specifications can be comma separated.  If the specifcation starts
            with "g=" then the group permissions are set.  If it starts with
            "o=" then the world permissions are set.  If not one of these,
            then it must be the name of a UNIX group, and the files and
            directories have their group set.  Examples are "o=-", "g=rx,o=",
            "wg-alegra,g=rws,o=---".

      -C, --postclean
            Clean out the test directory of each test after it shows a "pass".
            Only those tests that "pass" are cleaned out after they run.  The
            execute.log file is not removed.
      
      -A    Ignore platform restrictions.  Tests can be excluded by platform
            name in the test specification file.  Using -A causes any such
            exclusions to be ignored.

      -R    Rerun tests.  If a test has been run and completes, then by default
            it will not be run again.  This option causes all such tests to be
            run again.  (This was the -F option, now deprecated.)

      --force
            Force vvtest to run.  When vvtest finishes running tests, a mark
            is placed in the testlist file.  If another vvtest execution is
            started while the first is still running, the second will refuse
            to run because it cannot find the mark.  It is possible that vvtest
            dies badly and the mark is never placed (and never will be).  So
            vvtest can be forced to run in this case by using --force.

      -M directory
            Use the given directory to actually contain the test results.
            The tests will run faster if a local disk is used to write to.
            The special value "any" will search for a scratch disk.

      -n integer
            Set the number of processers to the given value.  By default, an
            attempt is made to determine the full number of processors
            available on a machine and utilize all of them.
      
      --plat platform_name
            Use the given platform name for the plugin and defaults.
            This can be used to specify the platform name, thus overriding
            the default platform name.  For example, you could use "--plat Linux".
      
      --platopt <optname>[=value]
            Platform options.  Some platform plugins understand special
            options.  Use this to send command line options into the platform
            plugin.  For example, use "--platopt ppn=4" to set the number
            of processors per node to an artificial value of 4.
      
      -T timeout
            Apply a timeout value in seconds to each test.  Zero means no
            timeout.  In pipeline/batch mode, note that the time given to
            the batch queue for each batch job is the sum of the timeouts
            of each test.
      
      --timeout-multiplier mult
            Apply a float multiplier to the timeout value for each test.
      
      -L    Do not use log files.  By default, each test output is sent to
            a log file contained in the run directory.  This option turns
            off this redirection.  This can be used to debug a single test
            for example, so that the results of the run are seen immediately.
            Note that "-n 1" can be used with the -L option to send many test
            results to stdout/stderr without interleaving the output (the
            "-n 1" prevents multiple tests from being run simultaneously).
      
      -e    Use environment variable overrides.  By default, all environment
            variables that can influence the execution of the tests are
            cleared.  This switch will allow certain environment variables
            to take precedence over the defaults determined by each platform
            plugin.
      
      -a, --analyze
            Only execute the analysis portions of the tests (and skip running
            the main code again).  This is only useful if the tests have
            already been run previously.
      
      --config <directory>
            Specify the directory containing the platform configuration and
            test helpers.  Vvtest looks for a file called platform_plugin.py
            in that directory.

            An environment variable can be used instead, VVTEST_CONFIGDIR.

      --pipeline
            Collect tests in sequential pipelines and submit them to a batch
            queue rather than executing individual tests as resources become
            open.  In order to use the qsub capability of batch systems, the
            plugin for the platform must be set up to launch batch jobs.
      
      --qsub-limit <integer>
            Used in conjunction with the --pipeline switch, this will limit
            the number of pipelines submitted to the queue at any one time.
            The default is 5 concurrent pipelines.
      
      --qsub-length=<integer>
            Used in conjunction with the --pipeline switch, this limits the
            number of tests that are placed into each pipeline batch.  The
            sum of the timeouts of the tests in each batch will be less than
            the given number of seconds.  The default is 30 minutes.  The
            longer the length, the more tests will go in each batch job; the
            shorter the length, the fewer.  A value of zero will force each
            test to run in a separate batch job.
      
      --check=<name>
            Activates optional sections in the test files.  The execution
            blocks in the test files may have an ifdef="CHECK_NAME" attribute,
            where the NAME portion is just upper case of <name>.  Those blocks
            are not active by default.  Using this option will cause those
            blocks to be executed as part of the test.
      
      --save-results
            If used with the -i option, this saves test results for an
            existing run to the testing directory in a file called
            'results.<date>.<platform>.<compiler>.<tag>' where the "tag" is
            an optional, arbitrary string determined by the --results-tag
            option.  The testing directory is determined by the platform
            plugins, but can be overridden by defining the environment
            variable TESTING_DIRECTORY to the desired absolute path name.

            If used without the -i option, this causes an empty results
            file to be written at the start of the testing sequence, and a
            final results file to be written when the test sequence finishes.
      
      --results-tag=<string>
            When used with --save-results, this adds a string to the results
            file name within the testing directory.
      
      --vg, -vg
            Pass the -vg option to the test launch script.
'''

############################################################################

toolsdir = None
exeName = None

# the globally available option dictionary
optdict = {}

search_fnmatch = ['*.inp','*.apr','*.i']

testlist_name = 'testlist'

# a FilterExpressions.WordExpression() object filled with the command line
# -k and -K specifications
keyword_expr = None

class Configuration:
    
    defaults = { \
                 'toolsdir':None,  # the top level tools directory
                 'configdir':None,  # the configuration directory
                 'exepath':None,  # the path to the executables
                 'onopts':[],
                 'offopts':[],
                 'refresh':1,
                 'postclean':0,
                 'timeout':None,
                 'multiplier':1.0,
                 'preclean':1,
                 'analyze':0,
                 'logfile':1,
               }
    
    def get(self, name):
        """
        """
        return self.attrs[name]
    
    def set(self, name, value):
        """
        """
        if value == None:
          self.attrs[name] = Configuration.defaults[name]
        else:
          self.attrs[name] = value
    
    def __init__(self):
        self.attrs = {}
        for n,v in Configuration.defaults.items():
          self.attrs[n] = v


config = Configuration()


############################################################################

def XstatusString( t, add_date, test_dir, cwd ):
    """
    Returns a formatted string containing the job and its status.  If the
    add_date argument is true, it adds the date the job finished to the
    returned string.
    """
    
    if isinstance(t, TestExec.TestExec):
      ref = t.atest
      s = "%-20s " % ref.getName()
    else:
      ref = t
      s = "%-20s " % t.getName()
    
    state = ref.getAttr('state')
    if state != "notrun":
      
      if state == "done":
        result = ref.getAttr('result')
        if result == 'diff':
          s = s + "%-7s %-8s" % ("Exit", "diff")
        elif result ==  'pass':
          s = s + "%-7s %-8s" % ("Exit", "pass")
        elif result == "timeout":
          s = s + "%-7s %-8s" % ("TimeOut", 'SIGINT')
        else:
          s = s + "%-7s %-8s" % ("Exit", "fail(1)")
        
        xtime = ref.getAttr('xtime')
        if xtime >= 0: s = s + (" %-4s" % (str(xtime)+'s'))
        else:          s = s + "     "
      else:
        s = s + "%-7s %-8s     " % ("Running", "")
    
    else:
      s = s + "%-7s %-8s     " % ("NotRun", "")
    
    if add_date:
      xdate = ref.getAttr('xdate')
      if xdate > 0:
        s = s + time.strftime( " %m/%d %H:%M:%S", time.localtime(xdate) )
      else:
        s = s + "               "
    
    s += ' ' + relative_execute_directory( ref, test_dir, cwd )
    
    return s


def XstatusResult(t):
    
    if isinstance(t, TestExec.TestExec): ref = t.atest
    else:                        ref = t
    
    state = ref.getAttr('state')
    if state == "notrun" or state == "notdone":
      return state
    
    return ref.getAttr('result')


def relative_execute_directory( tst, testdir, cwd ):
    """
    Returns the test execute directory relative to the given current working
    directory.
    """
    xdir = tst.getExecuteDirectory()

    if testdir == None:
      return xdir
    
    d = os.path.join( testdir, xdir )
    sdir = issubdir( cwd, d )
    if sdir == None or sdir == "":
        return os.path.basename( xdir )
    
    return sdir


##############################################################################
#
# generation of tests
 
def generateTestList( dirs, plat, ufilter ):
    """
    """
    # default scan directory is the current working directory
    if len(dirs) == 0: dirs = ['.']
    
    tlist = TestList.TestList( ufilter )
    for d in dirs:
      if not os.path.exists(d):
        sys.stderr.write('*** ' + exeName + \
            ': error: directory does not exist: ' + d + '\n')
        sys.exit(1);
      tlist.scanDirectory( d, optdict.get('-S',None) )
    
    tlist.loadTests()
    
    test_dir = None
    cwd = os.getcwd()
    testdirname = testResultsDir( plat.getName() )
    test_dir = os.path.join( cwd, testdirname )

    perms = None
    if '--perms' in optdict:
        perms = PermissionSetter( test_dir, optdict['--perms'] )
    
    createTestDir( testdirname, perms )
    writeCommandInfo( test_dir, plat, perms )
    printResults( tlist, False, test_dir, False, perms )
    tlist.createTestExecs( test_dir, plat, config, perms )

    tf = os.path.join( test_dir, testlist_name )
    tlist.stringFileWrite( tf )
    tlist.writeFinished()

    if perms != None:
        perms.set( os.path.abspath( tf ) )
    
    print "Test directory:", testdirname


def extractTestFiles( dirs, plat, target_dir, ufilter ):
    """
    Uses all the regular filtering mechanisms to gather tests from a test
    source area and copies the files used for each test into a separate
    directory.
    """
    # default scan directory is the current working directory
    if len(dirs) == 0: dirs = ['.']
    
    tlist = TestList.TestList( ufilter )
    for d in dirs:
      if not os.path.exists(d):
        sys.stderr.write('*** ' + exeName + \
            ': error: directory does not exist: ' + d + '\n')
        sys.exit(1);
      tlist.scanDirectory( d, optdict.get('-S',None) )
    
    tlist.loadTests()
    
    if not os.path.isabs(target_dir):
      target_dir = os.path.abspath(target_dir)
    if not os.path.exists(target_dir):
      os.makedirs( target_dir )
    
    uniqD = {}
    
    def wvisit( arg, dname, files ):
        """
        copy a directory tree, but leave out version control files
        """
        for n in ['CVS','.cvsignore','.svn','.git','.gitignore']:
          while (n in files): files.remove(n)
        fd = os.path.normpath( os.path.join( arg[0], dname ) )
        td = os.path.normpath( os.path.join( arg[1], dname ) )
        if not os.path.exists(td):
          os.makedirs(td)
        for f1 in files:
          f2 = os.path.join(fd,f1)
          if not os.path.isdir(f2):
            tf = os.path.join(td,f1)
            shutil.copy2( f2, tf )
    
    for xdir,t in tlist.active.items():
      
      tname = t.getName()
      T = (tname, t.getFilename())
      
      from_dir = os.path.dirname( t.getFilename() )
      p = os.path.dirname( t.getFilepath() )
      if p: to_dir = os.path.normpath( os.path.join( target_dir, p ) )
      else: to_dir = target_dir
      
      if not os.path.exists( to_dir ):
        os.makedirs( to_dir )
      tof = os.path.join( target_dir, t.getFilepath() )
      
      if not uniqD.has_key(tof):
        uniqD[tof] = None
        try: shutil.copy2( t.getFilename(), tof )
        except IOError, e: pass
      
      for srcf in t.getSourceFiles():

        if os.path.exists( os.path.join( from_dir, srcf ) ):
            fL = [ srcf ]
        else:
            cwd = os.getcwd()
            try:
                os.chdir( from_dir )
                fL = glob.glob( srcf )
            except:
                fL = []
            os.chdir( cwd )

        for f in fL:
            fromf = os.path.join( from_dir, f )
            tof = os.path.join( to_dir, f )
            tod = os.path.dirname(tof)
            if not uniqD.has_key(tof):
              uniqD[tof] = None
              if not os.path.exists(tod):
                os.makedirs(tod)
              
              if os.path.isdir(fromf):
                cwd = os.getcwd()
                os.chdir(fromf)
                os.path.walk( '.', wvisit, (fromf, tof) )
                os.chdir(cwd)
                
              else:
                try: shutil.copy2( fromf, tof )
                except IOError, e: pass


##############################################################################

def keywordInformation(tlist):
    
    if optdict.has_key('--keys'):
      print "\nresults keywords: " + string.join( TestSpec.results_keywords )
      kd = {}
      for t in tlist.getActiveTests():
        for k in t.getKeywords():
          if k not in TestSpec.results_keywords and k != t.getName():
            kd[k] = None
      L = kd.keys()
      L.sort()
      print "\ntest keywords: "
      while len(L) > 0:
        k1 = L.pop(0)
        if len(L) > 0: k2 = L.pop(0)
        else:          k2 = ''
        if len(L) > 0: k3 = L.pop(0)
        else:          k3 = ''
        print "  %-20s %-20s %-20s" % (k1,k2,k3)
    else:
      assert optdict.has_key('--files')
      D = {}
      for t in tlist.getActiveTests():
        d = os.path.normpath( t.getFilename() )
        D[d] = None
      L = D.keys()
      L.sort()
      for d in L:
        print d

##############################################################################


def testResultsDir( platform_name ):
    """
    Generates and returns the directory name to hold test results, which is
    unique up to the platform and on/off options.
    """
    testdirname = 'TestResults.' + platform_name
    if '-o' in optdict:
      testdirname += '.ON=' + '_'.join( optdict['-o'] )
    if '-O' in optdict:
      testdirname += '.OFF=' + '_'.join( optdict['-O'] )
    
    return testdirname


def createTestDir( testdirname, perms ):
    """
    Create the given directory name.  If -M is given in the command line
    options, then a mirror directory is created and 'testdirname' will be
    created as a soft link pointing to the mirror directory.
    """
    if '-M' in optdict and \
            makeMirrorDirectory( optdict['-M'], testdirname, perms ):
        pass
    
    else:
        if os.path.exists( testdirname ):
            if not os.path.isdir( testdirname ):
                # replace regular file with a directory
                os.remove( testdirname )
                os.mkdir( testdirname )
        else:
            if os.path.islink( testdirname ):
                os.remove( testdirname )  # remove broken softlink
            os.mkdir( testdirname )

        if perms:
            perms.set( os.path.abspath( testdirname ) )


def makeMirrorDirectory( Mval, testdirname, perms ):
    """
    Create a directory in another location then soft link 'testdirname' to it.
    Returns False only if 'Mval' is the word "any" and a suitable scratch
    directory could not be found.
    """
    assert testdirname == os.path.basename( testdirname )

    if Mval == 'any':
      
        usr = getUserName()
        for d in ['/var/scratch', '/scratch', '/var/scratch1', '/scratch1', \
                  '/var/scratch2', '/scratch2', '/var/scrl1', '/gpfs1']:
            if os.path.exists(d) and os.path.isdir(d):
                ud = os.path.join( d, usr )
                if os.path.exists(ud):
                    if os.path.isdir(ud) and \
                       os.access( ud, os.X_OK ) and os.access( ud, os.W_OK ):
                        Mval = ud
                        break
                elif os.access( d, os.X_OK ) and os.access( d, os.W_OK ):
                    try:
                        os.mkdir(ud)
                    except:
                        pass
                    else:
                        Mval = ud
                        break
        
        if Mval == 'any':
            return False  # a scratch dir could not be found
        
        # include the current directory name in the mirror location
        curdir = os.path.basename( os.getcwd() )
        Mval = os.path.join( Mval, curdir )

        if not os.path.exists( Mval ):
            os.mkdir( Mval )

    else:
        Mval = os.path.abspath( Mval )
    
    if not os.path.exists( Mval ) or not os.path.isdir( Mval ) or \
       not os.access( Mval, os.X_OK ) or not os.access( Mval, os.W_OK ):
        raise Exception( "invalid or non-existent mirror directory: "+Mval )

    if os.path.samefile( Mval, os.getcwd() ):
        raise Exception( "mirror directory and current working directory " + \
                "are the same: "+Mval+' == '+os.getcwd() )

    mirdir = os.path.join( Mval, testdirname )

    if os.path.exists( mirdir ):
        if not os.path.isdir( mirdir ):
            # replace regular file with a directory
            os.remove( mirdir )
            os.mkdir( mirdir )
    else:
        if os.path.islink( mirdir ):
            os.remove( mirdir )  # remove broken softlink
        os.mkdir( mirdir )
    
    if perms:
        perms.set( os.path.abspath( mirdir ) )

    if os.path.islink( testdirname ):
        path = os.readlink( testdirname )
        if path != mirdir:
            os.remove( testdirname )
            os.symlink( mirdir, testdirname )

    else:
        if os.path.exists( testdirname ):
            if os.path.isdir( testdirname ):
                shutil.rmtree( testdirname )
            else:
                os.remove( testdirname )
        os.symlink( mirdir, testdirname )
    
    return True


def writeCommandInfo( test_dir, plat, perms ):
    """
    Creates the test results information file.
    """
    f = os.path.join(test_dir, 'test.cache')
    if not os.path.exists( f ):
      fp = open( f, "w" )
      fp.write( 'VERSION=' + str(version) + '\n' )
      fp.write( 'DIR=' + os.getcwd() + '\n' )
      if optdict.has_key('--plat'):
        fp.write( 'PLATFORM=' + string.strip(optdict['--plat']) + '\n' )
      else:
        fp.write( 'PLATFORM=' + plat.getName() + '\n' )
      # only write the non-results expression to the info file
      expr = keyword_expr.getNonResultsExpression()
      if expr and expr.strip():
        fp.write( 'KEYWORDS=' + expr + '\n' )
      if optdict.has_key('-p'):
        fp.write( 'PARAMETERS=' + str( optdict['-p'] ) + '\n' )
      if config.get('exepath'):
        fp.write( \
            'PROJECT=' + os.path.abspath( config.get('exepath') ) + '\n' )
      if optdict.has_key('-o'):
        fp.write( 'ONOPTS=' + string.join(optdict['-o'], '+') + '\n' )
      if optdict.has_key('-O'):
        fp.write( 'OFFOPTS=' + string.join(optdict['-O'], '+') + '\n' )
      if optdict.has_key('-T'):
        fp.write( 'TIMEOUT=' + string.strip( str(optdict['-T']) ) + '\n' )
      if optdict.has_key('--timeout-multiplier'):
        fp.write( 'TIMEOUT_MULTIPLIER=' + \
             string.strip( str(optdict['--timeout-multiplier']) ) + '\n' )
      if optdict.has_key('-e'):
        fp.write( 'USE_ENV=1\n' )
      if optdict.has_key('-A'):
        fp.write( 'ALL_PLATFORMS=1\n' )
      if optdict.has_key('--check'):
        fp.write( 'CHECK=' + string.join( optdict['--check'] ) + '\n' )
      fp.close()

    if perms != None:
        perms.set( os.path.abspath(f) )


def readCommandInfo():
    """
    Check for a file called 'test.cache' that indicates whether the
    current working directory is a TestResults directory (or subdirectory)
    then open that file for information.  The test results directory is
    returned, or None if not in a TestRestults directory.
    """
    test_dir = None
    
    d = os.getcwd()

    while d != '/':
      test_cache = os.path.join(d,'test.cache')
      if os.path.exists( test_cache ):
        test_dir = d
        break
      d = os.path.dirname(d)
    
    if test_dir != None:
      
      if optdict.has_key('-o') or optdict.has_key('-O') or \
         optdict.has_key('-g'):
        sys.stderr.write('*** ' + exeName + \
            ': error: the -g, -o, and -O options are not allowed ' + \
            'in a TestResults directory\n')
        sys.exit(1);
      
      fp = open( test_cache, "r" )
      write_version = 0
      for line in fp.readlines():
        line = string.strip(line)
        kvpair = string.split(line, '=', 1)
        if kvpair[0] == 'VERSION':
          write_version = int(kvpair[1])
        elif kvpair[0] == 'DIR':
          previous_run_dir = kvpair[1]
        elif kvpair[0] == 'PLATFORM':
          optdict['--plat'] = kvpair[1]
        elif kvpair[0] == 'KEYWORDS':
          if write_version < 27:
            L = string.split(kvpair[1])
            if len(L) > 0:
              keyword_expr.append( L, 'and' )
          else:
            expr = kvpair[1].strip()
            if expr:
              keyword_expr.append( expr, 'and' )
          
        elif kvpair[0] == 'PARAMETERS':
          L = eval( kvpair[1] )
          if optdict.has_key('-p'): optdict['-p'].extend(L)
          else:                     optdict['-p'] = L
        elif kvpair[0] == 'PROJECT':
          # do not replace if the command line contains -j
          if not optdict.has_key('-j'):
            optdict['-j'] = kvpair[1]
            config.set( 'exepath', kvpair[1] )
        elif kvpair[0] == 'ONOPTS':
          optdict['-o'] = string.split( kvpair[1], '+' )
          config.set( 'onopts', optdict['-o'] )
        elif kvpair[0] == 'OFFOPTS':
          optdict['-O'] = string.split( kvpair[1], '+' )
          config.set( 'offopts', optdict['-O'] )
        elif kvpair[0] == 'TIMEOUT':
          # do not replace if the command line contains -T
          if not optdict.has_key('-T'):
            optdict['-T'] = float(kvpair[1])
            config.set( 'timeout', optdict['-T'] )
        elif kvpair[0] == 'TIMEOUT_MULTIPLIER':
          if not optdict.has_key('--timeout-multiplier'):
            optdict['--timeout-multiplier'] = float(kvpair[1])
            config.set( 'multiplier', optdict['--timeout-multiplier'] )
        elif kvpair[0] == 'USE_ENV':
          optdict['-e'] = ''
        elif kvpair[0] == 'ALL_PLATFORMS':
          optdict['-A'] = ''
        elif kvpair[0] == 'CHECK':
          optdict['--check'] = string.split( kvpair[1] )
      fp.close()
    
    return test_dir


def runTests( dirs, plat, ufilter ):
    """
    Executes a list of tests.
    """
    # determine the directory that stores the test results then create it
    testdirname = testResultsDir( plat.getName() )
    test_dir = os.path.join( os.getcwd(), testdirname )
    tfile = os.path.join( test_dir, testlist_name )
    
    if os.path.exists( tfile ):
        tl = TestList.TestList()
        tl.scanFile( tfile )
        if tl.getFinishDate() == None and '--force' not in optdict:
            print '*** error: tests are currently running in another process'
            print '    (or a previous run crashed); use --force to run anyway'
            sys.exit(1)
        tl = None
    
    perms = None
    if '--perms' in optdict:
      perms = PermissionSetter( test_dir, optdict['--perms'] )
    
    createTestDir( testdirname, perms )
    
    if optdict.has_key('-w'):
      print '/bin/rm -rf ' + testdirname + '/*'
      for f in os.listdir(testdirname):
        df = os.path.join( testdirname, f )
        if os.path.isdir(df):
          shutil.rmtree( df, 0 )
        else:
          try: os.remove(df)
          except: pass
    
    writeCommandInfo( test_dir, plat, perms )
    
    tlist = TestList.TestList( ufilter )

    # generate test list by scanning directories
    if len(dirs) == 0:
      dirs = ['.']
    for d in dirs:
      if not os.path.exists(d):
        sys.stderr.write('*** ' + exeName + \
            ': error: directory does not exist: ' + d + '\n')
        sys.exit(1);
      tlist.scanDirectory( d, optdict.get('-S',None) )
    
    if os.path.exists( tfile ):
        tlist.readFile( tfile )  # merge in existing test results
    
    pruneL = tlist.loadTests( analyze_only=optdict.has_key('-a'),
                              prune=True )
    
    if len(pruneL) > 0:
        print3()
        for pt,ct in pruneL:
            print3( '*** Warning: analyze test',
                    relative_execute_directory( pt, test_dir, os.getcwd() ),
                    'will not be run due to child',
                    relative_execute_directory( ct, test_dir, os.getcwd() ) )

    loadExecutionTimes( tlist, plat )
    
    # save the test list in the TestResults directory
    tlist.stringFileWrite( tfile )

    try:
        
        if perms != None:
            perms.set( os.path.abspath( tfile ) )
        
        if len(tlist.active) > 0:
          
          if '--pipeline' not in optdict:
            plat.initProcs( test_dir )

          print3( "Running these tests:" )
          printResults( tlist, True, test_dir, False, perms, True )
          print3()

          tlist.createTestExecs( test_dir, plat, config, perms )
          
          if optdict.has_key('--save-results'):
            tag = optdict.get('--results-tag',None)
            saveResults( tlist, plat, test_dir, tag, inprogress=True )

          if not optdict.has_key('--pipeline'):
            executeTestList( tlist, test_dir, plat, perms, tfile )
          
          else:
            pipeLineTestList( tlist, test_dir, plat, perms )
        
          print3( "Test directory:", testdirname )
          
          if optdict.has_key('--save-results'):
            tag = optdict.get('--results-tag',None)
            saveResults( tlist, plat, test_dir, tag )
        
        else:
          print "\n--------- no tests to run -----------\n"
    
    except:
        tlist.writeFinished()
        raise
    tlist.writeFinished()


def printResults( atestlist, add_date, test_dir, do_html, perms, short=False ):
    """
    Prints a summary to the screen and also creates an HTML summary file.
    If 'short' is True, then only a handfull of tests are written to the
    screen.
    """
    rawlist = atestlist.getActiveTests( optdict.get( '--sort', '' ) )
    
    Lfail = []; Ltime = []; Ldiff = []; Lpass = []; Lnrun = []; Lndone = []
    for atest in rawlist:
        statr = XstatusResult(atest)
        if   statr == "fail":    Lfail.append(atest)
        elif statr == "timeout": Ltime.append(atest)
        elif statr == "diff":    Ldiff.append(atest)
        elif statr == "pass":    Lpass.append(atest)
        elif statr == "notrun":  Lnrun.append(atest)
        elif statr == "notdone": Lndone.append(atest)
    sumstr = str(len(Lpass)) + " pass, " + \
             str(len(Ltime)) + " timeout, " + \
             str(len(Ldiff)) + " diff, " + \
             str(len(Lfail)) + " fail, " + \
             str(len(Lnrun)) + " notrun, " + \
             str(len(Lndone)) + " notdone"
    
    cwd = os.getcwd()
    print3( "==================================================" )
    if short and len(rawlist) > 16:
        for atest in rawlist[:8]:
            print3( XstatusString( atest, add_date, test_dir, cwd ) )
        print3( "..." )
        for atest in rawlist[-8:]:
            print3( XstatusString( atest, add_date, test_dir, cwd ) )
    else:
        for atest in rawlist:
            print3( XstatusString( atest, add_date, test_dir, cwd ) )
    print3( "==================================================" )
    print3( "Summary:", sumstr )
    
    if do_html:
        printHTMLResults( sumstr, test_dir, perms,
                          Lfail, Ltime, Ldiff, Lpass, Lnrun, Lndone )


def printHTMLResults( sumstr, test_dir, perms,
                      Lfail, Ltime, Ldiff, Lpass, Lnrun, Lndone ):
    """
    Opens and writes an HTML summary file in the test directory.
    """
    
    if test_dir == ".":
      test_dir = os.getcwd()
    if not os.path.isabs(test_dir):
      test_dir = os.path.abspath(test_dir)
    
    fp = open("summary.htm","w")
    fp.write( "<html>\n<head>\n<title>Test Results</title>\n" )
    fp.write( "</head>\n<body>\n" )
    
    # a summary section
    
    fp.write( "<h1>Summary</h1>\n" )
    fp.write( "  <ul>\n" )
    fp.write( "  <li> Directory: " + test_dir + "\n" )
    fp.write( "  <li> Options: " )
    if optdict.has_key('-o'):
      fp.write( ' -o ' + string.join(optdict['-o'],'+') )
    if optdict.has_key('-O'):
      fp.write( ' -O ' + string.join(optdict['-O'],'+') )
    if optdict.has_key('-j'):
      fp.write( ' -j ' + optdict['-j'] )
    if optdict.has_key('--plat'):
      fp.write( ' --plat ' + optdict['--plat'] )
    if optdict.has_key('-T'):
      fp.write( ' -T ' + str(optdict['-T']) )
    if optdict.has_key('--timeout-multiplier'):
      fp.write( ' --timeout-multiplier ' + \
                        str(optdict['--timeout-multiplier']) )
    if optdict.has_key('-e'):
      fp.write( ' -e ' )
    fp.write( "  </li>\n" )
    fp.write( "  <li> " + sumstr + "</li>\n" )
    fp.write( "  </ul>\n" )
    
    # segregate the tests into implicit keywords, such as fail and diff
    
    fp.write( '<h1>Tests that showed "fail"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Lfail )
    fp.write( '<h1>Tests that showed "timeout"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Ltime )
    fp.write( '<h1>Tests that showed "diff"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Ldiff )
    fp.write( '<h1>Tests that showed "notdone"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Lndone )
    fp.write( '<h1>Tests that showed "pass"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Lpass )
    fp.write( '<h1>Tests that showed "notrun"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Lnrun )
    
    fp.write( "</body>\n</html>\n" )
    fp.close()

    if perms != None:
        perms.set( os.path.abspath( 'summary.htm' ) )


def writeHTMLTestList( fp, test_dir, tlist ):
    """
    Used by printHTMLResults().  Writes the HTML for a list of tests to the
    HTML summary file.
    """
    cwd = os.getcwd()
    
    fp.write( '  <ul>\n' )
    
    for atest in tlist:
      
      fp.write( '  <li><code>' + XstatusString(atest, 1, test_dir, cwd) + '</code>\n' )
      
      if isinstance(atest, TestExec.TestExec): ref = atest.atest
      else:                            ref = atest
      
      tdir = os.path.join( test_dir, ref.getExecuteDirectory() )
      assert cwd == tdir[:len(cwd)]
      reltdir = tdir[len(cwd)+1:]
      
      fp.write( "<ul>\n" )
      thome = atest.getRootpath()
      xfile = os.path.join( thome, atest.getFilepath() )
      fp.write( '  <li>XML: <a href="file://' + xfile + '" ' + \
                       'type="text/plain">' + xfile + "</a></li>\n" )
      fp.write( '  <li>Parameters:<code>' )
      for (k,v) in atest.getParameters().items():
        fp.write( ' ' + k + '=' + v )
      fp.write( '</code></li>\n' )
      fp.write( '  <li>Keywords: <code>' + string.join(atest.getKeywords()) + \
                 ' ' + string.join( atest.getResultsKeywords() ) + \
                 '</code></li>\n' )
      fp.write( '  <li>Status: <code>' + XstatusString(atest, 1, test_dir, cwd) + \
                 '</code></li>\n' )
      fp.write( '  <li> Files:' )
      if os.path.exists(reltdir):
        for f in os.listdir(reltdir):
          fp.write( ' <a href="file:' + os.path.join(reltdir,f) + \
                    '" type="text/plain">' + f + '</a>' )
      fp.write( '</li>\n' )
      fp.write( "</ul>\n" )
      fp.write( "</li>\n" )
    fp.write( '  </ul>\n' )


def executeTestList( tlist, test_dir, plat, perms, tfile ):
    """
    """
    plat.display()
    starttime = time.time()
    print3( "Start time:", time.ctime() )
    
    # execute tests

    if perms != None:
        perms.set( os.path.abspath( tfile ) )

    cwd = os.getcwd()

    while True:

        tnext = tlist.popNext( plat )

        if tnext != None:
            print3( 'Starting:',
                    relative_execute_directory( tnext.atest, test_dir, cwd ) )
            tnext.start()
            tlist.AppendTestResult( tnext.atest )
        
        elif tlist.numRunning() == 0:
            break

        else:
            time.sleep(1)

        showprogress = False
        for tx in tlist.getRunning():
            if tx.poll():
                xs = XstatusString( tx, False, test_dir, cwd )
                print3( "Finished:", xs )
                tlist.testDone( tx )
                showprogress = True
      
        unit_test_hook( 'tests', tlist.numRunning(), tlist.numDone() )

        if showprogress:
            ndone = tlist.numDone()
            ntot = tlist.numActive()
            pct = 100 * float(ndone) / float(ntot)
            div = str(ndone)+'/'+str(ntot)
            dt = pretty_time( time.time() - starttime )
            print3( "Progress: " + div+" = %%%.1f"%pct + ', time = '+dt )
    
    # any remaining tests cannot run, so print warnings
    tL = tlist.popRemaining()
    if len(tL) > 0:
        print3()
    for tx in tL:
        deptx = tx.badChild()
        assert tx.hasChildren() and deptx != None
        print3( '*** Warning: analyze test skipped for "' + \
                tx.atest.getExecuteDirectory() + \
                '" due to child "' + deptx.atest.getExecuteDirectory() + '"' )
    
    print3()
    do_html = ( '--qsub-id' not in optdict )
    printResults( tlist, True, test_dir, do_html, perms )
    
    elapsed = pretty_time( time.time() - starttime )
    print3( "\nFinish date:", time.ctime() + " (elapsed time "+elapsed+")" )


def pipeLineTestList( tlist, test_dir, plat, perms ):
    """
    The 'tlist' is a TestList class instance.
    """
    assert '--qsub-id' not in optdict

    qsublimit = optdict.get( '--qsub-limit', plat.getDefaultQsubLimit() )
    
    pipes = Pipelines( plat, test_dir, tlist, perms, qsublimit )
    
    plat.display()
    starttime = time.time()
    print3( "Start time:", time.ctime() )
    
    # clean up old pipeline files
    print3( "rm -rf batchset*" )
    for f in os.listdir( test_dir ):
        if f.startswith( 'batchset' ):
            shutil.rmtree( os.path.join( test_dir, f ) )
    
    # write testlist files for each qsub
    numjobs = pipes.writeQsubScripts()

    print3( 'Total number of batch jobs: ' + str(numjobs) + \
            ', maximum concurrent jobs: ' + str(qsublimit) )

    if optdict.has_key('-g'):
      return
    
    cwd = os.getcwd()
    qsleep = int( os.environ.get( 'VVTEST_BATCH_SLEEP_LENGTH', 15 ) )
    
    while True:

        qid = pipes.checkstart()
        if qid != None:
            # nothing to print here because the qsubmit prints
            pass
        elif pipes.numRunning() == 0:
            break
        else:
            time.sleep( qsleep )

        qidL,doneL = pipes.checkdone()
        
        if len(qidL) > 0:
            ids = ' '.join( [ str(qid) for qid in qidL ] )
            print3( 'Finished batch IDS:', ids )
        for t in doneL:
            ts = XstatusString( t, False, test_dir, cwd )
            print3( "Finished:", ts )
        
        unit_test_hook( 'batch', pipes.numRunning(), pipes.numDone() )

        if len(doneL) > 0:
            jpct = 100 * float(pipes.numDone()) / float(numjobs)
            jdiv = 'jobs '+str(pipes.numDone())+'/'+str(numjobs)
            jflt = '(in flight '+str(pipes.numStarted())+')'
            ndone = pipes.tlist.numDone()
            ntot = pipes.tlist.numActive()
            tpct = 100 * float(ndone) / float(ntot)
            tdiv = 'tests '+str(ndone)+'/'+str(ntot)
            dt = pretty_time( time.time() - starttime )
            print3( "Progress: " + \
                    jdiv+" = %%%.1f"%jpct + ' '+jflt+', ' + \
                    tdiv+" = %%%.1f"%tpct + ', ' + \
                    'time = '+dt )

    # any remaining tests cannot be run; flush then print warnings
    NS, NF, nrL = pipes.flush()
    if len(NS)+len(NF)+len(nrL) > 0:
        print3()
    if len(NS) > 0:
      print3( "*** Warning: these pipe numbers did not seem to start:",
              ' '.join(NS) )
    if len(NF) > 0:
      print3( "*** Warning: these pipe numbers did not seem to finish:",
              ' '.join(NF) )
    for tx,deptx in nrL:
        assert tx.hasChildren() and deptx != None
        print3( '*** Warning: analyze test skipped for "' + \
                tx.atest.getExecuteDirectory() + \
                '" due to child "' + deptx.atest.getExecuteDirectory() + '"' )
    
    print3()
    printResults( tlist, True, test_dir, True, perms )
    
    elapsed = pretty_time( time.time() - starttime )
    print3( "\nFinish date:", time.ctime() + " (elapsed time "+elapsed+")" )


def pretty_time( nseconds ):
    """
    Returns a string with the given number of seconds written in an easily
    readable form.
    """
    h = int( nseconds / 3600 )
    sh = str(h)+'h'

    m = int( ( nseconds - 3600*h ) / 60 )
    sm = str(m)+'m'

    s = int( ( nseconds - 3600*h - 60*m ) )
    if h == 0 and m == 0 and s == 0: s = 1
    ss = str(s) + 's'

    if h > 0: return sh+' '+sm+' '+ss
    if m > 0: return sm+' '+ss
    return ss


def unit_test_hook( loop, numrun, numdone ):
    """
    The VVTEST_INTERRUPT_COUNT and VVTEST_INTERRUPT_BATCH environment variables
    can be used to interrupt the execution sequence for unit testing.
    """
    if loop == 'tests':
        snum = os.environ.get( 'VVTEST_INTERRUPT_COUNT', '' )
        if snum:
            sqid = os.environ.get( 'VVTEST_INTERRUPT_QID', '' )
            qok = True
            if sqid and '--qsub-id' in optdict:
                assert type( optdict['--qsub-id'] ) == type('')
                # if the QID is defined and equal to this batch job's queue id
                # then the interrupt will take place; if it is defined but
                # not equal to this job's queue id then no interrupt will occur
                if sqid != optdict['--qsub-id']:
                    qok = False
            if qok and numdone >= int(snum):
                sig = os.environ.get( 'VVTEST_SIGNAL', 'SIGINT' )
                sig = eval( 'signal.'+sig )
                os.kill( os.getpid(), sig )
                time.sleep(60)
                sys.exit(1)
    elif loop == 'batch':
        snum = os.environ.get( 'VVTEST_INTERRUPT_BATCH', '' )
        if snum:
            if numdone >= int(snum):
                sig = os.environ.get( 'VVTEST_SIGNAL_BATCH', 'SIGINT' )
                sig = eval( 'signal.'+sig )
                os.kill( os.getpid(), sig )
                time.sleep(60)
                sys.exit(1)


def loadExecutionTimes( tlist, plat ):
    """
    For each test, a 'runtimes' file will be read (if it exists) and the
    run time for this platform extracted.  This run time is saved in the
    test as the 'runtime' attribute.  Also, a timeout is calculated for
    each test and placed in the 'timeout' attribute.
    """
    pname = plat.getName()
    cplr = plat.getCompiler()
    
    cache = results.LookupCache( plat.testingDirectory() )
    
    for t in tlist.getActiveTests():
      
        # grab explicit timeout value, if the test specifies it
        tout = t.getTimeout()
        
        # look for a previous runtime value
        tlen,tresult = results.get_execution_time( t, pname, cplr, cache )
        
        if tlen != None:
            
            if tout == None:
                if tresult == "timeout":
                    # for tests that timed out, make timeout much larger
                    if t.hasKeyword( "long" ):
                        # only long tests get timeouts longer than an hour
                        if tlen < 60*60:
                            tout = 4*60*60
                        elif tlen < 5*24*60*60:  # even longs are capped
                            tout = 4*tlen
                    else:
                        tout = 60*60
            
                else:
                    # pick timeout to allow for some runtime variability
                    if tlen < 120:
                        tout = max( 120, 2*tlen )
                    elif tlen < 300:
                        tout = max( 300, 1.5*tlen )
                    elif tlen < 4*60*60:
                        tout = int( float(tlen)*1.5 )
                    else:
                        tout = int( float(tlen)*1.3 )
      
        else:  # no previous result

            if tout != None:
                # use the explicit timeout value as the runtime
                tlen = tout
            else:
                # with no information, the default depends on 'long' keyword
                if t.hasKeyword("long"):
                    tlen = 5*60*60  # five hours
                else:
                    tlen = 60*60  # one hour
                tout = tlen
      
        t.setAttr( 'runtime', int(tlen) )
        
        tout = int( optdict.get( '-T', tout ) )
        tout = int( float(tout) * optdict.get('--timeout-multiplier',1.0) )
        t.setAttr( 'timeout', tout )
    
    cache = None


class Pipelines:
    
    def __init__(self, plat, test_dir, tlist, perms, maxjobs):
        """
        The 'tlist' is a TestList class instance.
        """
        self.plat = plat
        self.test_dir = test_dir
        self.tlist = tlist
        self.perms = perms
        # TODO: make Tzero a platform plugin thing
        self.Tzero = 21*60*60  # no timeout in pipeline mode is 21 hours
        
        self.createTestGroups()

        self.maxjobs = maxjobs

        # queue jobs to be submitted, qid -> BatchJob
        self.qtodo = {}
        # queue jobs submitted, qid -> BatchJob
        self.qstart = {}
        # queue jobs submitted then have left the queue, qid -> BatchJob
        self.qstop  = {}
        # queue jobs whose final results have been read, qid -> BatchJob
        self.qread  = {}

        # allow these values to be set by environment variable, mainly for
        # unit testing; if setting these is needed more regularly then a
        # command line option should be added
        val = int( os.environ.get( 'VVTEST_BATCH_READ_DELAY', 30 ) )
        self.read_delay = val
        val = int( os.environ.get( 'VVTEST_BATCH_READ_DELAY_MAX', 5*60 ) )
        self.read_max_delay = val
    
    def createTestGroups(self):
        """
        """
        qlen = optdict.get( '--qsub-length', 30*60 )
        qL = []
        for np in self.tlist.getTestExecProcList():
          xL = []
          for tx in self.tlist.getTestExecList(np):
            xL.append( (tx.atest.getAttr('timeout'),tx) )
          xL.sort()
          grpL = []
          tsum = 0
          for rt,tx in xL:
            if tx.hasChildren() or tx.atest.getAttr('timeout') < 1:
              # analyze tests and those with no timeout get their own group
              qL.append( [ self.Tzero, len(qL), [tx] ] )
            else:
              if len(grpL) > 0 and tsum + rt > qlen:
                qL.append( [ tsum, len(qL), grpL ] )
                grpL = []
                tsum = 0
              grpL.append( tx )
              tsum += rt
          if len(grpL) > 0:
            qL.append( [ tsum, len(qL), grpL ] )
        
        qL.sort()
        qL.reverse()
        self.qsublists = map( lambda L: L[2], qL )
    
    def filename(self, basename, pipeid, relative=False):
        """
        Given a base file name and a pipe/batch id, this function returns
        the file name in the batchset subdirectory and with the id appended.
        If 'relative' is true, then the path is relative to the TestResults
        directory.
        """
        subd = batchset( pipeid )
        if relative:
            fn = os.path.join( subd, basename+'.'+str(pipeid) )
        else:
            fn = os.path.join( self.test_dir, subd, basename+'.'+str(pipeid) )
        return fn

    def writeQsubScripts(self):
        """
        """
        for i in range(len(self.qsublists)):
          f = self.filename( testlist_name, i )
          try:
              os.unlink( f )
          except:
              pass
        
        commonopts = ''
        if '-e' in optdict: commonopts += ' -e'
        if '-m' in optdict: commonopts += ' -m'
        if '-C' in optdict: commonopts += ' -C'
        for s in optdict.get('-p',[]):
            s = s.replace( '!', '\\!' )  # have to escape exclamation points
            commonopts += ' -p "'+s+'"'
        if '--perms' in optdict:
            val = ','.join( optdict['--perms'] )
            commonopts += ' --perms '+val
        if config.get('configdir'):
            commonopts += ' --config='+config.get('configdir')
        for k,v in optdict.get('--platopt',{}).items():
            commonopts += ' --platopt ' + k + '=' + v
        
        qsubids = {}  # maps qpipe id to max number of processors for that pipe
        
        qid = 0
        for qL in self.qsublists:
          self.do_queue_pipe( qid, qL, qsubids, commonopts )
          qid += 1
        
        L = qsubids.keys()
        L.sort()
        for i in L:
          incl = self.filename( testlist_name, i, relative=True )
          self.tlist.AddIncludeFile( incl )
        
        if self.perms != None:
            for f in os.listdir( self.test_dir ):
                if f.startswith( 'batchset' ):
                    p = os.path.join( self.test_dir, f )
                    self.perms.recurse( p )

        return len( qsubids )
    
    def do_queue_pipe(self, qnumber, qlist, npD, comopts):
        """
        """
        qidstr = str(qnumber)
        
        tl = TestList.TestList()
        tL = []
        maxnp = 0
        qtime = 0
        for tx in qlist:
          np = int( tx.atest.getParameters().get('np', 0) )
          if np <= 0: np = 1
          maxnp = max( maxnp, np )
          tl.addTest(tx.atest)
          tL.append( tx )
          qtime += int( tx.atest.getAttr('timeout') )
        
        if qtime == 0:
          qtime = self.Tzero  # give it the "no timeout" length of time
        else:
          # allow more time in the queue than calculated
          if qtime < 60:
            qtime = 120
          elif qtime < 600:
            qtime *= 2
          else:
            qtime = int( float(qtime) * 1.3 )
        
        npD[qnumber] = maxnp
        pout = self.filename( 'pipe-out', qnumber )
        tout = self.filename( 'testlist', qnumber )
        self.qtodo[ qnumber ] = BatchJob( qnumber, maxnp, pout, tout, tL )
        
        fn = self.filename( testlist_name, qidstr )
        tl.stringFileWrite( fn )
        tl.writeFinished()
        
        fn = self.filename( 'pipe', qidstr )
        fp = open( fn, "w" )
        
        hdr = self.plat.getQsubScriptHeader( maxnp, qtime, self.test_dir, pout )
        fp.writelines( [ hdr + '\n\n',
                         'cd ' + self.test_dir + ' || exit 1\n',
                         'echo "job start time = `date`"\n' + \
                         'echo "job time limit = ' + str(qtime) + '"\n' ] )
        
        # set the environment variables from the platform into the script
        for k,v in self.plat.getEnvironment().items():
          fp.write( 'setenv ' + k + ' "' + v  + '"\n' )
        
        # collect relevant options to pass to the qsub vvtest invocation
        taopts = '--qsub-id=' + qidstr + ' '
        taopts += comopts
        if len(qlist) == 1:
          # force a timeout for pipes with only one test
          if qtime < 600: taopts += ' -T ' + str(qtime*0.90)
          else:           taopts += ' -T ' + str(qtime-120)
        
        cmd = toolsdir+'/vvtest ' + taopts + ' || exit 1'
        fp.writelines( [ cmd+'\n\n' ] )
        
        # put in an echo to help debug when jobs timeout
        fp.writelines( [ 'echo "queue job finished cleanly"\n' ] )
        
        fp.close()

    def checkstart(self):
        """
        Launches a new batch job if possible.  If it does, the batch id is
        returned.
        """
        if len( self.qstart ) < self.maxjobs:
            for qid,bjob in self.qtodo.items():
                if self._check_deps( bjob ) == None:
                    pin = self.filename( 'pipe', qid )
                    jobid = self.plat.Qsubmit( self.test_dir, bjob.outfile, pin )
                    self._start( qid, jobid )
                    return qid
        return None

    def numRunning(self):
        """
        Returns the number of batch jobs are still running or stopped but
        whose results have not been read yet.
        """
        return len( self.qstart ) + len( self.qstop )

    def numStarted(self):
        """
        Number of batch jobs currently running (those that have been started
        and still appear to be in the batch queue).
        """
        return len( self.qstart )

    def numDone(self):
        """
        Number of batch jobs that ran and completed.
        """
        return len( self.qread )

    def checkdone(self):
        """
        Uses the platform to find batch jobs that ran but are now no longer
        in the batch queue.  These jobs are moved from the started list to
        the stopped list.

        Then the jobs in the "stopped" list are visited and their test
        results are read.  When a job is successfully read, the job is moved
        from the "stopped" list to the "read" list.

        Returns a list of job ids that were removed from the batch queue,
        and a list of tests that were successfully read in.
        """
        qdoneL = []
        jobidL = [ bjob.jobid for bjob in self.qstart.values() ]
        if len(jobidL) > 0:
            statusD = self.plat.Qquery( jobidL )
            tnow = time.time()
            for qid,bjob in self.qstart.items():
                if not statusD[ bjob.jobid ]:
                    # empty status means its not in the queue, but double
                    # check that either the out file exists or enough time
                    # has elapsed since the submit occurred
                    elapsed = tnow - bjob.tstart
                    if elapsed > 30 or os.path.exists( bjob.outfile ):
                        self._stop( qid )
                        qdoneL.append( qid )
                    if self.perms != None and os.path.exists( bjob.outfile ):
                        self.perms.set( bjob.outfile )

        tnow = time.time()
        tdoneL = []
        for qid,bjob in self.qstop.items():
            if bjob.tcheck < tnow:
                # try to read the results
                clean = False
                fin = False
                # first check the batch output file
                try:
                    sz = os.path.getsize( bjob.outfile )
                    off = max(sz-512, 0)
                    fp = open( bjob.outfile, 'r' )
                except:
                    pass
                else:
                    try:
                        fp.seek(off)
                        buf = fp.read(512)
                    except:
                        pass
                    else:
                        if buf.find("queue job finished cleanly") >= 0:
                            clean = True
                    fp.close()
                if clean:
                    # then check the testlist file for the "finished" mark
                    tl = TestList.TestList()
                    try:
                        tl.scanFile( bjob.testfile )
                        if tl.getFinishDate() != None:
                            fin = True
                    except:
                        pass
                    tl = None
                if fin:
                    # read worked, so load the results into the TestList
                    tdoneL.extend( self._finish( qid, bjob, 'clean' ) )
                else:
                    # read failed
                    if tnow < bjob.tstop+self.read_max_delay:
                        # set the time for the next read
                        bjob.tcheck = tnow + self.read_delay
                    else:
                        # too many attempts to read; assume the queue job
                        # failed somehow, but force a read anyway
                        tdoneL.extend( self._finish( qid, bjob ) )

        return qdoneL, tdoneL

    def flush(self):
        """
        Remove any remaining jobs from the "todo" list, add them to the "read"
        list, but mark them as not run.

        Returns a triple
            - a list of batch ids that were not run
            - a list of batch ids that did not finish
            - a list of the tests that did not run, each of which is a
              pair (a test, failed dependency test)
        """
        # should not be here if there are jobs currently running
        assert len( self.qstart ) == 0 and len( self.qstop ) == 0

        # force remove the rest of the jobs that were not run and gather
        # the list of tests that were not run
        notrunL = []
        for qid,bjob in self.qtodo.items():
            tx1 = self._check_deps( bjob )
            assert tx1 != None  # otherwise checkstart() should have ran it
            for tx0 in bjob.testL:
                notrunL.append( (tx0,tx1) )
            self.qtodo.pop( qid )
            self.qread[ qid ] = bjob
            bjob.finished( 'notrun' )
        
        # TODO: rather than only reporting the jobs left on qtodo as not run,
        #       loop on all jobs in qread and look for 'notrun' mark

        notrun = []
        notdone = []
        for qid,bjob in self.qread.items():
            if bjob.result == 'notrun': notrun.append( str(qid) )
            elif bjob.result == 'notdone': notdone.append( str(qid) )

        return notrun, notdone, notrunL
    
    def _start(self, qid, jobid):
        """
        """
        bjob = self.qtodo.pop( qid )
        bjob.start( jobid )
        self.qstart[ qid ] = bjob

    def _stop(self, qid):
        """
        """
        bjob = self.qstart.pop( qid )
        bjob.stop( self.read_delay )
        self.qstop[ qid ] = bjob

    def _check_deps(self, bjob):
        """
        """
        for tx in bjob.testL:
            if tx.hasChildren():
                for childtx in tx.getChildren():
                    chxdir = childtx.atest.getExecuteDirectory()
                    if chxdir not in self.tlist.stopped:
                        return childtx
                childtx = tx.badChild()
                if childtx != None:
                    return childtx
        return None
    
    def _finish(self, qid, bjob, mark=None):
        """
        """
        tL = []

        if not os.path.exists( bjob.outfile ):
            bjob.finished( 'notrun' )
        
        elif os.path.exists( bjob.testfile ):
            if mark == None:
                bjob.finished( 'notdone' )
            else:
                bjob.finished( mark )
            counts = {}
            self.tlist.readFile( bjob.testfile, counts )
            # only add tests to the stopped list that have a count
            # of three (initial, launched, & finished)
            for tx in bjob.testL:
                xdir = tx.atest.getExecuteDirectory()
                # TODO: could mark the tests that were not run somehow
                if counts.get( xdir, 0 ) == 3:
                    tL.append( tx.atest )
                    self.tlist.stopped[ xdir ] = tx
        
        else:
            bjob.finished( 'fail' )
        
        self.qstop.pop( qid )
        self.qread[ qid ] = bjob

        return tL


class BatchJob:
    
    def __init__(self, qid, maxnp, fout, flist, testL):
        self.qid = qid
        self.maxnp = maxnp
        self.outfile = fout
        self.testfile = flist
        self.testL = testL  # list of TestExec objects
        self.jobid = None
        self.tstart = None
        self.tstop = None
        self.tcheck = None
        self.result = None

    def start(self, jobid):
        """
        """
        self.jobid = jobid
        self.tstart = time.time()

    def stop(self, delay):
        """
        """
        self.tstop = time.time()
        self.tcheck = self.tstop + delay

    def finished(self, result):
        """
        """
        assert result in ['clean','notrun','notdone','fail']
        self.result = result


def batchset( qid ):
    """
    Given a queue/batch/pipe id, this function returns the corresponding
    subdirectory name.  The 'qid' argument can be a string or integer.
    """
    return 'batchset' + str( int( float(qid)/50 + 0.5 ) )


def saveResults( tlist, plat, test_dir, tag, inprogress=False ):
    """
    Option is
    
      --save-results
    
    which writes to the platform config testing directory (which looks first at
    the TESTING_DIRECTORY env var).  Can add
    
      --results-tag <string>
    
    which is appended to the results file name.
    """
    pname = plat.getName()
    cplr = plat.getCompiler()
    
    L = []
    for o in optdict.get('-o',[]):
      if o != cplr:
        L.append(o)
    L.sort()
    L.insert( 0, cplr )
    optstag = '+'.join(L)
    
    rdir = plat.testingDirectory()
    if rdir == None or not os.path.isdir(rdir):
      raise Exception( "invalid testing directory: " + str(rdir) )
    
    datestamp = tlist.getDateStamp( time.time() )
    
    datestr = time.strftime( "%Y_%m_%d", time.localtime( datestamp ) )
    L = ['results',datestr,pname,optstag]
    if tag != None: L.append(tag)
    fname = os.path.join( rdir, string.join( L, '.' ) )
    
    tr = results.TestResults()
    for t in tlist.getActiveTests():
      tr.addTest(t)
    mach = os.uname()[1]
    tr.writeResults( fname, pname, cplr, mach, test_dir, inprogress )


def restartTests( test_dir, plat, ufilter ):
    
    assert test_dir != None
    
    path_filter = os.getcwd()
    
    qid = optdict.get( '--qsub-id', None )
    if qid == None:
        tfile = os.path.join( test_dir, testlist_name )
    else:
        # batch jobs have --qsub-id set and land here
        qid = optdict['--qsub-id']
        subd = batchset( qid )
        tfile = os.path.join( test_dir, subd, testlist_name+'.'+qid )

    tlist = TestList.TestList( ufilter )
    tlist.readFile( tfile )

    if tlist.getFinishDate() == None and '--force' not in optdict:
        print '*** error: tests are currently running in another process'
        print '    (or a previous run crashed); use --force to run anyway'
        sys.exit(1)
    
    reld = computeRelativePath( os.path.abspath(test_dir), os.getcwd() )
    
    pruneL = tlist.loadTests( filter_dir=reld,
                              analyze_only=optdict.has_key('-a'),
                              prune=True )
    
    if len(pruneL) > 0:
        print3()
        for pt,ct in pruneL:
            print3( '*** Warning: analyze test',
                    relative_execute_directory( pt, test_dir, os.getcwd() ),
                    'will not be run due to child',
                    relative_execute_directory( ct, test_dir, os.getcwd() ) )
    
    if qid == None:
      loadExecutionTimes( tlist, plat )
    
    perms = None
    if '--perms' in optdict:
        perms = PermissionSetter( test_dir, optdict['--perms'] )

    tlist.stringFileWrite( tfile )

    try:
        
        if perms != None:
            perms.set( os.path.abspath( tfile ) )
        
        if len(tlist.active) > 0:
          
          print3( "Running these tests:" )
          printResults( tlist, True, test_dir, False, perms, True )
          print3()
          
          if optdict.has_key('--save-results'):
            tag = optdict.get('--results-tag',None)
            saveResults( tlist, plat, test_dir, tag, inprogress=True )
          
          if not optdict.has_key('--pipeline'):
            plat.initProcs( test_dir )
            tlist.createTestExecs( test_dir, plat, config, perms )
            executeTestList( tlist, test_dir, plat, perms, tfile )
          
          else:
            tlist.createTestExecs( test_dir, plat, config, perms )
            pipeLineTestList( tlist, test_dir, plat, perms )
        
          if optdict.has_key('--save-results'):
            tag = optdict.get('--results-tag',None)
            saveResults( tlist, plat, test_dir, tag )
        
        else:
          print3( "\n--------- no tests to run -----------\n" )
    
    except:
        tlist.writeFinished()
        raise
    tlist.writeFinished()


def baselineTests( test_dir, plat, ufilter ):
    
    cwd = os.getcwd()
    
    path_filter = None
    if test_dir == None:
      test_dir = os.path.join( cwd, testResultsDir(plat.getName()) )
    else:
      path_filter = os.getcwd()
    
    tfile = os.path.join( test_dir, testlist_name )

    tlist = TestList.TestList( ufilter )
    tlist.readFile( tfile )
    
    filter_dir = None
    if path_filter != None:
      filter_dir = computeRelativePath( test_dir, path_filter )
    
    # if the keyword expression does not include a results keyword, then
    # add the 'diff' keyword so that only diffs are rebaselined by default
    if not keyword_expr.containsResultsKeywords():
      keyword_expr.append( ['diff'], 'and' )
    
    tlist.loadTests( filter_dir=filter_dir )
    
    if len(tlist.active) > 0:
      
      print "Baselining these tests:"
      printResults( tlist, True, test_dir, False, None, False )
      
      perms = None
      if '--perms' in optdict:
          perms = PermissionSetter( test_dir, optdict['--perms'] )
      
      plat.initProcs( test_dir )
      
      tlist.createTestExecs( test_dir, plat, config, perms )
      
      failures = 0
      for tx in tlist.getTestExecList():
        if isinstance(tx, TestExec.TestExec): ref = tx.atest
        else:                         ref = tx
        sys.stdout.write("baselining " + ref.getExecuteDirectory() + "...")
        sys.stdout.flush()
        tx.start( baseline=1 )
        for i in range(30):
          time.sleep(1)
          if tx.poll():
            if tx.atest.getAttr('result') == "pass":
              sys.stdout.write("done\n")
              sys.stdout.flush()
            else:
              failures = 1
              sys.stdout.write("FAILED\n")
              sys.stdout.flush()
            break
        if not tx.isDone():
          tx.killJob()
          failures = 1
          sys.stdout.write("TIMED OUT\n")
          sys.stdout.flush()
      
      if failures:
        print "\n\n !!!!!!!!!!!!!!  THERE WERE FAILURES  !!!!!!!!!!!!! \n\n"
    
    else:
      print "\n--------- no tests to baseline -----------\n"


###########################################################################

def getUserName():
    """
    Retrieves the user name associated with this process.
    """
    usr = None
    try:
        import getpass
        usr = getpass.getuser()
    except:
        usr = None
    
    if usr == None:
        try:
            uid = os.getuid()
            import pwd
            usr = pwd.getpwuid( uid )[0]
        except:
            usr = None
    
    if usr == None:
        try:
            p = os.path.expanduser( '~' )
            if p != '~':
                usr = os.path.basename( p )
        except:
            usr = None
    
    if usr == None:
        # try manually checking the environment
        for n in ['USER', 'LOGNAME', 'LNAME', 'USERNAME']:
            if os.environ.get(n,'').strip():
                usr = os.environ[n]
                break

    if usr == None:
        raise Exception( "could not determine this process's user name" )

    return usr


def computeRelativePath(d1, d2):
    """
    Compute relative path from directory d1 to directory d2.
    """
    
    assert os.path.isabs(d1)
    assert os.path.isabs(d2)
    
    d1 = os.path.normpath(d1)
    d2 = os.path.normpath(d2)
    
    list1 = string.split( d1, os.sep )
    list2 = string.split( d2, os.sep )
    
    while 1:
      try: list1.remove('')
      except: break
    while 1:
      try: list2.remove('')
      except: break
    
    i = 0
    while i < len(list1) and i < len(list2):
      if list1[i] != list2[i]:
        break
      i = i + 1
    
    p = []
    j = i
    while j < len(list1):
      p.append('..')
      j = j + 1
    
    j = i
    while j < len(list2):
      p.append(list2[j])
      j = j + 1
    
    if len(p) > 0:
      return os.path.normpath( string.join(p, os.sep) )
    
    return "."


def issubdir(parent_dir, subdir):
    """
    TODO: test for relative paths
    """
    lp = len(parent_dir)
    ls = len(subdir)
    if ls > lp and parent_dir + '/' == subdir[0:lp+1]:
      return subdir[lp+1:]
    return None


def print3( *args, **kwargs ):
    s = ' '.join( [ str(x) for x in args ] )
    if len(kwargs) > 0:
        s += ' ' + ' '.join( [ str(k)+'='+str(v) for k,v in kwargs.items() ] )
    sys.stdout.write( s + os.linesep )
    sys.stdout.flush()


###########################################################################
#
# main

def parseOptionsAndGenerateOptionDictionary():
    """
    Generates a dictionary of the options.  Multiple values for an option are
    appended to a list.  Checks are performed for the correct usage of the
    options.  Options that are expected to only have one value are entered
    as a single value, rather than a list with one element.
    """
    argL = []
    for s in sys.argv[1:]:
      if s in ['-h']:
        optdict['-h'] = ''
        return []
      elif s in ['-H','-help','--help']:
        optdict['-H'] = ''
        return []
      elif s =='-vg':
        argL.append( '--vg' )
      else:
        argL.append(s)
    
    import getopt
    try: opts, dirs = getopt.getopt( argL,
                        'hHvgGk:K:p:P:S:o:O:wAj:FRM:n:ibT:Lmeas:x:X:C',
                        longopts=['version','plat=','platopt=',
                                  'keys', 'files', 'pipeline',
                                  'qsub-id=', 'qsub-limit=', 'qsub-length=',
                                  'html', 'check=', 'analyze', 'cdir=',
                                  'search=', 'extract=',
                                  'timeout-multiplier=', 'postclean',
                                  'vg', 'save-results', 'results-tag=',
                                  'config=', 'perms=', 'sort=', 'force' ] )
    except getopt.error, e:
      sys.stderr.write('*** ' + exeName + ': error: ' + str(e) + '\n')
      sys.exit(1)
    
    for optpair in opts:
      n = optpair[0]
      if   n == '--analyze': n = '-a'
      elif n == '--search' : n = '-s'
      elif n == '--postclean': n = '-C'
      elif n == '-F': n = '-R'  # -F is deprecated, use -R instead
      if n == '--results-tag':
        optdict[n] = optpair[1]
      elif not optdict.has_key(n):
        optdict[n] = [optpair[1]]
      else:
        optdict[n].append(optpair[1])
    
    if optdict.has_key('-h'): return []
    if optdict.has_key('-H'): return []
    
    if optdict.has_key('-v') or optdict.has_key('--version'):
      print version
      sys.exit(0)
    
    if optdict.has_key('-G'):
      # the -g and -G options changed into the same thing (now just -g)
      optdict['-g'] = None
    
    if optdict.has_key('-g') and ( optdict.has_key('--plat') or \
                                   optdict.has_key('-i') or \
                                   optdict.has_key('-b') or \
                                   optdict.has_key('-n') or \
                                   optdict.has_key('-m') or \
                                   optdict.has_key('-L') ):
      sys.stderr.write('*** ' + exeName + \
            ': error: the only options allowed with -g are -oOMfkKpPS\n')
      sys.exit(1)
    
    if optdict.has_key('-i') and ( optdict.has_key('-L') or \
                                   optdict.has_key('-g') or \
                                   optdict.has_key('-R') or \
                                   optdict.has_key('-w') or \
                                   optdict.has_key('-S') or \
                                   optdict.has_key('-b') ):
      sys.stderr.write('*** ' + exeName + \
            ': error: the options -fLgbRSF cannot be used with -i\n')
      sys.exit(1)
    
    if optdict.has_key('-b') and ( optdict.has_key('-A') or \
                                   optdict.has_key('-g') or \
                                   optdict.has_key('-w') or \
                                   optdict.has_key('-S') or \
                                   optdict.has_key('-M') ):
      sys.stderr.write('*** ' + exeName + \
            ': error: the options -glfMARS cannot be used with -b\n')
      sys.exit(1)
    
    # parse -k & -K options into two convenient objects
    
    keywL = []
    if optdict.has_key('-k'):
      keywL.extend( optdict['-k'] )
    
    # change -K into -k expressions by using the '!' operator
    for s in optdict.get('-K',[]):
      bangL = map( lambda k: '!'+k, s.split('/') )
      keywL.append( string.join( bangL, '/' ) )
    
    if len(keywL) > 0:
      keyword_expr.append( keywL, 'and' )
    
    # check validity of -p & -P options and combine them
    try:
      pf = FilterExpressions.ParamFilter( optdict.get('-p',None) )
    except ValueError, e:
      sys.stderr.write('*** ' + exeName + \
            ': error: in -p specification: ' + str(e) + '\n')
      sys.exit(1)
    try:
      pf = FilterExpressions.ParamFilter( optdict.get('-P',None) )
    except ValueError, e:
      sys.stderr.write('*** ' + exeName + \
            ': error: in -P specification: ' + str(e) + '\n')
      sys.exit(1)
    
    if optdict.has_key('-P'):
      # convert -P values into -p values
      for s in optdict['-P']:
        s = string.strip(s)
        if s:
          orL = []
          for k in string.split(s,'/'):
            k = string.strip(k)
            if k:
              orL.append( '!' + k )
          if not optdict.has_key('-p'):
            optdict['-p'] = []
          optdict['-p'].append( string.join( orL, '/' ) )
      del optdict['-P']
    
    if optdict.has_key('-X'):
      for s in optdict['-X']:
        s = string.strip(s)
        if s:
          orL = []
          for p in string.split(s,'/'):
            p = string.strip(p)
            if p:
              orL.append( '!' + p )
          if not optdict.has_key('-x'):
            optdict['-x'] = []
          optdict['-x'].append( string.join( orL, '/' ) )
    
    if optdict.has_key('-s'):
      for pat in optdict['-s']:
        try:
          c = re.compile(pat)
        except:
          sys.stderr.write( '*** ' + exeName + ': error: -s option ' + \
                            'specified an invalid regular expression: ' + pat )
          sys.exit(1)
    
    if optdict.has_key('-S'):
      # -S should be a list of param=value strings, which we convert into
      # a dictionary of param name -> list of values
      sdict = {}
      for s in optdict['-S']:
        L = string.split(s,'=',1)
        if len(L) < 2 or not L[0] or not L[1]:
          sys.stderr.write('*** ' + exeName + \
              ': error: -S option values must be "param=value"\n')
          sys.exit(1)
        if sdict.has_key(L[0]): sdict[L[0]].extend( string.split(L[1]) )
        else:                   sdict[L[0]] = string.split( L[1] )
      optdict['-S'] = sdict
    
    if '--sort' in optdict:
        val = ''.join( [ s.strip() for s in optdict['--sort'] ] )
        for c in val:
            if c not in 'nxtdsr':
                sys.stderr.write('*** ' + exeName + \
                    ': error: invalid --sort character: ' + c + '\n')
                sys.exit(1)
        optdict[ '--sort' ] = val

    if optdict.has_key('--plat'):
      if len(optdict['--plat']) > 1:
        sys.stderr.write('*** ' + exeName + \
              ': error: only one --plat option allowed\n')
        sys.exit(1)
      optdict['--plat'] = optdict['--plat'][0]
    
    if optdict.has_key('--platopt'):
      # create a dictionary storing option->value pairs
      popts = {}
      for po in optdict['--platopt']:
        tmp = string.split(po, '=', 1)
        if len(tmp) == 1: popts[ po ] = ''
        else:             popts[ tmp[0] ] = tmp[1]
      # reset the platopt option with the dictionary
      optdict['--platopt'] = popts
    
    if optdict.has_key('--qsub-id'):
      assert len( optdict['--qsub-id'] ) == 1
      optdict['--qsub-id'] = optdict['--qsub-id'][0]
    
    if optdict.has_key('--qsub-limit'):
      if len(optdict['--qsub-limit']) > 1:
        sys.stderr.write('*** ' + exeName + \
              ': error: only one --qsub-limit option allowed\n')
        sys.exit(1)
      try: val = int(optdict['--qsub-limit'][0])
      except:
          sys.stderr.write('*** ' + exeName + \
            ': error: the --qsub-limit option must be followed by an integer\n')
          sys.exit(1)
      if val <= 0:
          sys.stderr.write('*** ' + exeName + \
            ': error: the --qsub-limit value must be a positive integer\n')
          sys.exit(1)
      optdict['--qsub-limit'] = val
    
    if optdict.has_key('--qsub-length'):
      if len(optdict['--qsub-length']) > 1:
        sys.stderr.write('*** ' + exeName + \
              ': error: only one --qsub-length option allowed\n')
        sys.exit(1)
      try: val = int(optdict['--qsub-length'][0])
      except:
          sys.stderr.write('*** ' + exeName + \
            ': error: the --qsub-length option must be followed by an integer\n')
          sys.exit(1)
      if val < 0:
          sys.stderr.write('*** ' + exeName + \
            ': error: the --qsub-length value cannot be negative\n')
          sys.exit(1)
      optdict['--qsub-length'] = val
    
    # clean up option strings
    
    if optdict.has_key('-o'):
      on = {}
      for o1 in optdict.get('-o',[]):
        for o2 in string.split( o1, '+' ):
          for o3 in string.split(o2):
            on[o3] = ''
      on = on.keys()
      on.sort()
      optdict['-o'] = on
      config.set( 'onopts', on )
    
    if optdict.has_key('-O'):
      off = {}
      for o1 in optdict.get('-O'):
        for o2 in string.split( o1, '+' ):
          for o3 in string.split(o2):
            off[o3] = ''
      off = off.keys()
      off.sort()
      optdict['-O'] = off
      config.set( 'offopts', off )
    
    if optdict.has_key('-j') and len(optdict['-j']) > 1:
      sys.stderr.write('*** ' + exeName + \
            ': error: only one -j option allowed\n')
      sys.exit(1)
    
    if optdict.has_key('-M') and len(optdict['-M']) > 1:
      sys.stderr.write('*** ' + exeName + \
            ': error: only one -M option allowed\n')
      sys.exit(1)
    
    if optdict.has_key('-n'):
      if len(optdict['-n']) > 1:
          sys.stderr.write('*** ' + exeName + \
                ': error: only one -n option is allowed\n')
          sys.exit(1)
      try: val = int(optdict['-n'][0])
      except:
          sys.stderr.write('*** ' + exeName + \
                ': error: the -n option must be followd by an integer\n')
          sys.exit(1)
      if val <= 0:
        sys.stderr.write('*** ' + exeName + \
              ': error: the -n option must specify a positive integer\n')
        sys.exit(1);
     
    if optdict.has_key('-T'):
      if len(optdict['-T']) > 1:
          sys.stderr.write('*** ' + exeName + \
                ': error: only one -T option is allowed\n')
          sys.exit(1)
      try: val = float(optdict['-T'][0])
      except:
          sys.stderr.write('*** ' + exeName + \
                ': error: the -T option must be followd by a number\n')
          sys.exit(1);
      if val < 0.0: optdict['-T'] = 0.0
      else:         optdict['-T'] = val
    
    if optdict.has_key('--timeout-multiplier'):
      if len(optdict['--timeout-multiplier']) > 1:
          sys.stderr.write('*** ' + exeName + \
                ': error: only one --timeout-multiplier option is allowed\n')
          sys.exit(1)
      try: val = float(optdict['--timeout-multiplier'][0])
      except:
          sys.stderr.write('*** ' + exeName + \
               ': error: the --timeout-multiplier option must be ' + \
               'followd by a number\n')
          sys.exit(1);
      if val <= 0.0:
        sys.stderr.write('*** ' + exeName + \
             ': error: the --timeout-multiplier option must be ' + \
             'followd by a positive number\n')
        sys.exit(1);
      optdict['--timeout-multiplier'] = val
     
    # collapse the list of length one for single argument options
    
    if optdict.has_key('-j'):
      d = optdict['-j'][0]
      if not os.path.exists(d):
        sys.stderr.write('*** ' + exeName + ': error: project directory ' + \
                         'does not exist: ' + d + os.linesep )
        sys.exit(1)
      d = os.path.abspath(d)
      optdict['-j'] = d
      config.set( 'exepath', d )
    
    if optdict.has_key('-M'):
      optdict['-M'] = optdict['-M'][0]
    
    if optdict.has_key('-n'):
      optdict['-n'] = optdict['-n'][0]
    
    if optdict.has_key('--extract'):
      optdict['--extract'] = optdict['--extract'][-1]
    
    if '--config' in optdict:
        config.set( 'configdir', os.path.abspath( optdict['--config'][-1] ) )
    else:
        d = os.getenv( 'VVTEST_CONFIGDIR' )
        if d == None:
            d = os.path.join( config.get('toolsdir'), 'config' )
        config.set( 'configdir', os.path.abspath(d) )

    config.set( 'refresh', not optdict.has_key('-m') )
    config.set( 'postclean', optdict.has_key('-C') )
    if optdict.has_key('-T'):
      config.set( 'timeout', optdict['-T'] )
    if optdict.has_key('--timeout-multiplier'):
      config.set( 'multiplier', optdict['--timeout-multiplier'] )
    config.set( 'preclean', not optdict.has_key('-m') )
    config.set( 'analyze', optdict.has_key('-a') )
    config.set( 'logfile', not optdict.has_key('-L') )
    
    return dirs


def getExeDirAndName():
    d = sys.path[0]
    if not d:                  d = os.getcwd()
    elif not os.path.isabs(d): d = os.path.abspath(d)
    n = os.path.basename( sys.argv[0] )
    return d, n


##############################################################################


if __name__ == '__main__':
  
  # get directory containing this script and the script name itself
  toolsdir, exeName = getExeDirAndName()
  
  sys.path.insert( 1, toolsdir + '/libvvtest' )
  
  import platform
  import TestSpec
  import TestExec
  import TestList
  import TestSpecCreator
  import FilterExpressions
  import results
  from PermissionSetter import PermissionSetter
  
  keyword_expr = FilterExpressions.WordExpression()
  
  # set this before options are parsed
  config.set( 'toolsdir', toolsdir )
  
  # this creates the global "optdict" dictionary and returns non-option args
  dirs = parseOptionsAndGenerateOptionDictionary()
  
  # set this after options are parsed
  sys.path.insert( 1, config.get('configdir') )
  
  # the help option quits immediately
  if optdict.has_key('-h'):
    print usagepage
    sys.exit(0)
  if optdict.has_key('-H'):
    print manpage
    sys.exit(0)
  
  # 'test_dir' non-None only if the CWD is in a TestResults.* directory
  test_dir = readCommandInfo()
  
  if optdict.has_key( '--check' ):
    for n in optdict['--check']:
      os.environ[ 'CHECK_' + string.upper(n) ] = ''
  
  if optdict.has_key( '--vg' ):
    os.environ[ 'CHECK_VALGRIND' ] = ''
  
  if test_dir and optdict.has_key('-S'):
    print "*** error: cannot use -S in a TestResults directory"
    sys.exit(1)
  
  if test_dir and optdict.has_key('-g'):
    print "*** error: cannot use -g in a TestResults directory"
    sys.exit(1)
  
  # construct platform
  plat = platform.construct_Platform( toolsdir, optdict )
  
  ufilter = FilterExpressions.ExpressionSet( \
                param_expr_list=optdict.get('-p', None),
                keyword_expr=keyword_expr,
                option_list=optdict.get('-o', []) + [plat.getCompiler()],
                platform_name=plat.getName(),
                ignore_platforms=optdict.has_key('-A'),
                platform_expr_list=optdict.get('-x', None),
                search_file_globs=search_fnmatch,
                search_patterns=optdict.get('-s',None) )
  if optdict.has_key('--qsub-id'):
    ufilter.setAttr( 'include_all', True )
  
  info = ( optdict.has_key('-i') or \
           optdict.has_key('--files') or \
           optdict.has_key('--keys') )
  
  if optdict.has_key('-g') and not info:
    assert test_dir == None
    generateTestList( dirs, plat, ufilter )
  
  elif info:
    
    tlist = TestList.TestList( ufilter )
    if test_dir != None:
      tfile = os.path.join( test_dir, testlist_name )
      tlist.readFile( tfile )
      tlist.loadTests( filter_dir=computeRelativePath( test_dir, os.getcwd() ) )
    elif os.path.exists( testResultsDir(plat.getName()) ):
      test_dir = os.path.join( os.getcwd(), testResultsDir(plat.getName()) )
      tfile = os.path.join( test_dir, testlist_name )
      tlist.readFile( tfile )
      tlist.loadTests()
    elif optdict.has_key('--keys') or optdict.has_key('--files'):
      if len(dirs) == 0:
        dirs = ['.']
      for d in dirs:
        if os.path.exists(d):
          tlist.scanDirectory( d, optdict.get('-S',None) )
      tlist.loadTests()
    else:
      print "*** warning: previous TestResults directory not found"
      test_dir = os.path.join( os.getcwd(), testResultsDir(plat.getName()) )
      tlist.loadTests()
    
    if optdict.has_key('--keys') or optdict.has_key('--files'):
      keywordInformation(tlist)
    elif optdict.has_key('--save-results'):
      saveResults( tlist, plat, test_dir, optdict.get('--results-tag',None) )
    else:
      printResults( tlist, True, test_dir, optdict.has_key('--html'), None )
 
  elif optdict.has_key('-b'):
    
    if '-R' in optdict or '-w' in optdict:
      print "*** error: cannot use -R or -w with -b (baseline)"
      sys.exit(1)
    
    baselineTests( test_dir, plat, ufilter )
  
  elif optdict.has_key('--extract'):
    extractTestFiles( dirs, plat, optdict['--extract'], ufilter )
  
  else:
    
    # if no results keywords are specified, then add -k notrun/notdone
    if not keyword_expr.containsResultsKeywords() and \
       '-w' not in optdict and '-R' not in optdict:
      keyword_expr.append( ['notrun/notdone'], 'and' )
    
    if test_dir != None:
      restartTests( test_dir, plat, ufilter )
    else:
      runTests( dirs, plat, ufilter )
