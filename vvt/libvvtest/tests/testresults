#!/usr/bin/env python

# Copyright 2018 National Technology & Engineering Solutions of Sandia, LLC
# (NTESS). Under the terms of Contract DE-NA0003525 with NTESS, the U.S.
# Government retains certain rights in this software.

#RUNTEST:

import sys
sys.dont_write_bytecode = True
sys.excepthook = sys.__excepthook__
import os
import re
import time
import filecmp
import shutil
import glob
import unittest

import testutils as util
from testutils import print3

import libvvtest.TestSpec as TestSpec
import libvvtest.TestSpecCreator as TestSpecCreator
import libvvtest.xmlwrapper as xmlwrapper

sys.path.insert( 0, os.path.dirname( os.path.dirname( util.testsrcdir ) ) )
import results

timesfname = results.runtimes_filename
multifname = results.multiruntimes_filename


class test_results_tests( unittest.TestCase ):

    def setUp(self):
        ""
        util.setup_test()

    def test_make_and_parse_test_attributes(self):
        ""
        aD = { 'xdate':int(time.time()),
               'xtime':23, 'state':'done', 'result':'pass' }
        s = results.make_attr_string( aD )
        D2 = results.read_attrs( s.split() )
        assert aD == D2, str(aD) + ' != ' + str(D2)

        aD = { 'xdate':int(time.time()), 'state':'notdone' }
        s = results.make_attr_string( aD )
        D2 = results.read_attrs( s.split() )
        assert aD == D2, str(aD) + ' != ' + str(D2)

        aD = { 'state':'notrun' }
        s = results.make_attr_string( aD )
        D2 = results.read_attrs( s.split() )
        assert aD == D2, str(aD) + ' != ' + str(D2)

    def test_nominal_uses_of_TestResults_class(self):
        ""
        write_tests_cat_dog_circle()
        time.sleep(1)

        vrun = util.runvvtest()
        vrun.assertCounts( total=3, npass=3 )
        tdir = vrun.resultsDir()

        util.runvvtest( '-i --save-results' )
        resultsfname = util.globfile('results.*')

        # without a runtimes file, no tests should have been added (because the
        # --save-results logic cannot determine the test root relative paths)
        tr = results.TestResults( resultsfname )
        assert tr.testdir() == tdir
        assert len( tr.dirList() ) == 0
        # also check that a write will produce the same file as a read
        platname,cplrname = tr.platform(), tr.compiler()
        mach = os.uname()[1]
        tr.writeResults( resultsfname+'.cmp', platname, cplrname, mach, tdir )
        assert filecmp.cmp( resultsfname, resultsfname+'.cmp' )

        # create runtimes file to enable root relative path determination
        create_and_save_runtimes()

        tr = results.TestResults( resultsfname )
        assert tr.testdir() == tdir
        assert len( tr.dirList() ) == 2
        bd = os.path.basename( os.getcwd() )
        assert tr.dirList() == [bd+'/one',bd+'/two']
        assert tr.testList(bd+'/one') == ['cat','dog']
        assert tr.testList(bd+'/two') == ['circle']
        assert tr.testAttrs(bd+'/one','cat')['xtime'] >= 1
        assert tr.testAttrs(bd+'/one','dog')['xtime'] >= 2
        assert tr.testAttrs(bd+'/two','circle')['xtime'] >= 3
        # also check that a write will produce the same file as a read
        tr.writeResults( resultsfname+'.cmp', platname, cplrname, mach, tdir )
        assert filecmp.cmp( resultsfname, resultsfname+'.cmp' )

        # write root runtimes files in the subdirectories then save results again
        os.remove( timesfname )
        util.runcmd( util.resultspy + ' save', chdir='one' )
        util.runcmd( util.resultspy + ' save', chdir='two' )
        util.runvvtest( '-i --save-results' )
        # this time the tests should have different root-relative paths
        tr = results.TestResults( resultsfname )
        assert tr.dirList() == ['one','two']
        assert tr.testList('one') == ['cat','dog']
        assert tr.testList('two') == ['circle']
        assert tr.testAttrs('one','cat')['xtime'] >= 1
        assert tr.testAttrs('one','dog')['xtime'] >= 2
        assert tr.testAttrs('two','circle')['xtime'] >= 3

        # the runtimes are empty at first (no files to "results.py save")
        tr = results.TestResults( 'one/runtimes' )
        assert len( tr.dirList() ) == 0
        tr = results.TestResults( 'two/runtimes' )
        assert len( tr.dirList() ) == 0

        # saving runtimes again to the subdirectories should populate the
        # runtimes but only include tests at or below the root
        util.runcmd( util.resultspy + ' save ' + os.path.abspath( resultsfname ),
                     chdir='one' )
        util.runcmd( util.resultspy + ' save ' + os.path.abspath( resultsfname ),
                     chdir='two' )
        assert len( util.grepfiles( 'cat*pass',    'one/'+timesfname ) ) == 1
        assert len( util.grepfiles( 'dog*pass',    'one/'+timesfname ) ) == 1
        assert len( util.grepfiles( 'circle*pass', 'two/'+timesfname ) ) == 1
        assert len( util.grepfiles( 'circle*pass', 'one/'+timesfname ) ) == 0
        assert len( util.grepfiles( 'cat*pass',    'two/'+timesfname ) ) == 0
        assert len( util.grepfiles( 'dog*pass',    'two/'+timesfname ) ) == 0

    def test_save_results_with_multiple_parameterize(self):
        ""
        util.writefile( "cat.xml", """
            <rtest name="cat">
            <parameterize direction="front rear"/>
            <parameterize side="left right"/>
              <execute>
                echo "xcat direction $direction side $side"
              </execute>
            </rtest>""" )
        time.sleep(1)

        vrun = util.runvvtest()
        vrun.assertCounts( total=4, npass=4 )

        create_and_save_runtimes()

        resultsfname = util.globfile('results.*')
        tr = results.TestResults( resultsfname )

        assert len( util.grepfiles( "cat", resultsfname ) ) == 4
        assert len( util.grepfiles( "cat.direction=front.side=left", resultsfname ) ) == 1
        assert len( util.grepfiles( "cat.direction=front.side=right", resultsfname ) ) == 1
        assert len( util.grepfiles( "cat.direction=rear.side=left", resultsfname ) ) == 1
        assert len( util.grepfiles( "cat.direction=rear.side=right", resultsfname ) ) == 1

    def test_save_results_with_multiple_parameterize_and_an_analyze(self):
        ""
        util.writefile( "cat.xml", """
            <rtest name="cat">
            <parameterize direction="front rear"/>
            <parameterize side="left right"/>
              <execute>
                echo "xcat direction $direction side $side"
              </execute>
              <analyze>
                echo "analyze direction $PARAM_direction"
                echo "analyze side $PARAM_side"
              </analyze>
            </rtest>""" )
        time.sleep(1)

        vrun = util.runvvtest()
        vrun.assertCounts( total=5, npass=5 )

        create_and_save_runtimes()

        resultsfname = util.globfile('results.*')
        tr = results.TestResults( resultsfname )

        assert len( util.grepfiles( "cat", resultsfname ) ) == 5
        assert len( util.grepfiles( "cat.direction=front.side=left", resultsfname ) ) == 1
        assert len( util.grepfiles( "cat.direction=front.side=right", resultsfname ) ) == 1
        assert len( util.grepfiles( "cat.direction=rear.side=left", resultsfname ) ) == 1
        assert len( util.grepfiles( "cat.direction=rear.side=right", resultsfname ) ) == 1

    def test_save_results_with_zipped_parameterize(self):
        ""
        util.writefile( "cat.xml", """
            <rtest name="cat">
            <parameterize direction="front rear"
                          side="left right"/>
              <execute>
                echo "xcat direction $direction side $side"
              </execute>
            </rtest>""" )
        time.sleep(1)

        vrun = util.runvvtest()
        vrun.assertCounts( total=2, npass=2 )

        create_and_save_runtimes()

        resultsfname = util.globfile('results.*')
        tr = results.TestResults( resultsfname )

        assert len( util.grepfiles( "cat", resultsfname ) ) == 2
        assert len( util.grepfiles( "cat.direction=front.side=left", resultsfname ) ) == 1
        assert len( util.grepfiles( "cat.direction=rear.side=right", resultsfname ) ) == 1

    def test_save_results_with_zipped_parameterize_and_an_analyze(self):
        ""
        util.writefile( "cat.xml", """
            <rtest name="cat">
            <parameterize direction="front rear"
                          side="left right"/>
              <execute>
                echo "xcat direction $direction side $side"
              </execute>
              <analyze>
                echo "analyze direction plus side $PARAM_direction_side"
              </analyze>
            </rtest>""" )
        time.sleep(1)

        vrun = util.runvvtest()
        vrun.assertCounts( total=3, npass=3 )

        create_and_save_runtimes()

        resultsfname = util.globfile('results.*')
        tr = results.TestResults( resultsfname )

        assert len( util.grepfiles( "cat", resultsfname ) ) == 3
        assert len( util.grepfiles( "cat.direction=front.side=left", resultsfname ) ) == 1
        assert len( util.grepfiles( "cat.direction=rear.side=right", resultsfname ) ) == 1

    def test_nominal_use_of_MultiResults_class(self):
        ""
        write_tests_cat_dog_circle()
        time.sleep(1)

        vrun = util.runvvtest()
        vrun.assertCounts( total=3, npass=3 )
        tdir = vrun.resultsDir()

        # bootstrap with runtimes files then save the test results
        util.runcmd( util.resultspy + ' save', chdir='one' )
        util.runcmd( util.resultspy + ' save', chdir='two' )
        util.runvvtest( '-i --save-results' )

        resultsfname = util.globfile('results.*')
        tr = results.TestResults( resultsfname )
        platname,cplrname = tr.platform(), tr.compiler()

        util.runcmd( util.resultspy + ' merge ' + resultsfname )

        assert os.path.exists(multifname)
        mr = results.MultiResults()
        mr.readFile( multifname )
        assert len( util.grepfiles( 'cat*'+platname+'*'+cplrname+'*pass', multifname ) ) == 1
        assert len( util.grepfiles( 'dog*'+platname+'*'+cplrname+'*pass', multifname ) ) == 1
        assert len( util.grepfiles( 'circle*'+platname+'*'+cplrname+'*pass', multifname ) ) == 1

        # create a second results file with a different platform/compiler name
        tr = results.TestResults( resultsfname )
        resultsfname2 = 'results2'
        mach = os.uname()[1]
        tr.writeResults( resultsfname2, 'Plat2', 'Cplr2', mach, tdir )

        # merge a second platform/compiler file into first
        util.runcmd( util.resultspy + ' merge -x '+resultsfname2 )
        assert len( util.grepfiles( 'cat*'+platname+'*'+cplrname+'*pass', multifname ) ) == 1
        assert len( util.grepfiles( 'dog*'+platname+'*'+cplrname+'*pass', multifname ) ) == 1
        assert len( util.grepfiles( 'circle*'+platname+'*'+cplrname+'*pass', multifname ) ) == 1
        assert len( util.grepfiles( 'cat*Plat2'+'*Cplr2'+'*pass', multifname ) ) == 1
        assert len( util.grepfiles( 'dog*Plat2'+'*Cplr2'+'*pass', multifname ) ) == 1
        assert len( util.grepfiles( 'circle*Plat2'+'*Cplr2'+'*pass', multifname ) ) == 1

        # spot check the run time of the 'dog' test
        mr = results.MultiResults()
        mr.readFile( multifname )
        aD = mr.testAttrs( 'one', 'dog', platname+'/'+cplrname )
        t1 = aD.get( 'xtime', -1 )
        assert t1 > 0 and t1 < 4

        # increase the run time of one test and run vvtest again
        util.writefile( "one/dog.xml", """
            <rtest name="dog">
              <execute>
                sleep 4
              </execute>
            </rtest>""" )
        time.sleep(1)

        vrun = util.runvvtest( '-w' )
        vrun.assertCounts( total=3, npass=3 )

        # merge over the top of the multi platform file
        util.runvvtest( '-i --save-results' )
        util.runcmd( util.resultspy + ' merge -x '+resultsfname )
        # now check the 'dog' test time
        mr = results.MultiResults()
        mr.readFile( multifname )
        aD = mr.testAttrs( 'one', 'dog', platname+'/'+cplrname )
        t2 = aD.get( 'xtime', -1 )
        assert t2 > t1

        # check that the other 'dog' time did not get overwritten
        aD = mr.testAttrs( 'one', 'dog', 'Plat2/Cplr2' )
        t3 = aD.get( 'xtime', -1 )
        assert t2 > t3

    def test_merging_multiplatform_files_by_test_date_and_globbing(self):
        ""
        write_test_cat()
        write_test_dog()
        time.sleep(1)

        vrun = util.runvvtest()
        vrun.assertCounts( total=2, npass=2 )

        # create a multiplatform timings file
        util.runcmd( util.resultspy + ' save' )
        util.runvvtest( '-i --save-results' )
        resultsfname = util.globfile('results.*')
        util.runcmd( util.resultspy + ' merge -x '+resultsfname )

        # get the platform name & compiler
        tr = results.TestResults( resultsfname )
        platname,cplrname = tr.platform(), tr.compiler()
        platcplr = platname+'/'+cplrname

        time.sleep(1)  # force a gap between vvtest invocations

        # make the test diff this time
        util.writefile( "one/cat.xml", """
            <rtest name="cat">
              <execute>
                sleep 1
                set have_diff = 1
              </execute>
            </rtest>""" )
        # also remove the dog test
        os.remove( "one/dog.xml" )
        time.sleep(1)

        vrun = util.runvvtest( '-w' )
        vrun.assertCounts( total=1, diff=1 )

        # save a second results file
        util.runvvtest( '-i --save-results --results-tag diffs' )
        results2fname = None
        for f in glob.glob('results.*'):
          if f != resultsfname:
            results2fname = f
            break
        assert results2fname and resultsfname != results2fname

        # merge the second results file into the timings file
        util.runcmd( util.resultspy + ' merge -x '+results2fname )

        # check that the timings file contains the new test results
        mr = results.MultiResults()
        mr.readFile( multifname )
        root = os.path.basename( os.getcwd() )
        catD = mr.testAttrs( root+'/one', 'cat', platcplr )
        assert catD.get('result','') == 'diff'

        # merge the first results file into the timings file
        util.runcmd( util.resultspy + ' merge -x '+resultsfname )

        # check that the timings file does NOT contain the first test results
        mr = results.MultiResults()
        mr.readFile( multifname )
        catD = mr.testAttrs( root+'/one', 'cat', platcplr )
        assert catD.get('result','') == 'diff'

        # now force a merge from the first results file
        util.runcmd( util.resultspy + ' merge -w '+resultsfname )

        # check that the timings file DOES contain the first test results
        mr = results.MultiResults()
        mr.readFile( multifname )
        catD = mr.testAttrs( root+'/one', 'cat', platcplr )
        assert catD.get('result','') == 'pass'

        # modify the name of the old results file to test the -d option
        nL = resultsfname.split('.')
        tm = time.mktime( time.strptime( nL[1], '%Y_%m_%d' ) )
        d5 = time.strftime( "%Y_%m_%d", time.localtime( tm - 5*24*60*60 ) )
        newfname = '.'.join( [nL[0],d5]+nL[2:] )
        os.rename( resultsfname, newfname )
        resultsfname = newfname

        # create the timings file with the newest results
        os.remove( multifname )
        util.runcmd( util.resultspy + ' merge -x '+results2fname )

        # the timings file should contain the new results and not contain the dog test
        mr = results.MultiResults()
        mr.readFile( multifname )
        catD = mr.testAttrs( root+'/one', 'cat', platcplr )
        assert catD.get('result','') == 'diff'
        dogD = mr.testAttrs( root+'/one', 'dog', platcplr )
        assert len(dogD) == 0

        # merge again with -d but should still just pick up latest results
        util.runcmd( util.resultspy + ' merge -d 2 -g results.*' )

        # the timings file should still contain only the latest results
        mr = results.MultiResults()
        mr.readFile( multifname )
        catD = mr.testAttrs( root+'/one', 'cat', platcplr )
        assert catD.get('result','') == 'diff'
        dogD = mr.testAttrs( root+'/one', 'dog', platcplr )
        assert len(dogD) == 0

        # now merge from 6 days ago to catch the old results file
        util.runcmd( util.resultspy + ' merge -d 7 -g results.*' )

        # should be the latest cat results but the old dog test should appear
        mr = results.MultiResults()
        mr.readFile( multifname )
        catD = mr.testAttrs( root+'/one', 'cat', platcplr )
        assert catD.get('result','') == 'diff'
        dogD = mr.testAttrs( root+'/one', 'dog', platcplr )
        assert dogD.get('result','') == 'pass'

    def test_merging_a_multiplatform_file_into_another_multiplatform_file(self):
        ""
        write_tests_cat_dog_circle( in_subdir='tsrc' )
        time.sleep(1)

        vrun = util.runvvtest( 'tsrc' )
        vrun.assertCounts( total=3, npass=3 )
        tdir = vrun.resultsDir()

        util.runcmd( util.resultspy + ' save', chdir='tsrc' )

        os.mkdir( 'testing' )
        os.environ['TESTING_DIRECTORY'] = os.path.abspath( 'testing' )

        util.runvvtest( '-i --save-results' )
        resultsfname = os.path.abspath( util.globfile('testing/results.*') )

        # create initial multiplat file
        util.runcmd( util.resultspy + ' merge -x '+ resultsfname,
                     chdir='testing' )
        assert os.path.exists( os.path.join( 'testing', multifname ) )

        # create a second results file with a different platform/compiler name
        tr = results.TestResults( resultsfname )
        resultsfname2 = os.path.abspath( 'testing/results2' )
        mach = os.uname()[1]
        tr.writeResults( resultsfname2, 'Plat2', 'Cplr2', mach, tdir )

        # create second multiplat file
        util.runcmd( util.resultspy + ' merge -x '+resultsfname2 )

        # merge second multiplat into first multiplat
        util.runcmd( util.resultspy + ' merge -x '+ os.path.abspath(multifname),
                     chdir='testing' )

        # now check the combined multiplat file
        mr = results.MultiResults()
        mr.readFile( 'testing/'+multifname )
        assert mr.dirList() == ['tsrc/one','tsrc/two']
        assert mr.testList('tsrc/one') == ['cat','dog']
        assert mr.testList('tsrc/two') == ['circle']
        assert len( mr.platformList('tsrc/one','cat') ) == 2
        assert len( mr.platformList('tsrc/one','dog') ) == 2
        assert len( mr.platformList('tsrc/two','circle') ) == 2

    def test_creating_source_tree_runtimes_files(self):
        ""
        write_tests_cat_dog_circle( in_subdir='tsrc' )
        time.sleep(1)

        vrun = util.runvvtest()
        vrun.assertCounts( total=3, npass=3 )
        tdir = vrun.resultsDir()
        # bootstrap runtimes
        util.runcmd( util.resultspy + ' save', chdir='tsrc' )

        os.mkdir( 'testing' )
        os.environ['TESTING_DIRECTORY'] = os.path.join( os.getcwd(), 'testing' )

        util.runvvtest( '-i --save-results' )
        resultsfname = os.path.abspath( util.globfile('testing/results.*') )

        # use the results.py script to save the results to the runtimes file
        util.runcmd( util.resultspy + ' save ' + resultsfname, chdir='tsrc' )

        tr = results.TestResults( 'tsrc/'+timesfname )
        assert tr.dirList() == ['tsrc/one','tsrc/two']
        assert tr.testList('tsrc/one') == ['cat','dog']
        assert tr.testList('tsrc/two') == ['circle']

        # hold on to the runtime for the cat test
        tr = results.TestResults( resultsfname )
        cat_t1 = tr.testAttrs( 'tsrc/one', 'cat' )['xtime']

        # increase the run time of one test and run vvtest again
        util.writefile( "tsrc/one/cat.xml", """
            <rtest name="cat">
              <execute>
                sleep 4
              </execute>
            </rtest>""" )
        time.sleep(1)

        vrun = util.runvvtest( '-w -k cat tsrc' )
        vrun.assertCounts( total=1, npass=1 )
        util.runvvtest( '-i --save-results --results-tag 2nd' )

        rfn2 = os.path.abspath( util.globfile( 'testing/results.*.2nd' ) )

        # extract the runtime for the second cat test
        tr = results.TestResults( rfn2 )
        cat_t2 = tr.testAttrs( 'tsrc/one', 'cat' )['xtime']
        assert cat_t1 < cat_t2

        # merge two results files into the runtimes file
        util.runcmd( util.resultspy + ' save ' + rfn2 + ' ' + resultsfname,
                     chdir='tsrc' )

        tr = results.TestResults( 'tsrc/'+timesfname )
        assert tr.dirList() == ['tsrc/one','tsrc/two']
        assert tr.testList('tsrc/one') == ['cat','dog']
        assert tr.testList('tsrc/two') == ['circle']
        # two instances of the cat test means the timings should be averaged
        assert tr.testAttrs( 'tsrc/one', 'cat' )['xtime'] == int((cat_t1+cat_t2)/2)

        # modify a test then generate a multi platform results file
        util.writefile( "tsrc/two/circle.xml", """
            <rtest name="circle">
              <execute>
                sleep 1
              </execute>
            </rtest>""" )
        time.sleep(1)

        vrun = util.runvvtest()
        vrun.assertCounts( total=3, npass=3 )
        util.runvvtest( '-i --save-results --results-tag 3rd' )
        rfn3 = os.path.abspath( util.globfile( 'testing/results.*.3rd' ) )
        # force the results to be a different platform/compiler
        tr = results.TestResults( rfn3 )
        mach = os.uname()[1]
        tr.writeResults( rfn3, 'Plat2', 'Cplr2', mach, tdir )
        util.runcmd( util.resultspy + ' merge '+rfn3, chdir='testing' )

        # save off the circle runtime from first execution
        tr = results.TestResults( 'tsrc/'+timesfname )
        circ1 = tr.testAttrs( 'tsrc/two', 'circle' )['xtime']

        # update the test src runtimes file to include the multiplat contents
        mf = os.path.abspath( 'testing/'+multifname )
        util.runcmd( util.resultspy + ' save '+mf, chdir='tsrc' )

        tr = results.TestResults( 'tsrc/'+timesfname )
        circ2 = tr.testAttrs( 'tsrc/two', 'circle' )['xtime']
        assert circ1 != circ2
        mr = results.MultiResults( 'testing/'+multifname )
        assert mr.testAttrs( 'tsrc/two', 'circle', 'Plat2/Cplr2' )['xtime'] == circ2

    def test_merging_using_the_default_merge_method(self):
        ""
        write_test_cat( sleep_time=3 )
        write_test_dog( sleep_time=1 )
        time.sleep(1)

        vrun = util.runvvtest()
        vrun.assertCounts( total=2, npass=2 )
        util.runcmd( util.resultspy + ' save' )  # bootstrap runtimes

        # create a multiplatform timings file
        util.runvvtest( '-i --save-results' )
        resultsfname = os.path.abspath( util.globfile('results.*') )
        util.runcmd( util.resultspy + ' merge '+resultsfname )

        # get the platform name & compiler
        tr = results.TestResults( resultsfname )
        platname,cplrname = tr.platform(), tr.compiler()
        platcplr = platname+'/'+cplrname

        time.sleep(1)  # force a gap between vvtest invocations

        # make the test run faster
        write_test_cat( sleep_time=1 )
        time.sleep(1)

        vrun = util.runvvtest( '-w' )
        vrun.assertCounts( total=2, npass=2 )

        # save a second results file
        util.runvvtest( '-i --save-results --results-tag faster' )
        results2fname = None
        for f in glob.glob('results.*'):
            f = os.path.abspath(f)
            if f != resultsfname:
                results2fname = f
                break
        assert results2fname and resultsfname != results2fname

        # get the old runtimes from the multiplat file
        mr = results.MultiResults( multifname )
        root = os.path.basename( os.getcwd() )
        oldcatD = mr.testAttrs( root+'/one', 'cat', platcplr )
        olddogD = mr.testAttrs( root+'/one', 'dog', platcplr )

        # merge the second results file into the timings file
        util.runcmd( util.resultspy + ' merge '+results2fname )

        # check that the timings file contains the old test results
        mr = results.MultiResults( multifname )
        catD = mr.testAttrs( root+'/one', 'cat', platcplr )
        assert catD.get('xtime','') == oldcatD.get('xtime','no')
        dogD = mr.testAttrs( root+'/one', 'dog', platcplr )
        assert dogD.get('xtime','') == olddogD.get('xtime','no')

        # make a test run slower
        write_test_dog( sleep_time=3 )
        time.sleep(1)

        vrun = util.runvvtest( '-w' )
        vrun.assertCounts( total=2, npass=2 )

        # save a third results file
        util.runvvtest( '-i --save-results --results-tag slower' )
        results3fname = None
        for f in glob.glob('results.*slower*'):
            f = os.path.abspath(f)
            if f != resultsfname:
                results3fname = f
                break
        assert results3fname and results2fname != results3fname

        # merge the third results file into the timings file
        util.runcmd( util.resultspy + ' merge '+results3fname )

        # check that the timings file contains new test results for the slower test
        mr = results.MultiResults( multifname )
        newcatD = mr.testAttrs( root+'/one', 'cat', platcplr )
        assert newcatD.get('xtime','') == catD.get('xtime','no')
        newdogD = mr.testAttrs( root+'/one', 'dog', platcplr )
        assert newdogD.get('xtime','') > dogD.get('xtime','no')

        # check that including all the results files does not change anything
        util.runcmd( util.resultspy + ' merge -d 6 -g results.*' )

        mr = results.MultiResults( multifname )
        cD = mr.testAttrs( root+'/one', 'cat', platcplr )
        assert cD.get('xtime','') == newcatD.get('xtime','no')
        dD = mr.testAttrs( root+'/one', 'dog', platcplr )
        assert dD.get('xtime','') == newdogD.get('xtime','no')

    def test_merging_using_more_than_one_g_option(self):
        ""
        write_test_cat( sleep_time=3 )
        write_test_dog( sleep_time=1 )
        time.sleep(1)

        vrun = util.runvvtest()
        vrun.assertCounts( total=2, npass=2 )
        util.runcmd( util.resultspy + ' save' )  # bootstrap runtimes

        # create a multiplatform timings file
        util.runvvtest( '-i --save-results --results-tag first' )
        resultsfname = os.path.abspath( util.globfile('results.*') )

        # get the platform name & compiler
        tr = results.TestResults( resultsfname )
        platname,cplrname = tr.platform(), tr.compiler()
        platcplr = platname+'/'+cplrname

        time.sleep(1)  # force a gap between vvtest invocations

        # make the test run faster
        write_test_cat( sleep_time=1 )
        time.sleep(1)

        vrun = util.runvvtest( '-w' )
        vrun.assertCounts( total=2, npass=2 )

        # save a second results file
        util.runvvtest( '-i --save-results --results-tag second' )
        results2fname = os.path.abspath( util.globfile('results.*.second') )
        assert results2fname != resultsfname

        time.sleep(1)  # force a gap between vvtest invocations

        # make the test run slower
        write_test_cat( sleep_time=6 )
        time.sleep(1)

        vrun = util.runvvtest( '-w' )
        vrun.assertCounts( total=2, npass=2 )

        # save a second results file
        util.runvvtest( '-i --save-results --results-tag third' )
        results3fname = os.path.abspath( util.globfile('results.*.third') )
        assert results3fname != resultsfname and results3fname != results2fname

        # merge the first and second results files
        util.runcmd( util.resultspy + ' merge' + \
                          ' -g results.*.first -g results.*.second' )

        # check that the timings file took the max
        mr = results.MultiResults( multifname )
        root = os.path.basename( os.getcwd() )
        catD = mr.testAttrs( root+'/one', 'cat', platcplr )
        assert abs( catD.get('xtime') - 3 ) < 2

        # again but merge the second two results files
        os.remove( multifname )
        util.runcmd( util.resultspy + ' merge' + \
                          ' -g results.*.third -g results.*.second' )
        mr = results.MultiResults( multifname )
        catD = mr.testAttrs( root+'/one', 'cat', platcplr )
        assert abs( catD.get('xtime') - 6 ) < 2

        # now merge by order on the command line
        os.remove( multifname )
        util.runcmd( util.resultspy + ' merge -w' + \
                          ' -g results.*.third -g results.*.first' )
        mr = results.MultiResults( multifname )
        catD = mr.testAttrs( root+'/one', 'cat', platcplr )
        assert catD.get('xtime') < 6

    def test_source_tree_runtimes_files_within_subdirectories(self):
        ""
        write_tests_cat_dog_circle( in_subdir='tsrc' )
        time.sleep(1)

        vrun = util.runvvtest( 'tsrc' )
        vrun.assertCounts( total=3, npass=3 )

        os.mkdir( 'testing' )
        os.environ['TESTING_DIRECTORY'] = os.path.abspath( 'testing' )

        # write a (blank) runtimes file into test source to bootstrap
        util.runcmd( util.resultspy + ' save', chdir='tsrc' )

        # save the results
        util.runvvtest( '-i --save-results' )
        resultsfname = os.path.abspath( util.globfile('testing/results.*') )

        # create a runtimes file in a subdirectory
        util.runcmd( util.resultspy + ' save ' + resultsfname, chdir='tsrc/one' )

        tr = results.TestResults( 'tsrc/'+timesfname )
        assert len(tr.dirList()) == 0
        tr.readResults( 'tsrc/one/'+timesfname )
        assert len(tr.dirList()) == 1
        assert tr.testList('tsrc/one') == ['cat','dog']
        assert not os.path.exists('tsrc/two/'+timesfname)

        # add a test to 'one', rerun the tests, then save runtimes at the top level
        util.writefile( "tsrc/one/ferret.xml", """
            <rtest name="ferret">
              <execute>
                sleep 1
              </execute>
            </rtest>""" )
        time.sleep(1)

        vrun = util.runvvtest( '-w tsrc' )
        vrun.assertCounts( total=4, npass=4 )

        util.runvvtest( '-i --save-results' )
        util.runcmd( util.resultspy + ' save ' + resultsfname, chdir='tsrc' )

        tr = results.TestResults( 'tsrc/'+timesfname )
        assert tr.dirList() == ['tsrc/one','tsrc/two']
        assert tr.testList('tsrc/one') == ['cat','dog','ferret']
        assert tr.testList('tsrc/two') == ['circle']
        tr.readResults( 'tsrc/one/'+timesfname )
        assert len(tr.dirList()) == 1
        assert tr.testList('tsrc/one') == ['cat','dog','ferret']
        assert not os.path.exists('tsrc/two/'+timesfname)

    def test_saving_source_tree_runtimes_files_without_merging(self):
        ""
        write_tests_cat_dog_circle( in_subdir='tsrc' )
        time.sleep(1)

        vrun = util.runvvtest( 'tsrc' )
        vrun.assertCounts( total=3, npass=3 )

        os.mkdir( 'testing' )
        os.environ['TESTING_DIRECTORY'] = os.path.join( os.getcwd(), 'testing' )

        # write a (blank) runtimes file into test source to bootstrap
        util.runcmd( util.resultspy + ' save', chdir='tsrc' )

        # save the results
        util.runvvtest( '-i --save-results' )
        resultsfname = os.path.abspath( util.globfile('testing/results.*') )

        resultsfname = os.path.abspath( util.globfile('testing/results.*') )

        # create a runtimes file in a subdirectory
        util.runcmd( util.resultspy + ' save ' + resultsfname, chdir='tsrc' )

        tr = results.TestResults( 'tsrc/'+timesfname )
        assert tr.dirList() == ['tsrc/one','tsrc/two']
        assert tr.testList('tsrc/one') == ['cat','dog']
        assert tr.testList('tsrc/two') == ['circle']

        # remove a test
        os.remove( 'tsrc/one/dog.xml' )

        vrun = util.runvvtest( '-w tsrc' )
        vrun.assertCounts( total=2, npass=2 )

        util.runvvtest( '-i --save-results' )
        util.runcmd( util.resultspy+' save -w '+resultsfname, chdir='tsrc' )

        # dog gone
        tr = results.TestResults( 'tsrc/'+timesfname )
        assert tr.dirList() == ['tsrc/one','tsrc/two']
        assert tr.testList('tsrc/one') == ['cat']
        assert tr.testList('tsrc/two') == ['circle']
        assert len( util.grepfiles( 'dog', 'tsrc/'+timesfname ) ) == 0

    def test_listing_of_results_files(self):
        ""
        write_tests_cat_dog_circle( in_subdir='tsrc' )
        time.sleep(1)

        vrun = util.runvvtest( 'tsrc' )
        vrun.assertCounts( total=3, npass=3 )
        tdir = vrun.resultsDir()
        util.runcmd( util.resultspy + ' save', chdir='tsrc' )  # bootstrap runtimes

        os.mkdir( 'testing' )
        os.environ['TESTING_DIRECTORY'] = os.path.abspath( 'testing' )

        util.runvvtest( '-i --save-results' )
        resultsfname = os.path.abspath( util.globfile('testing/results.*') )

        # create a duplicate results file with known platform/compiler names
        tr = results.TestResults( resultsfname )
        resultsfname1 = os.path.abspath( 'testing/results1' )
        mach = os.uname()[1]
        tr.writeResults( resultsfname1, 'Plat1', 'Cplr1', mach, tdir )
        resultsfname2 = os.path.abspath( 'testing/results2' )
        tr.writeResults( resultsfname2, 'Plat2', 'Cplr2', mach, tdir )

        # check listing a results file
        x,out = util.runcmd( util.resultspy + ' list -p '+resultsfname1 )
        assert out.strip() == 'Plat1/Cplr1'
        x,out = util.runcmd( util.resultspy + ' list '+resultsfname1 )
        assert len( util.grep( out, 'Plat1/Cplr1' ) ) == 0
        assert len( util.grep( out, 'tsrc/two/circle' ) ) == 1
        assert len( util.grep( out, 'tsrc/one/dog' ) ) == 1
        assert len( util.grep( out, 'tsrc/one/cat' ) ) == 1

        # create multiplat file with two platform/compiler combos
        util.runcmd( util.resultspy + ' merge -x ' + \
                               resultsfname1 + ' ' + resultsfname2,
                     chdir='testing' )

        # check listing a multiplatform file
        x,out = util.runcmd( util.resultspy + ' list -p testing/'+multifname )
        assert out.split() == ['Plat1/Cplr1','Plat2/Cplr2']
        
        x,out = util.runcmd( util.resultspy + ' list testing/'+multifname )
        assert len( util.grep( out, 'Plat1/Cplr1' ) ) == 3
        assert len( util.grep( out, 'Plat2/Cplr2' ) ) == 3
        assert len( util.grep( out, 'tsrc/two/circle' ) ) == 2
        assert len( util.grep( out, 'tsrc/one/dog' ) ) == 2
        assert len( util.grep( out, 'tsrc/one/cat' ) ) == 2

        # save a copy the multiplat file
        shutil.copy( 'testing/'+multifname, 'testing/save' )

        # remove Plat1/Cplr1 from the multiplat file
        util.runcmd( util.resultspy + ' clean -p Plat1/Cplr1 testing/'+multifname )
        x,out = util.runcmd( util.resultspy + ' list testing/'+multifname )
        assert len( util.grep( out, 'Plat1/Cplr1' ) ) == 0
        assert len( util.grep( out, 'Plat2/Cplr2' ) ) == 3
        assert len( util.grep( out, 'tsrc/two/circle' ) ) == 1
        assert len( util.grep( out, 'tsrc/one/dog' ) ) == 1
        assert len( util.grep( out, 'tsrc/one/cat' ) ) == 1

    def test_the_process_files_function(self):
        ""
        # globbing files
        optD = {}
        fL = ['file.txt']
        results.process_files( optD, fL, None )
        assert fL == ['file.txt']

        util.writefile( 'file1.txt', """
            file one contents
            """ )
        util.writefile( 'file2.txt', """
            file two contents
            """ )
        util.writefile( 'file3.log', """
            file three contents
            """ )
        util.writefile( 'foo.dat', """
            foo contents
            """ )
        time.sleep(1)

        optD = { '-g':['file*.txt','fo*.*'] }
        fL = ['bar.txt']
        results.process_files( optD, fL, None )
        fL.sort()
        assert fL == ['bar.txt','file1.txt','file2.txt','foo.dat']

        # specifying the platform
        fileL = [ 'results.2016_02_10.Linux.gnu4.bnb',
                  'results.2016_02_10.SunOS.gnu4.bnb',
                  'results.2016_02_11.Linux.gnu4.bnb' ]
        optD = { '-p':['Linux'] }
        results.process_files( optD, fileL, None )
        assert fileL == ['results.2016_02_10.Linux.gnu4.bnb',
                         'results.2016_02_11.Linux.gnu4.bnb']

        fileL = [ 'results.2016_02_10.TLCC.gnu4.bnb',
                  'results.2016_02_10.SunOS.gnu4.bnb',
                  'results.2016_02_11.Darwin.gnu4.bnb' ]
        optD = { '-p':['SunOS','TLCC'] }
        results.process_files( optD, fileL, None )
        assert fileL == ['results.2016_02_10.TLCC.gnu4.bnb',
                         'results.2016_02_10.SunOS.gnu4.bnb']

        fileL = [ 'results.2016_02_10.TLCC.gnu4.bnb',
                  'results.2016_02_10.SunOS.gnu4.bnb',
                  'results.2016_02_11.Darwin.gnu4.bnb' ]
        optD = { '-P':['TLCC'] }
        results.process_files( optD, fileL, None )
        assert fileL == ['results.2016_02_10.SunOS.gnu4.bnb',
                         'results.2016_02_11.Darwin.gnu4.bnb']

        fileL = [ 'results.2016_02_10.TLCC.gnu4.bnb',
                  'results.2016_02_10.SunOS.gnu4.bnb',
                  'results.2016_02_11.Darwin.gnu4.bnb' ]
        optD = { '-P':['Darwin','TLCC'] }
        results.process_files( optD, fileL, None )
        assert fileL == ['results.2016_02_10.SunOS.gnu4.bnb' ]

        fileL = [ 'results.2016_02_10.TLCC.gnu4.bnb',
                  'results.2016_02_10.SunOS.gnu4.bnb',
                  'results.2016_02_11.Darwin.gnu4.bnb' ]
        optD = { '-p':['SunOS'], '-P':['Darwin'] }
        results.process_files( optD, fileL, None )
        assert fileL == ['results.2016_02_10.SunOS.gnu4.bnb' ]

        # specifying options
        fileL = [ 'results.2016_02_10.Linux.gnu4.bnb',
                  'results.2016_02_10.Linux.gnu4.dev',
                  'results.2016_02_11.Linux.gnu4.bnb' ]
        optD = { '-t':['bnb'] }
        results.process_files( optD, fileL, None )
        assert fileL == ['results.2016_02_10.Linux.gnu4.bnb',
                        'results.2016_02_11.Linux.gnu4.bnb' ]

        # specifying options
        fileL = [ 'results.2016_02_10.Linux.gnu4.bnb',
                  'results.2016_02_10.Linux.gnu4+cxx11.dev',
                  'results.2016_02_11.Linux.gnu4.bnb' ]
        optD = { '-o':['gnu4'] }
        results.process_files( optD, fileL, None )
        assert fileL == ['results.2016_02_10.Linux.gnu4.bnb',
                         'results.2016_02_10.Linux.gnu4+cxx11.dev',
                         'results.2016_02_11.Linux.gnu4.bnb' ]

        fileL = [ 'results.2016_02_10.Linux.gnu4.bnb',
                  'results.2016_02_10.Linux.gnu4+cxx11.dev',
                  'results.2016_02_11.Linux.gnu4.bnb' ]
        optD = { '-o':['cxx11'] }
        results.process_files( optD, fileL, None )
        assert fileL == ['results.2016_02_10.Linux.gnu4+cxx11.dev' ]

        fileL = [ 'results.2016_02_10.Linux.intel.bnb',
                  'results.2016_02_10.Linux.gnu4.dev',
                  'results.2016_02_11.Linux.intel+cxx11.bnb' ]
        optD = { '-o':['intel','cxx11'] }
        results.process_files( optD, fileL, None )
        assert fileL == ['results.2016_02_10.Linux.intel.bnb',
                         'results.2016_02_11.Linux.intel+cxx11.bnb' ]

        fileL = [ 'results.2016_02_10.Linux.intel.bnb',
                  'results.2016_02_10.Linux.gnu4.dev',
                  'results.2016_02_11.Linux.intel+cxx11.bnb' ]
        optD = { '-o':['gnu4','cxx11'] }
        results.process_files( optD, fileL, None )
        assert fileL == ['results.2016_02_10.Linux.gnu4.dev',
                         'results.2016_02_11.Linux.intel+cxx11.bnb' ]

        fileL = [ 'results.2016_02_10.Linux.intel.bnb',
                  'results.2016_02_10.Linux.gnu4+cxx11.dev',
                  'results.2016_02_11.Linux.intel+cxx11.bnb' ]
        optD = { '-o':['intel'], '-O':['cxx11'] }
        results.process_files( optD, fileL, None )
        assert fileL == ['results.2016_02_10.Linux.intel.bnb' ]

        fileL = [ 'results.2016_02_10.Linux.intel+dbg.bnb',
                  'results.2016_02_10.Linux.gnu4+dbg.dev',
                  'results.2016_02_11.Linux.intel+cxx11.bnb' ]
        optD = { '-O':['cxx11','gnu4'] }
        results.process_files( optD, fileL, None )
        assert fileL == ['results.2016_02_10.Linux.intel+dbg.bnb' ]

        # specifying tags
        fileL = [ 'results.2016_02_10.Linux.gnu4.bnb',
                  'results.2016_02_10.Linux.gnu4.dev',
                  'results.2016_02_11.Linux.gnu4.bnb' ]
        optD = { '-t':['bnb'] }
        results.process_files( optD, fileL, None )
        assert fileL == ['results.2016_02_10.Linux.gnu4.bnb',
                         'results.2016_02_11.Linux.gnu4.bnb' ]

        fileL = [ 'results.2016_02_10.Linux.gnu4.bnb',
                  'results.2016_02_10.Linux.gnu4.dev',
                  'results.2016_02_11.Linux.gnu4.bnb' ]
        optD = { '-T':['dev'] }
        results.process_files( optD, fileL, None )
        assert fileL == ['results.2016_02_10.Linux.gnu4.bnb',
                         'results.2016_02_11.Linux.gnu4.bnb' ]

        fileL = [ 'results.2016_02_10.Linux.gnu4.bnb',
                  'results.2016_02_10.Linux.gnu4.dev',
                  'results.2016_02_11.Linux.gnu4.longbnb' ]
        optD = { '-T':['dev'], '-t':['longbnb'] }
        results.process_files( optD, fileL, None )
        assert fileL == ['results.2016_02_11.Linux.gnu4.longbnb' ]

    def test_the_report_subcommand(self):
        ""
        os.mkdir( 'config' )  # force the test to use default plat & cplr

        util.writefile( "one/cat.xml", """
            <rtest name="cat">
              <execute> sleep 1 </execute>
            </rtest>""" )
        util.writefile( "one/dog.xml", """
            <rtest name="dog">
              <execute> set have_diff = yes </execute>
            </rtest>""" )
        util.writefile( "one/ferret.xml", """
            <rtest name="ferret">
              <execute> exit 1 </execute>
            </rtest>""" )
        time.sleep(1)

        vrun = util.runvvtest( '--config config' )
        vrun.assertCounts( total=3, npass=1, diff=1, fail=1 )

        # bootstrap with runtimes files then save the test results
        util.runcmd( util.resultspy + ' save', chdir='one' )
        util.runvvtest( '--config config -i --save-results' )

        rf1 = util.globfile('results.*')
        tr = results.TestResults( rf1 )
        platname,cplrname = tr.platform(), tr.compiler()
        L = rf1.split('.')
        rf2 = L[0]+'.'+L[1]+'.Fake.'+L[3]
        mach = os.uname()[1]
        tr.writeResults( rf2, 'Fake', L[3], mach, '/some/fake/path' )

        x,out = util.runcmd( util.resultspy + ' report ' + rf1 + ' ' + rf2 )

        assert len( util.grep( out, 'Fake.gcc' ) ) == 3
        assert len( util.grep( out, 'pass=1 ' ) ) == 2
        assert len( util.grep( out, 'diff=1 ' ) ) == 2
        assert len( util.grep( out, 'fail=1 ' ) ) == 2
        assert len( util.grep( out, 'one/dog' ) ) == 1
        assert len( util.grep( out, 'one/ferret' ) ) == 1

    def test_the_report_subcommand_with_G_option(self):
        ""
        util.writefile( "one/cat.xml", """
            <rtest name="cat">
              <execute> sleep 1 </execute>
            </rtest>""" )
        util.writefile( "one/dog.xml", """
            <rtest name="dog">
              <execute> set have_diff = yes </execute>
            </rtest>""" )
        util.writefile( "one/ferret.xml", """
            <rtest name="ferret">
              <execute> exit 1 </execute>
            </rtest>""" )
        time.sleep(1)

        vrun = util.runvvtest()
        vrun.assertCounts( total=3, npass=1, diff=1, fail=1 )

        # bootstrap with runtimes files then save the test results
        util.runcmd( util.resultspy + ' save', chdir='one' )
        util.runvvtest( '-i --save-results' )

        rf = util.globfile('results.*')

        x,out = util.runcmd( util.resultspy + ' report -G ' + rf )

        assert len( util.grep( out, 'pass=1 ' ) ) == 1
        assert len( util.grep( out, 'diff=1 ' ) ) == 1
        assert len( util.grep( out, 'fail=1 ' ) ) == 1
        assert len( util.grep( out, 'one/dog' ) ) == 0
        assert len( util.grep( out, 'one/ferret' ) ) == 0

        x,out = util.runcmd( util.resultspy + ' report -g ' + rf )

        assert len( util.grep( out, 'pass=1 ' ) ) == 1
        assert len( util.grep( out, 'diff=1 ' ) ) == 1
        assert len( util.grep( out, 'fail=1 ' ) ) == 1
        assert len( util.grep( out, 'one/dog' ) ) == 1
        assert len( util.grep( out, 'one/ferret' ) ) == 1

    def test_the_in_progress_results_file(self):
        ""
        util.writefile( "cat.xml", """
            <rtest name="cat">
              <execute> sleep 1 </execute>
            </rtest>""" )
        util.writefile( "dog.xml", """
            <rtest name="dog">
              <execute> sleep 2 </execute>
            </rtest>""" )
        util.writefile( "circle.xml", """
            <rtest name="circle">
              <execute> sleep 3 </execute>
            </rtest>""" )
        time.sleep(1)

        util.interrupt_vvtest_run( '--save-results', count=2 )
        tdir = os.path.abspath( util.results_dir() )

        resultsfname = util.globfile('results.*')

        x,out = util.runcmd( util.resultspy + ' report ' + resultsfname )

        tr = results.TestResults( resultsfname )
        assert tr.inProgress()

        for line in util.grep( out, ' [.]' ):
            if line.strip().startswith('.'):
                s = line.split('pass')[0].strip()
                assert s[-1] == 's'
                break

    def test_the_results_date_option(self):
        ""
        util.writefile( "cat.xml", """
            <rtest name="cat">
              <execute> sleep 1 </execute>
            </rtest>""" )
        util.writefile( "dog.xml", """
            <rtest name="dog">
              <execute> sleep 2 </execute>
            </rtest>""" )
        util.writefile( "circle.xml", """
            <rtest name="circle">
              <execute> sleep 3 </execute>
            </rtest>""" )
        time.sleep(1)

        olddate = time.time() - 2*24*60*60
        vrun = util.runvvtest(
            '--save-results --results-date ' + str(olddate) )
        vrun.assertCounts( total=3, npass=3 )

        rfile1 = util.globfile('results.*')

        vrun = util.runvvtest( '-w --save-results' )
        vrun.assertCounts( total=3, npass=3 )
        L = glob.glob('results.*')
        assert len(L) == 2
        L.sort()
        assert L[0] == rfile1
        assert L[1] != rfile1
        rfile2 = L[1]
        print3( 'results files:', rfile1, rfile2 )

        vrun = util.runvvtest(
            '-w --save-results --results-date 2016_07_15' )
        vrun.assertCounts( total=3, npass=3 )
        L = glob.glob('results.*')
        assert len(L) == 3
        L.sort()
        print3( 'results list', L )
        assert L[1] == rfile1
        assert L[2] == rfile2
        rfile3 = L[0]
        assert '2016_07_15' in rfile3
        assert rfile3 not in [ rfile1, rfile2 ]
        print3( 'results files:', rfile1, rfile2, rfile3 )

    def test_that_timeout_only_reported_if_two_or_more_times_in_a_row(self):
        ""
        util.writefile( "subdir/test1.xml", """
            <rtest name="test1">
              <timeout value="5"/>
              <execute> sleep 2 </execute>
            </rtest>""" )
        util.writefile( "subdir/test2.xml", """
            <rtest name="test2">
              <timeout value="10"/>
              <execute> sleep 1 </execute>
            </rtest>""" )
        time.sleep(1)

        vrun = util.runvvtest( 'subdir' )
        vrun.assertCounts( total=2, npass=2 )

        # bootstrap with runtimes files then save the test results
        util.runcmd( util.resultspy + ' save', chdir='subdir' )
        util.runvvtest( '-i --save-results' )

        # compute a new result date to be yesterday's date
        rf1 = util.globfile('results.*')
        s1,d,s2 = rf1.split('.',2)
        y,m,d = d.split('_')
        if d == '01':
            if m == '01':
                return  # I'm lazy
            m = int(m)-1
            if m < 10: m = '0'+str(m)
            else:      m = str(m)
            d = '28'
        else:
            d = int(d)-1
            if d < 10: d = '0'+str(d)
            else:      d = str(d)
        rf2 = s1+'.'+y+'_'+m+'_'+d+'.'+s2

        # rewrite the results file to a file with yesterday's date
        tr = results.TestResults( rf1 )
        platname,cplrname = tr.platform(), tr.compiler()
        mach,testdir = tr.machine(), tr.testdir()
        tr.writeResults( rf2, platname, cplrname, mach, testdir )

        # rerun the tests
        time.sleep(1)
        vrun = util.runvvtest( '-w --save-results subdir' )
        vrun.assertCounts( total=2, npass=2 )

        x,out = util.runcmd( util.resultspy + ' report ' + rf1 + ' ' + rf2 )
        assert len( util.grep( out, 'test1' ) ) == 0
        assert len( util.grep( out, 'test2' ) ) == 0

        # now make a test timeout
        util.writefile( "subdir/test1.xml", """
            <rtest name="test1">
              <timeout value="5"/>
              <execute> sleep 10 </execute>
            </rtest>""" )
        time.sleep(1)

        vrun = util.runvvtest( '-w --save-results subdir' )
        vrun.assertCounts( total=2, npass=1, timeout=1 )

        # the timed out test should not be reported
        x,out = util.runcmd( util.resultspy + ' report ' + rf1 + ' ' + rf2 )
        assert len( util.grep( out, 'test1' ) ) == 0
        assert len( util.grep( out, 'test2' ) ) == 0

        # rename the results with the timed out test to yesterday's date
        os.remove( rf2 )
        os.rename( rf1, rf2 )

        # rerun one more time, in which the test will timeout again
        vrun = util.runvvtest( '-w --save-results subdir' )
        vrun.assertCounts( total=2, npass=1, timeout=1 )

        # the timed out test should now be reported
        x,out = util.runcmd( util.resultspy + ' report ' + rf1 + ' ' + rf2 )
        assert len( util.grep( out, 'test1' ) ) == 1
        assert len( util.grep( out, 'test2' ) ) == 0


############################################################################

def write_tests_cat_dog_circle( in_subdir=None ):
    ""
    cwd = os.getcwd()

    if in_subdir:
        if not os.path.exists( in_subdir ):
            os.mkdir( in_subdir )
            time.sleep(1)

        os.chdir( in_subdir )

    try:
        write_test_cat()
        write_test_dog()
        write_test_circle()

    finally:
        if in_subdir:
            os.chdir( cwd )

def write_test_cat( sleep_time=1 ):
    ""
    util.writefile( "one/cat.xml", """
        <rtest name="cat">
          <execute>
            sleep """+str(sleep_time)+"""
          </execute>
        </rtest>""" )

def write_test_dog( sleep_time=2 ):
    ""
    util.writefile( "one/dog.xml", """
        <rtest name="dog">
          <execute>
            sleep """+str(sleep_time)+"""
          </execute>
        </rtest>""" )

def write_test_circle():
    ""
    util.writefile( "two/circle.xml", """
        <rtest name="circle">
          <execute>
            sleep 3
          </execute>
        </rtest>""" )


def create_and_save_runtimes():
    ""
    util.runcmd( util.resultspy + ' save' )

    # run vvtest to save the test results in TESTING_DIRECTORY
    util.runvvtest( '-i --save-results' )


############################################################################

util.run_test_cases( sys.argv, sys.modules[__name__] )
