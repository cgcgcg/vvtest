#!/usr/bin/env python

# Copyright 2018 National Technology & Engineering Solutions of Sandia, LLC
# (NTESS). Under the terms of Contract DE-NA0003525 with NTESS, the U.S.
# Government retains certain rights in this software.

#RUNTEST:

import sys
sys.dont_write_bytecode = True
sys.excepthook = sys.__excepthook__
import os
import time
import glob
import unittest

import testutils as util
from testutils import print3


class parameterize_analyze_logic( unittest.TestCase ):

    def setUp(self):
        util.setup_test()

    def test_analyze_with_dep_that_fails_then_passes(self):
        ""
        good_xml = """
            <rtest name="good">
              <keywords> fast medium </keywords>
              <parameterize timestep="1 2"/>
              <execute>
                touch afile.$timestep
              </execute>
              <analyze>
                 ls ../good.timestep=1/afile.1 || exit 1
                 ls ../good.timestep=2/afile.2 || exit 1
              </analyze>
            </rtest>"""
        bad_xml = """
            <rtest name="bad">
              <keywords> fast medium </keywords>
              <parameterize timestep="1 2"/>
              <execute>
                if ( "$timestep" == 2 ) then
                  echo "fake failure"
                  exit 1
                else
                  touch bfile.$timestep
                endif
              </execute>
              <analyze>
                 ls ../bad.timestep=1/bfile.1 || exit 1
                 ls ../bad.timestep=2/bfile.2 || exit 1
              </analyze>
            </rtest>"""
        fixed_xml = """
            <rtest name="bad">
              <keywords> fast medium </keywords>
              <parameterize timestep="1 2"/>
              <execute>
                touch bfile.$timestep
              </execute>
              <analyze>
                 ls ../bad.timestep=1/bfile.1 || exit 1
                 ls ../bad.timestep=2/bfile.2 || exit 1
              </analyze>
            </rtest>"""

        util.writefile( 'parentchild/good.xml', good_xml )
        time.sleep(1)

        for batch in [False,True]:

            util.remove_results()

            util.writefile( 'parentchild/bad.xml', bad_xml )
            time.sleep(1)

            vrun = util.runvvtest( '-k bad parentchild', batch=batch )
            vrun.assertCounts( total=3, npass=1, fail=1, notrun=1 )

            # "fix" the bad test
            util.writefile( 'parentchild/bad.xml', fixed_xml )
            time.sleep(1)

            vrun = util.runvvtest( '-R -k notdone/notrun/fail',
                                   '-k bad parentchild', batch=batch )
            vrun.assertCounts( total=2, npass=2 )

    def test_not_sure_what_this_tests(self):
        ""
        util.writefile( 'top/param_analyze.xml', """
            <rtest name="param_analyze">
              <keywords> fast medium </keywords>
              <parameterize keywords="fast"     np="1"/>
              <parameterize not_keywords="fast" np="2 4"/>
              <link_files> $NAME.inp </link_files>
              <execute>
                 touch execute_file.np=$np || exit 1
                 ls $NAME.inp || exit 1
              </execute>
              <analyze keywords="fast">
                 ls ../param_analyze.np=1/execute_file.np=1 || exit 1
              </analyze>
              <analyze not_keywords="fast">
                 ls ../param_analyze.np=2/execute_file.np=2 || exit 1
                 ls ../param_analyze.np=4/execute_file.np=4 || exit 1
              </analyze>
            </rtest>""" )

        util.writefile( 'top/param_analyze.inp', """
            some sort of input deck
            """ )

        util.writefile( 'parentchild/good.xml', """
            <rtest name="good">
              <keywords> fast medium </keywords>
              <parameterize timestep="1 2"/>
              <execute>
                touch afile.$timestep
              </execute>
              <analyze>
                 ls ../good.timestep=1/afile.1 || exit 1
                 ls ../good.timestep=2/afile.2 || exit 1
              </analyze>
            </rtest>""" )

        bad_xml = """
            <rtest name="bad">
              <keywords> fast medium </keywords>
              <parameterize timestep="1 2"/>
              <execute>
                echo "fake failure"
                exit 1
              </execute>
              <analyze>
                 ls ../bad.timestep=1/bfile.1 || exit 1
                 ls ../bad.timestep=2/bfile.2 || exit 1
              </analyze>
            </rtest>"""

        fix_bad_xml = """
            <rtest name="bad">
              <keywords> fast medium </keywords>
              <parameterize timestep="1 2"/>
              <execute>
                touch bfile.$timestep
              </execute>
              <analyze>
                 ls ../bad.timestep=1/bfile.1 || exit 1
                 ls ../bad.timestep=2/bfile.2 || exit 1
              </analyze>
            </rtest>"""

        time.sleep(1)

        for batch in [False,True]:

            util.remove_results()

            util.writefile( 'parentchild/bad.xml', bad_xml )
            time.sleep(1)

            vrun = util.runvvtest( 'parentchild', batch=batch )
            vrun.assertCounts( total=6, npass=3, fail=2, notrun=1 )

            # "fix" the bad test
            util.writefile( 'parentchild/bad.xml', fix_bad_xml )
            time.sleep(1)

            vrun = util.runvvtest( '-R -k notdone/notrun/fail',
                                   'parentchild', batch=batch )
            vrun.assertCounts( total=3, npass=3 )

    def test_a_test_with_analyze_must_have_parameterize(self):
        ""
        util.writefile( 'noparam.xml', """
            <rtest name="noparam">
              <keywords> fast medium </keywords>
              <execute> echo "exec standard" </execute>
              <analyze>
                echo "exec analyze"
              </analyze>
            </rtest>""" )
        time.sleep(1)

        vrun = util.runvvtest()
        vrun.assertCounts( total=0 )
        assert vrun.countLines( 'skipping *noparam' ) == 1

    def test_analyze_with_multiple_tests_in_one_file(self):
        """
        using testname filter with analyze
        """
        util.writefile( 'multi.xml', """
            <rtest name="multi">
              <rtest name="multi2"/>
              <parameterize param="one two"/>
              <execute> echo "exec test $NAME param $param" </execute>
              <analyze testname="multi">
                echo "analyze test $NAME (multi)"
              </analyze>
              <analyze testname="multi2">
                echo "analyze test $NAME (multi2)"
              </analyze>
            </rtest>""" )
        time.sleep(1)

        for batch in [False,True]:

            util.remove_results()

            vrun = util.runvvtest( batch=batch )
            vrun.assertCounts( total=6, npass=6 )

            assert len( util.grepfiles( 'exec test multi ',
                        'TestResults.*/multi.param=one/execute.log' ) ) >= 1
            assert len( util.grepfiles( 'exec test multi ',
                        'TestResults.*/multi.param=two/execute.log' ) ) >= 1
            assert len( util.grepfiles( 'analyze test multi ',
                        'TestResults.*/multi/execute.log' ) ) >= 1

            assert len( util.grepfiles( 'exec test multi2 ',
                        'TestResults.*/multi2.param=one/execute.log' ) ) >= 1
            assert len( util.grepfiles( 'exec test multi2 ',
                        'TestResults.*/multi2.param=two/execute.log' ) ) >= 1
            assert len( util.grepfiles( 'analyze test multi2 ',
                        'TestResults.*/multi2/execute.log' ) ) >= 1

    def test_mutiliple_testnames_only_one_with_analyze(self):
        """
        testname filter with analyze where one test does not have an analyze
        """
        util.writefile( 'multi.xml', """
            <rtest name="multi">
              <rtest name="multi2"/>
              <parameterize param="one two"/>
              <execute> echo "exec test $NAME param $param" </execute>
              <analyze testname="multi2">
                echo "analyze test $NAME (multi2)"
              </analyze>
            </rtest>""" )
        time.sleep(1)

        for batch in [False,True]:

            util.remove_results()

            vrun = util.runvvtest( batch=batch )
            vrun.assertCounts( total=5, npass=5 )

            assert len( util.grepfiles( 'exec test multi ',
                        'TestResults.*/multi.param=one/execute.log' ) ) >= 1
            assert len( util.grepfiles( 'exec test multi ',
                        'TestResults.*/multi.param=two/execute.log' ) ) >= 1
            assert len( glob.glob( 'TestResults.*/multi/execute.log' ) ) == 0

            assert len( util.grepfiles( 'exec test multi2 ',
                        'TestResults.*/multi2.param=one/execute.log' ) ) >= 1
            assert len( util.grepfiles( 'exec test multi2 ',
                        'TestResults.*/multi2.param=two/execute.log' ) ) >= 1
            assert len( util.grepfiles( 'analyze test multi2 ',
                        'TestResults.*/multi2/execute.log' ) ) >= 1

    def test_using_analyze_command_line_option(self):
        ""    
        util.writefile( 'atest.xml', """
            <rtest name="atest">
              <parameterize ival="1 2"/>
              <execute>
                echo "running touch"
                touch afile.$ival
              </execute>
              <execute analyze="yes">
                echo "running execute analyze"
              </execute>
              <analyze>
                echo "running analyze"
                ls ../atest.ival=1/afile.1 || exit 1
                ls ../atest.ival=2/afile.2 || exit 1
              </analyze>
            </rtest>""" )

        for batch in [False,True]:

            util.remove_results()

            vrun = util.runvvtest( batch=batch )
            vrun.assertCounts( total=3, npass=3 )

            assert len( util.grepfiles( 'running touch',
                        'TestResults.*/atest.ival=1/execute.log' ) ) == 1
            assert len( util.grepfiles( 'running execute analyze',
                        'TestResults.*/atest.ival=1/execute.log' ) ) == 1
            assert len( util.grepfiles( 'running touch',
                        'TestResults.*/atest.ival=2/execute.log' ) ) == 1
            assert len( util.grepfiles( 'running execute analyze',
                        'TestResults.*/atest.ival=2/execute.log' ) ) == 1
            assert len( util.grepfiles( 'running analyze',
                        'TestResults.*/atest/execute.log' ) ) == 1
            assert len( util.grepfiles( 'running touch',
                        'TestResults.*/atest/execute.log' ) ) == 0

            vrun = util.runvvtest( '-Ra', batch=batch )
            vrun.assertCounts( total=3, npass=3 )

            assert len( util.grepfiles( 'running touch',
                        'TestResults.*/atest.ival=1/execute.log' ) ) == 0
            assert len( util.grepfiles( 'running execute analyze',
                        'TestResults.*/atest.ival=1/execute.log' ) ) == 1
            assert len( util.grepfiles( 'running touch',
                        'TestResults.*/atest.ival=2/execute.log' ) ) == 0
            assert len( util.grepfiles( 'running execute analyze',
                        'TestResults.*/atest.ival=2/execute.log' ) ) == 1
            assert len( util.grepfiles( 'running analyze',
                        'TestResults.*/atest/execute.log' ) ) == 1
            assert len( util.grepfiles( 'running touch',
                        'TestResults.*/atest/execute.log' ) ) == 0


########################################################################

util.run_test_cases( sys.argv, sys.modules[__name__] )
