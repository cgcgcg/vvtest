#!/usr/bin/env python
#RUNTEST:

import sys
sys.dont_write_bytecode = True
sys.excepthook = sys.__excepthook__
import os
import time
import unittest

import testutils as util
from testutils import print3


class parameterize_analyze_dependency_handling( unittest.TestCase ):

    def setUp(self):
        util.setup_test()

    def test_analyze_tests_dont_wait_until_end_to_run(self):
        ""
        util.writefile( 'test1.xml', """
            <rtest name="test1">
              <parameterize timestep="1 2"/>
              <execute>
                sleep 1
                touch afile.$timestep
              </execute>
              <analyze>
                 ls ../test1.timestep=1/afile.1 || exit 1
                 ls ../test1.timestep=2/afile.2 || exit 1
              </analyze>
            </rtest>""" )
        util.writefile( 'test2.xml', """
            <rtest name="test2">
              <execute>
                sleep 10
                echo "that was a nice nap"
              </execute>
            </rtest>""" )
        time.sleep(1)

        self.run_analyze_tests_dont_wait_until_end_to_run( '' )
        self.run_analyze_tests_dont_wait_until_end_to_run(
                '--plat '+util.core_platform_name()+' --batch --qsub-limit 5' )

    def run_analyze_tests_dont_wait_until_end_to_run(self, opt):
        ""
        util.remove_results()

        out,np,nd,nf,nn = util.run_vvtest( opt+' -n 4 --sort x' )
        assert np == 4 and nd == 0 and nf == 0 and nn == 0
        tdir = util.results_dir()
        platname = util.platform_name( out )

        tL = util.testtimes( out )
        assert len(tL) == 4
        assert tL[0][0] == tdir+'/test1'
        assert tL[1][0] == tdir+'/test1.timestep=1'
        assert tL[2][0] == tdir+'/test1.timestep=2'
        assert tL[3][0] == tdir+'/test2'
        assert tL[0][1] >= tL[1][2] and tL[0][1] >= tL[2][2]
        assert tL[0][1] < tL[3][2]

    def test_analyze_tests_dont_run_child_failed_previously(self):
        ""
        self.run_analyze_tests_dont_run_child_failed_previously( '' )
        self.run_analyze_tests_dont_run_child_failed_previously(
                '--plat '+util.core_platform_name()+' --batch' )

    def run_analyze_tests_dont_run_child_failed_previously(self, opt):
        ""
        util.remove_results()

        util.writefile( 'atest.xml', """
            <rtest name="atest">
              <parameterize timestep="1 2"/>
              <execute>
                if ( "$timestep" == 2 ) then
                  echo "fake failure"
                  exit 1
                else
                  touch afile.$timestep
                endif
              </execute>
              <analyze>
                 ls ../atest.timestep=1/afile.1 || exit 1
                 ls ../atest.timestep=2/afile.2 || exit 1
              </analyze>
            </rtest>""" )
        time.sleep(1)

        # the analyze should not run because a child fails
        out,np,nd,nf,nn = util.run_vvtest( opt )
        assert np == 1 and nd == 0 and nf == 1 and nn == 1
        tdir = util.results_dir()
        platname = util.platform_name( out )

        # the analyze should not run here
        out,np,nd,nf,nn = util.run_vvtest( opt )
        assert np == 0 and nd == 0 and nf == 0 and nn == 0

        # double check the state
        out,np,nd,nf,nn = util.run_vvtest( "--plat "+platname+" -i" )
        assert np == 1 and nd == 0 and nf == 1 and nn == 1

        # "fix" the failure
        util.writefile( 'atest.xml', """
            <rtest name="atest">
              <parameterize timestep="1 2"/>
              <execute>
                touch afile.$timestep
              </execute>
              <analyze>
                 ls ../atest.timestep=1/afile.1 || exit 1
                 ls ../atest.timestep=2/afile.2 || exit 1
              </analyze>
            </rtest>""" )
        time.sleep(1)

        # rerun the failure
        out,np,nd,nf,nn = util.run_vvtest( opt+' -k fail' )
        assert np == 1 and nd == 0 and nf == 0 and nn == 0

        # now the analyze should run
        out,np,nd,nf,nn = util.run_vvtest( opt )
        assert np == 1 and nd == 0 and nf == 0 and nn == 0

        out,np,nd,nf,nn = util.run_vvtest( "--plat "+platname+" -i" )
        assert np == 3 and nd == 0 and nf == 0 and nn == 0


    def test_rerun_with_analyze_deps(self):
        ""
        self.run_rerun_with_analyze_deps( '' )
        self.run_rerun_with_analyze_deps(
                '--plat '+util.core_platform_name() + \
                ' --batch --qsub-length 1 --qsub-limit 5' )

    def run_rerun_with_analyze_deps(self, opt):
        ""
        util.remove_results()

        util.writefile( 'atest.xml', """
            <rtest name="atest">
              <parameterize timestep="1 2"/>
              <execute>
                sleep 5
                touch afile.$timestep
              </execute>
              <analyze>
                 ls ../atest.timestep=1/bfile.1 || exit 1
                 ls ../atest.timestep=2/bfile.2 || exit 1
              </analyze>
            </rtest>""" )
        time.sleep(1)
        
        # the analyze should fail
        out,np,nd,nf,nn = util.run_vvtest( opt+' -n 3' )
        assert np == 2 and nd == 0 and nf == 1 and nn == 0
        tdir = util.results_dir()
        platname = util.platform_name( out )

        # "fix" the execute
        util.writefile( 'atest.xml', """
            <rtest name="atest">
              <parameterize timestep="1 2"/>
              <execute>
                sleep 5
                touch bfile.$timestep
              </execute>
              <analyze>
                 ls ../atest.timestep=1/bfile.1 || exit 1
                 ls ../atest.timestep=2/bfile.2 || exit 1
              </analyze>
            </rtest>""" )
        time.sleep(1)
        
        # the analyze test should NOT run at the same time as the children
        # (if it does in this case, then the analyze test will show a fail)
        out,np,nd,nf,nn = util.run_vvtest( opt+' -R -n 3' )
        assert np == 3 and nd == 0 and nf == 0 and nn == 0

    def test_analyze_waits_for_deps_with_limited_num_procs(self):
        """
        make sure if a child test takes more processors than available, then
        the analyze will still wait for all the children to finish
        """
        util.writefile( 'atest.xml', """
            <rtest name="atest">
              <parameterize np="1 1 2 2" foo="bar baz bar baz"/>
              <execute>
                sleep 4
                touch afile.$foo.$np
              </execute>
              <analyze>
                 ls ../atest.foo=bar.np=1/afile.bar.1 || exit 1
                 ls ../atest.foo=baz.np=1/afile.baz.1 || exit 1
                 ls ../atest.foo=bar.np=2/afile.bar.2 || exit 1
                 ls ../atest.foo=baz.np=2/afile.baz.2 || exit 1
              </analyze>
            </rtest>""" )
        time.sleep(1)

        out,np,nd,nf,nn = util.run_vvtest( '-n 1' )
        assert np == 5 and nd == 0 and nf == 0 and nn == 0

    def test_failed_analyze_deps_are_still_listed_with_info_option(self):
        """
        analyze tests that have bad children are not pruned when using -i
        """
        util.writefile( 'bad.xml', """
            <rtest name="bad">
              <keywords> fast medium </keywords>
              <parameterize timestep="1 2"/>
              <execute>
                if ( "$timestep" == 2 ) then
                  echo "fake failure"
                  exit 1
                else
                  touch bfile.$timestep
                endif
              </execute>
              <analyze>
                 ls ../bad.timestep=1/bfile.1 || exit 1
                 ls ../bad.timestep=2/bfile.2 || exit 1
              </analyze>
            </rtest>""" )
        time.sleep(1)

        # the analyze should be run here
        out,np,nd,nf,nn = util.run_vvtest()
        assert np == 1 and nd == 0 and nf == 1 and nn == 1

        # the analyze should show up in the listing here
        out,np,nd,nf,nn = util.run_vvtest( '-i -k notrun/pass' )
        assert np == 1 and nd == 0 and nf == 0 and nn == 1


########################################################################

util.run_test_cases( sys.argv, sys.modules[__name__] )
