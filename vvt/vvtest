#!/usr/bin/env python

# Copyright 2018 National Technology & Engineering Solutions of Sandia, LLC
# (NTESS). Under the terms of Contract DE-NA0003525 with NTESS, the U.S.
# Government retains certain rights in this software.

import sys
sys.dont_write_bytecode = True
sys.excepthook = sys.__excepthook__
import os
import re
import stat
import time
import signal
import shutil
import types
import glob
import shlex
import string
import random

import libvvtest.cmdline as cmdline
import libvvtest.vvplatform as vvplatform
import libvvtest.misc as misc
import libvvtest.TestSpec as TestSpec
import libvvtest.TestExec as TestExec
import libvvtest.TestList as TestList
import libvvtest.batchutils as batchutils
import libvvtest.TestSpecCreator as TestSpecCreator
import libvvtest.FilterExpressions as FilterExpressions
from libvvtest.RuntimeConfig import RuntimeConfig
import results
from libvvtest.PermissionSetter import DummyPermissionSetter
from libvvtest.PermissionSetter import PermissionSetter
import libvvtest.testlistio as testlistio
import libvvtest.utesthooks as utesthooks


version = '1.1'

search_fnmatch = ['*.inp','*.apr','*.i']

testlist_name = 'testlist'

# a FilterExpressions.WordExpression() object filled with the command line
# -k and -K specifications
keyword_expr = None


class Configuration:
    
    defaults = { \
                 'toolsdir':None,  # the top level tools directory
                 'configdir':None,  # the configuration directory
                 'exepath':None,  # the path to the executables
                 'onopts':[],
                 'offopts':[],
                 'refresh':1,
                 'postclean':0,
                 'timeout':None,
                 'multiplier':1.0,
                 'preclean':1,
                 'analyze':0,
                 'logfile':1,
                 'testargs':[],
               }
    
    def get(self, name):
        """
        """
        return self.attrs[name]
    
    def set(self, name, value):
        """
        """
        if value == None:
          self.attrs[name] = Configuration.defaults[name]
        else:
          self.attrs[name] = value
    
    def __init__(self):
        self.attrs = {}
        for n,v in Configuration.defaults.items():
          self.attrs[n] = v


config = Configuration()


def populate_configuration_from_options( opts, optD ):
    ""
    if optD['onopts']:
        config.set( 'onopts', optD['onopts'] )
    if optD['offopts']:
        config.set( 'offopts', optD['offopts'] )

    if opts.bin_dir:
        config.set( 'exepath', opts.bin_dir )

    if opts.configpath:
        config.set( 'configdir', opts.configpath[-1] )
    else:
        d = os.getenv( 'VVTEST_CONFIGDIR' )
        if d == None:
            d = os.path.join( config.get('toolsdir'), 'config' )
        config.set( 'configdir', os.path.abspath(d) )

    config.set( 'refresh', not opts.nopreclean )
    config.set( 'postclean', opts.postclean == True )

    if opts.timeout != None:
        config.set( 'timeout', opts.timeout )
    if opts.multiplier != None:
        config.set( 'multiplier', opts.multiplier )

    config.set( 'preclean', not opts.nopreclean )
    config.set( 'analyze', opts.analyze == True )
    config.set( 'logfile', not opts.nologs )

    if opts.test_args:
        argL = []
        for args in opts.test_args:
            argL.extend( shlex.split( args ) )
        config.set( 'testargs', argL )


def main():
    ""
    # get directory containing this script and the script name itself
    toolsdir = getToolsDirectory()

    check_for_bootstrap_file()

    # set this before options are parsed
    config.set( 'toolsdir', toolsdir )

    # this calls sys.exit if help is requested
    opts,optD,dirs = cmdline.parse_command_line( sys.argv[1:], version )

    global keyword_expr
    keyword_expr = optD['keyword_expr']

    populate_configuration_from_options( opts, optD )

    # do this after options are parsed
    insert_configdir_into_sys_path( toolsdir, config )

    # 'test_dir' non-None only if the CWD is in a TestResults.* directory
    test_dir = readCommandInfo( opts, optD )

    if opts.checkname:
        for n in opts.checkname:
            os.environ[ 'CHECK_' + n.upper() ] = ''

    if test_dir and optD['param_dict']:
        print3( "*** error: cannot use -S in a TestResults directory" )
        sys.exit(1)

    if test_dir and opts.generate:
        print3( "*** error: cannot use -g in a TestResults directory" )
        sys.exit(1)

    plat = construct_platform_instance( toolsdir, opts, optD )

    testsubdir = test_results_subdir_name( opts.run_dir,
                                           optD['onopts'],
                                           optD['offopts'],
                                           plat.getName() )

    rtconfig = construct_RuntimeConfig( plat, opts, optD )

    info = ( opts.info or opts.keys or opts.files )

    if opts.generate and not info:
        assert test_dir == None
        generateTestList( opts, optD, dirs, plat, rtconfig, testsubdir )

    elif info:
        print_info_mode( opts, optD, plat, rtconfig,
                         dirs, test_dir, testsubdir )

    elif opts.baseline:

        if opts.rerun or opts.wipe:
            print3( "*** error: cannot use -R or -w with -b (baseline)" )
            sys.exit(1)

        baselineTests( opts, optD, test_dir, plat, rtconfig, testsubdir )

    elif opts.extract_dir:
        extractTestFiles( opts, optD, dirs, plat, opts.extract_dir, rtconfig )

    else:

        # if no results keywords are specified, then add -k notrun/notdone
        if not keyword_expr.containsResultsKeywords() and \
           not opts.wipe and not opts.rerun:
            keyword_expr.append( ['notrun/notdone'], 'and' )

        if test_dir != None:
            restartTests( opts, optD, toolsdir, test_dir, plat, rtconfig )
        else:
            runTests( opts, optD, toolsdir, dirs, plat, rtconfig, testsubdir )


def construct_platform_instance( toolsdir, opts, optD ):
    ""
    plat = vvplatform.create_Platform_instance(
                toolsdir,
                opts.platname,         # --plat
                optD['platopt_dict'],  # --platopt
                opts.useenv,           # -e
                opts.numprocs,         # -n
                opts.maxprocs,         # -N
                optD['onopts'],        # -o
                optD['offopts'],       # -O
                opts.qsub_id )         # --qsub-id

    return plat


def construct_RuntimeConfig( plat, opts, optD ):
    ""
    rtconfig = RuntimeConfig( \
                  param_expr_list=optD['param_list'],
                  keyword_expr=keyword_expr,
                  option_list=( optD['onopts'] + [plat.getCompiler()] ),
                  platform_name=plat.getName(),
                  ignore_platforms=( opts.ignore_platforms == True ),
                  set_platform_expr=optD['platform_expr'],
                  search_file_globs=search_fnmatch,
                  search_regexes=optD['search_regexes'],
                  include_tdd=( opts.include_tdd == True ),
                  runtime_range=[ opts.tmin, opts.tmax ],
                  runtime_sum=opts.tsum )

    if opts.qsub_id != None:
        rtconfig.setAttr( 'include_all', True )

    return rtconfig


def print_info_mode( opts, optD, plat, rtconfig,
                     scan_dirs, test_dir, testsubdir ):
    ""
    tlist = construct_info_test_list( opts, optD, plat, rtconfig, scan_dirs,
                                      test_dir, testsubdir )

    if not test_dir and testsubdir:
        test_dir = os.path.abspath( testsubdir )

    if opts.keys or opts.files:
        keywordInformation( tlist, opts.keys, opts.files )

    elif opts.save_results:
        saveResults( opts, optD, tlist, plat, test_dir )

    else:
        if opts.html or opts.junit_filename:
            do_stdout = False
        else:
            do_stdout = True

        printResults( tlist, test_dir, None,
                      optD['sort_letters'], opts.results_date,
                      tostdout=do_stdout,
                      tohtml=opts.html,
                      tojunit=opts.junit_filename )


def construct_info_test_list( opts, optD, plat, rtconfig, scan_dirs,
                              test_dir, testsubdir ):
    ""
    # always include tdd in info mode
    rtconfig.setAttr( 'include_tdd', True )

    if test_dir != None:
        tfile = os.path.join( test_dir, testlist_name )
        tlist = TestList.TestList( tfile, rtconfig )
        tlist.readTestList()
        tlist.readTestResults()
        tlist.loadAndFilter( plat.getMaxProcs(),
                    filter_dir=computeRelativePath( test_dir, os.getcwd() ) )

    elif os.path.exists( testsubdir ):
        test_dir = os.path.abspath( testsubdir )
        tfile = os.path.join( test_dir, testlist_name )
        tlist = TestList.TestList( tfile, rtconfig )
        tlist.readTestList()
        tlist.readTestResults()
        tlist.loadAndFilter( plat.getMaxProcs() )

    elif opts.keys or opts.files:
        tlist = TestList.TestList( None, rtconfig )
        scan_test_source_directories( tlist, scan_dirs, optD['param_dict'] )
        tlist.loadAndFilter( plat.getMaxProcs(), prune=True )

    else:
        print3( "*** warning: previous TestResults directory not found" )
        tlist = TestList.TestList( None, rtconfig )
        tlist.loadAndFilter( plat.getMaxProcs(), prune=True )

    return tlist


############################################################################

def XstatusString( t, add_date, test_dir, cwd ):
    """
    Returns a formatted string containing the job and its status.  If the
    add_date argument is true, it adds the date the job finished to the
    returned string.
    """
    
    if isinstance(t, TestExec.TestExec):
      ref = t.atest
      s = "%-20s " % ref.getName()
    else:
      ref = t
      s = "%-20s " % t.getName()
    
    state = ref.getAttr('state')
    if state != "notrun":
      
      if state == "done":
        result = ref.getAttr('result')
        if result == 'diff':
          s = s + "%-7s %-8s" % ("Exit", "diff")
        elif result ==  'pass':
          s = s + "%-7s %-8s" % ("Exit", "pass")
        elif result == "timeout":
          s = s + "%-7s %-8s" % ("TimeOut", 'SIGINT')
        else:
          s = s + "%-7s %-8s" % ("Exit", "fail(1)")
        
        xtime = ref.getAttr('xtime')
        if xtime >= 0: s = s + (" %-4s" % (str(xtime)+'s'))
        else:          s = s + "     "
      else:
        s = s + "%-7s %-8s     " % ("Running", "")
    
    else:
      s = s + "%-7s %-8s     " % ("NotRun", "")
    
    if add_date:
      xdate = ref.getAttr('xdate')
      if xdate > 0:
        s = s + time.strftime( " %m/%d %H:%M:%S", time.localtime(xdate) )
      else:
        s = s + "               "
    
    s += ' ' + relative_execute_directory( ref, test_dir, cwd )
    
    return s


def XstatusResult(t):
    
    if isinstance(t, TestExec.TestExec): ref = t.atest
    else:                        ref = t
    
    state = ref.getAttr('state')
    if state == "notrun" or state == "notdone":
      return state
    
    return ref.getAttr('result')


def relative_execute_directory( tst, testdir, cwd ):
    """
    Returns the test execute directory relative to the given current working
    directory.
    """
    xdir = tst.getExecuteDirectory()

    if testdir == None:
      return xdir
    
    d = os.path.join( testdir, xdir )
    sdir = issubdir( cwd, d )
    if sdir == None or sdir == "":
        return os.path.basename( xdir )
    
    return sdir


##############################################################################
#
# generation of tests


def scan_test_source_directories( tlist, scan_dirs, setparams ):
    ""
    # default scan directory is the current working directory
    if len(scan_dirs) == 0:
        scan_dirs = ['.']

    for d in scan_dirs:
        if not os.path.exists(d):
            sys.stderr.write(
                '*** error: directory does not exist: ' + str(d) + '\n')
            sys.exit(1);
        tlist.scanDirectory( d, setparams )


def generateTestList( opts, optD, dirs, plat, rtconfig, testsubdir ):
    """
    """
    test_dir = os.path.abspath( testsubdir )

    tfile = os.path.join( test_dir, testlist_name )

    tlist = TestList.TestList( tfile, rtconfig )

    scan_test_source_directories( tlist, dirs, optD['param_dict'] )

    loadRunTimes( tlist, plat,
                  opts.timeout, opts.multiplier, opts.maxtimeout )

    tlist.loadAndFilter( plat.getMaxProcs() )
    
    perms = make_PermissionSetter( test_dir, opts.perms )
    
    createTestDir( testsubdir, perms, opts.mirror_dir )
    writeCommandInfo( opts, optD, test_dir, plat, perms )
    printResults( tlist, test_dir, perms,
                  optD['sort_letters'], opts.results_date,
                  add_date=False )
    tlist.createTestExecs( test_dir, plat, config, perms )

    tlist.stringFileWrite()

    perms.set( os.path.abspath( tfile ) )
    
    print3( "Test directory:", testsubdir )


def extractTestFiles( opts, optD, dirs, plat, target_dir, rtconfig ):
    """
    Uses all the regular filtering mechanisms to gather tests from a test
    source area and copies the files used for each test into a separate
    directory.
    """
    tlist = TestList.TestList( None, rtconfig )

    scan_test_source_directories( tlist, dirs, optD['param_dict'] )

    loadRunTimes( tlist, plat,
                  opts.timeout, opts.multiplier, opts.maxtimeout )

    tlist.loadAndFilter( plat.getMaxProcs() )
    
    if not os.path.isabs(target_dir):
      target_dir = os.path.abspath(target_dir)
    if not os.path.exists(target_dir):
      os.makedirs( target_dir )
    
    uniqD = {}
    
    def wvisit( arg, dname, dirs, files ):
        """
        copy a directory tree, but leave out version control files
        """
        for n in ['CVS','.cvsignore','.svn','.git','.gitignore']:
            while (n in dirs): dirs.remove(n)
            while (n in files): files.remove(n)
        fd = os.path.normpath( os.path.join( arg[0], dname ) )
        td = os.path.normpath( os.path.join( arg[1], dname ) )
        if not os.path.exists(td):
            os.makedirs(td)
        for f1 in files:
            f2 = os.path.join(fd,f1)
            tf = os.path.join(td,f1)
            shutil.copy2( f2, tf )
    
    for xdir,t in tlist.active.items():
      
      tname = t.getName()
      T = (tname, t.getFilename())
      
      from_dir = os.path.dirname( t.getFilename() )
      p = os.path.dirname( t.getFilepath() )
      if p: to_dir = os.path.normpath( os.path.join( target_dir, p ) )
      else: to_dir = target_dir
      
      if not os.path.exists( to_dir ):
        os.makedirs( to_dir )
      tof = os.path.join( target_dir, t.getFilepath() )
      
      if tof not in uniqD:
        uniqD[tof] = None
        try: shutil.copy2( t.getFilename(), tof )
        except IOError: pass
      
      for srcf in t.getSourceFiles():

        if os.path.exists( os.path.join( from_dir, srcf ) ):
            fL = [ srcf ]
        else:
            cwd = os.getcwd()
            try:
                os.chdir( from_dir )
                fL = glob.glob( srcf )
            except:
                fL = []
            os.chdir( cwd )

        for f in fL:
            fromf = os.path.join( from_dir, f )
            tof = os.path.join( to_dir, f )
            tod = os.path.dirname(tof)
            if tof not in uniqD:
              uniqD[tof] = None
              if not os.path.exists(tod):
                os.makedirs(tod)
              
              if os.path.isdir(fromf):
                cwd = os.getcwd()
                os.chdir(fromf)
                for root,dirs,files in os.walk( '.' ):
                    wvisit( (fromf, tof), root, dirs, files )
                os.chdir(cwd)
                
              else:
                try: shutil.copy2( fromf, tof )
                except IOError: pass


##############################################################################

def keywordInformation( tlist, optkeys, optfiles ):
    
    if optkeys:
      print3( "\nresults keywords: " + ' '.join( TestSpec.results_keywords ) )
      kd = {}
      for t in tlist.getActiveTests():
        for k in t.getKeywords():
          if k not in TestSpec.results_keywords and k != t.getName():
            kd[k] = None
      L = list( kd.keys() )
      L.sort()
      print3( "\ntest keywords: " )
      while len(L) > 0:
        k1 = L.pop(0)
        if len(L) > 0: k2 = L.pop(0)
        else:          k2 = ''
        if len(L) > 0: k3 = L.pop(0)
        else:          k3 = ''
        print3( "  %-20s %-20s %-20s" % (k1,k2,k3) )
    else:
      assert optfiles
      D = {}
      for t in tlist.getActiveTests():
        d = os.path.normpath( t.getFilename() )
        D[d] = None
      L = list( D.keys() )
      L.sort()
      for d in L:
        print3( d )

##############################################################################


def test_results_subdir_name( rundir, onopts, offopts, platform_name ):
    """
    Generates and returns the subdirectory name to hold test results, which is
    unique up to the platform and on/off options.
    """
    if rundir:
        testdirname = rundir

    else:
        testdirname = 'TestResults.' + platform_name
        if onopts and len(onopts) > 0:
          testdirname += '.ON=' + '_'.join( onopts )
        if offopts and len(offopts) > 0:
          testdirname += '.OFF=' + '_'.join( offopts )
    
    return testdirname


def createTestDir( testdirname, perms, mirdir ):
    """
    Create the given directory name.  If -M is given in the command line
    options, then a mirror directory is created and 'testdirname' will be
    created as a soft link pointing to the mirror directory.
    """
    if mirdir and makeMirrorDirectory( mirdir, testdirname, perms ):
        pass

    else:
        if os.path.exists( testdirname ):
            if not os.path.isdir( testdirname ):
                # replace regular file with a directory
                os.remove( testdirname )
                os.mkdir( testdirname )
        else:
            if os.path.islink( testdirname ):
                os.remove( testdirname )  # remove broken softlink
            os.mkdir( testdirname )

        perms.set( os.path.abspath( testdirname ) )


def makeMirrorDirectory( Mval, testdirname, perms ):
    """
    Create a directory in another location then soft link 'testdirname' to it.
    Returns False only if 'Mval' is the word "any" and a suitable scratch
    directory could not be found.
    """
    assert testdirname == os.path.basename( testdirname )

    if Mval == 'any':
      
        usr = getUserName()
        for d in ['/var/scratch', '/scratch', '/var/scratch1', '/scratch1', \
                  '/var/scratch2', '/scratch2', '/var/scrl1', '/gpfs1']:
            if os.path.exists(d) and os.path.isdir(d):
                ud = os.path.join( d, usr )
                if os.path.exists(ud):
                    if os.path.isdir(ud) and \
                       os.access( ud, os.X_OK ) and os.access( ud, os.W_OK ):
                        Mval = ud
                        break
                elif os.access( d, os.X_OK ) and os.access( d, os.W_OK ):
                    try:
                        os.mkdir(ud)
                    except:
                        pass
                    else:
                        Mval = ud
                        break
        
        if Mval == 'any':
            return False  # a scratch dir could not be found
        
        # include the current directory name in the mirror location
        curdir = os.path.basename( os.getcwd() )
        Mval = os.path.join( Mval, curdir )

        if not os.path.exists( Mval ):
            os.mkdir( Mval )

    else:
        Mval = os.path.abspath( Mval )
    
    if not os.path.exists( Mval ) or not os.path.isdir( Mval ) or \
       not os.access( Mval, os.X_OK ) or not os.access( Mval, os.W_OK ):
        raise Exception( "invalid or non-existent mirror directory: "+Mval )

    if os.path.samefile( Mval, os.getcwd() ):
        raise Exception( "mirror directory and current working directory " + \
                "are the same: "+Mval+' == '+os.getcwd() )

    mirdir = os.path.join( Mval, testdirname )

    if os.path.exists( mirdir ):
        if not os.path.isdir( mirdir ):
            # replace regular file with a directory
            os.remove( mirdir )
            os.mkdir( mirdir )
    else:
        if os.path.islink( mirdir ):
            os.remove( mirdir )  # remove broken softlink
        os.mkdir( mirdir )
    
    perms.set( os.path.abspath( mirdir ) )

    if os.path.islink( testdirname ):
        path = os.readlink( testdirname )
        if path != mirdir:
            os.remove( testdirname )
            os.symlink( mirdir, testdirname )

    else:
        if os.path.exists( testdirname ):
            if os.path.isdir( testdirname ):
                shutil.rmtree( testdirname )
            else:
                os.remove( testdirname )
        os.symlink( mirdir, testdirname )
    
    return True


def writeCommandInfo( opts, optD, test_dir, plat, perms ):
    """
    Creates the test results information file.
    """
    f = os.path.join(test_dir, 'test.cache')
    if not os.path.exists( f ):
        fp = open( f, "w" )
        fp.write( 'VERSION=' + str(version) + '\n' )
        fp.write( 'DIR=' + os.getcwd() + '\n' )
        if opts.platname:
              fp.write( 'PLATFORM=' + opts.platname.strip() + '\n' )
        else:
              fp.write( 'PLATFORM=' + plat.getName() + '\n' )
        if optD['param_list']:
            fp.write( 'PARAMETERS=' + str( optD['param_list'] ) + '\n' )
        if config.get('exepath'):
            fp.write( \
                'PROJECT=' + os.path.abspath( config.get('exepath') ) + '\n' )
        if optD['onopts']:
            fp.write( 'ONOPTS=' + '+'.join( optD['onopts'] ) + '\n' )
        if optD['offopts']:
            fp.write( 'OFFOPTS=' + '+'.join( optD['offopts'] ) + '\n' )
        if opts.timeout != None:
            fp.write( 'TIMEOUT=' + str(opts.timeout).strip() + '\n' )
        if opts.multiplier != None:
            fp.write( 'TIMEOUT_MULTIPLIER=' + \
                                   str(opts.multiplier).strip() + '\n' )
        if opts.useenv:
            fp.write( 'USE_ENV=1\n' )
        if opts.ignore_platforms:
            fp.write( 'ALL_PLATFORMS=1\n' )
        if opts.include_tdd:
            fp.write( 'INCLUDE_TDD=True\n' )
        if opts.checkname:
            fp.write( 'CHECK=' + ' '.join( opts.checkname ) + '\n' )
        fp.close()

    perms.set( os.path.abspath(f) )


def readCommandInfo( opts, optD ):
    """
    Check for a file called 'test.cache' that indicates whether the
    current working directory is a TestResults directory (or subdirectory)
    then open that file for information.  The test results directory is
    returned, or None if not in a TestRestults directory.
    """
    # an environment variable is used to identify vvtest run recursion
    troot = os.environ.get( 'VVTEST_TEST_ROOT', None )

    test_cache = misc.find_vvtest_test_root_file(
                                        os.getcwd(), troot, 'test.cache' )

    if test_cache != None:

        if optD['onopts'] or optD['offopts'] or opts.generate:
            sys.stderr.write('*** error: ' + \
                'the -g, -o, and -O options are not allowed ' + \
                'in a TestResults directory\n')
            sys.exit(1);

        fp = open( test_cache, "r" )
        write_version = 0
        for line in fp.readlines():
            line = line.strip()
            kvpair = line.split( '=', 1 )
            if kvpair[0] == 'VERSION':
                write_version = kvpair[1]
            elif kvpair[0] == 'DIR':
                previous_run_dir = kvpair[1]
            elif kvpair[0] == 'PLATFORM':
                opts.platname = kvpair[1]
            elif kvpair[0] == 'PARAMETERS':
                L = eval( kvpair[1] )
                if optD['param_list']: optD['param_list'].extend(L)
                else:                  optD['param_list'] = L
            elif kvpair[0] == 'PROJECT':
                # do not replace if the command line contains -j
                if not opts.bin_dir:
                    opts.bin_dir = kvpair[1]
                    config.set( 'exepath', kvpair[1] )
            elif kvpair[0] == 'ONOPTS':
                optD['onopts'] = kvpair[1].split( '+' )
                config.set( 'onopts', optD['onopts'] )
            elif kvpair[0] == 'OFFOPTS':
                optD['offopts'] = kvpair[1].split( '+' )
                config.set( 'offopts', optD['offopts'] )
            elif kvpair[0] == 'TIMEOUT':
                # do not replace if the command line contains -T
                if opts.timeout == None:
                    opts.timeout = kvpair[1]
                    config.set( 'timeout', float(opts.timeout) )
            elif kvpair[0] == 'TIMEOUT_MULTIPLIER':
                if not opts.multiplier:
                    opts.multiplier = float(kvpair[1])
                    config.set( 'multiplier', opts.multiplier )
            elif kvpair[0] == 'USE_ENV':
                opts.useenv = True
            elif kvpair[0] == 'ALL_PLATFORMS':
                opts.ignore_platforms = True
            elif kvpair[0] == 'INCLUDE_TDD':
                opts.include_tdd = True
            elif kvpair[0] == 'CHECK':
                opts.checkname = kvpair[1].split()
        fp.close()

    if test_cache != None:
        return os.path.dirname( test_cache )
    return None


def load_filter_and_prune_test_list( test_dir, tlist, plat, filter_dir,
                                     optanalyze ):
    ""
    pruneL,cntmax = tlist.loadAndFilter( plat.getMaxProcs(),
                                         filter_dir=filter_dir,
                                         analyze_only=optanalyze,
                                         prune=True )
    
    if len(pruneL) > 0:
        print3()
        for pt,ct in pruneL:
            print3( '*** Warning: analyze test',
                    relative_execute_directory( pt, test_dir, os.getcwd() ),
                    'will not be run due to dependency',
                    relative_execute_directory( ct, test_dir, os.getcwd() ) )
    if cntmax > 0:
        print3()
        print3( 'Note: there were', cntmax, 'tests that exceeded the',
                'maximum number of processors - they will not be run' )


def runTests( opts, optD, toolsdir, dirs, plat, rtconfig, testsubdir ):
    """
    Executes a list of tests.
    """
    # determine the directory that stores the test results then create it
    test_dir = os.path.abspath( testsubdir )
    tfile = os.path.join( test_dir, testlist_name )

    tlist = TestList.TestList( tfile, rtconfig )

    check_for_currently_running_vvtest( tlist.getResultsFilenames(), opts.force )

    # this variable allows vvtest tests to run vvtest (ie, allows recursion)
    os.environ['VVTEST_TEST_ROOT'] = os.path.normpath( os.path.abspath( test_dir ) )

    perms = make_PermissionSetter( test_dir, opts.perms )

    createTestDir( testsubdir, perms, opts.mirror_dir )

    if opts.wipe:
        remove_directory_contents( testsubdir )

    writeCommandInfo( opts, optD, test_dir, plat, perms )

    scan_test_source_directories( tlist, dirs, optD['param_dict'] )

    # save the test list in the TestResults directory
    tlist.stringFileWrite()
    perms.set( os.path.abspath( tfile ) )

    tlist.readTestResults()
    tlist.ensureInlinedTestResultIncludes()

    loadRunTimes( tlist, plat,
                  opts.timeout, opts.multiplier, opts.maxtimeout )

    load_filter_and_prune_test_list( test_dir, tlist, plat, None, opts.analyze )

    if len(tlist.active) > 0:

        print3( "Running these tests:" )
        printResults( tlist, test_dir, perms,
                      optD['sort_letters'], opts.results_date,
                      short=True )
        print3()

        tlist.createTestExecs( test_dir, plat, config, perms )

        if opts.save_results:
            saveResults( opts, optD, tlist, plat, test_dir, inprogress=True )

        if not opts.batch:
            executeTestList( opts, optD, tlist, test_dir, plat, perms, tfile )

        else:
            batchTestList( opts, optD, toolsdir, tlist, test_dir, plat, perms )

        print3( "Test directory:", testsubdir )

        if opts.save_results:
            saveResults( opts, optD, tlist, plat, test_dir )

    else:
        print3( "\n--------- no tests to run -----------\n" )


def check_for_currently_running_vvtest( resultsfiles, optforce ):
    ""
    if not optforce:

        msg = '*** error: tests are currently running in another process\n' + \
              '    (or a previous run was killed); use --force to run anyway'

        if len(resultsfiles) > 0:

            rfile = resultsfiles[-1]

            tlr = testlistio.TestListReader( rfile )
            fin = tlr.scanForFinishDate()
            if fin == None:
                print3( msg )
                sys.exit(1)


def printResults( atestlist, test_dir, perms, optsort, optrdate,
                  short=False, add_date=True,
                  tostdout=True, tohtml=False, tojunit='' ):
    """
    Prints a summary to the screen and also creates an HTML summary file.
    If 'short' is True, then only a handfull of tests are written to the
    screen.
    """
    rawlist = atestlist.getActiveTests( optsort )
    
    Lfail = []; Ltime = []; Ldiff = []; Lpass = []; Lnrun = []; Lndone = []
    for atest in rawlist:
        statr = XstatusResult(atest)
        if   statr == "fail":    Lfail.append(atest)
        elif statr == "timeout": Ltime.append(atest)
        elif statr == "diff":    Ldiff.append(atest)
        elif statr == "pass":    Lpass.append(atest)
        elif statr == "notrun":  Lnrun.append(atest)
        elif statr == "notdone": Lndone.append(atest)
    sumstr = str(len(Lpass)) + " pass, " + \
             str(len(Ltime)) + " timeout, " + \
             str(len(Ldiff)) + " diff, " + \
             str(len(Lfail)) + " fail, " + \
             str(len(Lnrun)) + " notrun, " + \
             str(len(Lndone)) + " notdone"
    
    if tostdout:
        cwd = os.getcwd()
        print3( "==================================================" )
        if short and len(rawlist) > 16:
            for atest in rawlist[:8]:
                print3( XstatusString( atest, add_date, test_dir, cwd ) )
            print3( "..." )
            for atest in rawlist[-8:]:
                print3( XstatusString( atest, add_date, test_dir, cwd ) )
        else:
            for atest in rawlist:
                print3( XstatusString( atest, add_date, test_dir, cwd ) )
        print3( "==================================================" )
        print3( "Summary:", sumstr )
    
    if tohtml:
        printHTMLResults( sumstr, test_dir, perms,
                          Lfail, Ltime, Ldiff, Lpass, Lnrun, Lndone )

    if tojunit:

        if optrdate != None:
            try:
                datestamp = float(optrdate)
            except:
                print3( '*** vvtest error: --results-date must be seconds ' + \
                        'since epoch for use with --junit option' )
                sys.exit(1)
        else:
            datestamp = atestlist.getDateStamp( time.time() )

        print3( "Writing", len(rawlist), "tests to JUnit file", tojunit )
        results.write_JUnit_file( test_dir, rawlist, tojunit, datestamp )


def printHTMLResults( sumstr, test_dir, perms,
                      Lfail, Ltime, Ldiff, Lpass, Lnrun, Lndone ):
    """
    Opens and writes an HTML summary file in the test directory.
    """
    
    if test_dir == ".":
      test_dir = os.getcwd()
    if not os.path.isabs(test_dir):
      test_dir = os.path.abspath(test_dir)
    
    fp = open("summary.htm","w")
    fp.write( "<html>\n<head>\n<title>Test Results</title>\n" )
    fp.write( "</head>\n<body>\n" )
    
    # a summary section
    
    fp.write( "<h1>Summary</h1>\n" )
    fp.write( "  <ul>\n" )
    fp.write( "  <li> Directory: " + test_dir + "\n" )
    fp.write( "  <li> " + sumstr + "</li>\n" )
    fp.write( "  </ul>\n" )
    
    # segregate the tests into implicit keywords, such as fail and diff
    
    fp.write( '<h1>Tests that showed "fail"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Lfail )
    fp.write( '<h1>Tests that showed "timeout"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Ltime )
    fp.write( '<h1>Tests that showed "diff"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Ldiff )
    fp.write( '<h1>Tests that showed "notdone"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Lndone )
    fp.write( '<h1>Tests that showed "pass"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Lpass )
    fp.write( '<h1>Tests that showed "notrun"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Lnrun )
    
    fp.write( "</body>\n</html>\n" )
    fp.close()

    perms.set( os.path.abspath( 'summary.htm' ) )


def writeHTMLTestList( fp, test_dir, tlist ):
    """
    Used by printHTMLResults().  Writes the HTML for a list of tests to the
    HTML summary file.
    """
    cwd = os.getcwd()
    
    fp.write( '  <ul>\n' )
    
    for atest in tlist:
      
      fp.write( '  <li><code>' + XstatusString(atest, 1, test_dir, cwd) + '</code>\n' )
      
      if isinstance(atest, TestExec.TestExec): ref = atest.atest
      else:                            ref = atest
      
      tdir = os.path.join( test_dir, ref.getExecuteDirectory() )
      assert cwd == tdir[:len(cwd)]
      reltdir = tdir[len(cwd)+1:]
      
      fp.write( "<ul>\n" )
      thome = atest.getRootpath()
      xfile = os.path.join( thome, atest.getFilepath() )
      fp.write( '  <li>XML: <a href="file://' + xfile + '" ' + \
                       'type="text/plain">' + xfile + "</a></li>\n" )
      fp.write( '  <li>Parameters:<code>' )
      for (k,v) in atest.getParameters().items():
        fp.write( ' ' + k + '=' + v )
      fp.write( '</code></li>\n' )
      fp.write( '  <li>Keywords: <code>' + ' '.join(atest.getKeywords()) + \
                 ' ' + ' '.join( atest.getResultsKeywords() ) + \
                 '</code></li>\n' )
      fp.write( '  <li>Status: <code>' + XstatusString(atest, 1, test_dir, cwd) + \
                 '</code></li>\n' )
      fp.write( '  <li> Files:' )
      if os.path.exists(reltdir):
        for f in os.listdir(reltdir):
          fp.write( ' <a href="file:' + os.path.join(reltdir,f) + \
                    '" type="text/plain">' + f + '</a>' )
      fp.write( '</li>\n' )
      fp.write( "</ul>\n" )
      fp.write( "</li>\n" )
    fp.write( '  </ul>\n' )


def executeTestList( opts, optD, tlist, test_dir, plat, perms, tfile ):
    """
    """
    plat.display()
    starttime = time.time()
    print3( "Start time:", time.ctime() )

    uthook = utesthooks.construct_unit_testing_hook( 'run', opts.qsub_id )

    rfile = tlist.initializeResultsFile()

    try:

        # execute tests

        perms.set( os.path.abspath( rfile ) )

        cwd = os.getcwd()

        while True:

            tnext = tlist.popNext( plat )

            if tnext != None:
                print3( 'Starting:',
                        relative_execute_directory( tnext.atest, test_dir, cwd ) )
                tnext.start()
                tlist.appendTestResult( tnext.atest )
            
            elif tlist.numRunning() == 0:
                break

            else:
                time.sleep(1)

            showprogress = False
            for tx in list( tlist.getRunning() ):
                if tx.poll():
                    xs = XstatusString( tx, False, test_dir, cwd )
                    print3( "Finished:", xs )
                    tlist.testDone( tx )
                    showprogress = True
          
            uthook.check( tlist.numRunning(), tlist.numDone() )

            if showprogress:
                ndone = tlist.numDone()
                ntot = tlist.numActive()
                pct = 100 * float(ndone) / float(ntot)
                div = str(ndone)+'/'+str(ntot)
                dt = pretty_time( time.time() - starttime )
                print3( "Progress: " + div+" = %%%.1f"%pct + ', time = '+dt )

    finally:
        tlist.writeFinished()

    # any remaining tests cannot run, so print warnings
    tL = tlist.popRemaining()
    if len(tL) > 0:
        print3()
    for tx in tL:
        deptx = tx.getBlockingDependency()
        assert tx.hasDependency() and deptx != None
        print3( '*** Warning: analyze test skipped for "' + \
                tx.atest.getExecuteDirectory() + \
                '" due to dependency "' + deptx.atest.getExecuteDirectory() + '"' )
    
    do_html = ( opts.qsub_id == None )
    do_junit = ''
    if opts.qsub_id == None and opts.junit_filename:
        do_junit = opts.junit_filename
    print3()
    printResults( tlist, test_dir, perms,
                  optD['sort_letters'], opts.results_date,
                  tohtml=do_html, tojunit=do_junit )
    
    elapsed = pretty_time( time.time() - starttime )
    print3( "\nFinish date:", time.ctime() + " (elapsed time "+elapsed+")" )


def batchTestList( opts, optD, toolsdir, tlist, test_dir, plat, perms ):
    """
    The 'tlist' is a TestList class instance.
    """
    assert opts.qsub_id == None

    qsublimit = opts.batch_limit
    if qsublimit == None:
        qsublimit = plat.getDefaultQsubLimit()
    
    batch = Batcher( toolsdir, opts, optD,
                     plat, test_dir, tlist, perms, qsublimit )
    
    plat.display()
    starttime = time.time()
    print3( "Start time:", time.ctime() )

    results_suffix = tlist.setResultsSuffix()

    # write testlist files for each qsub
    numjobs = batch.writeQsubScripts( results_suffix )

    print3( 'Total number of batch jobs: ' + str(numjobs) + \
            ', maximum concurrent jobs: ' + str(qsublimit) )

    if opts.generate:
      return
    
    schedule = batch.getScheduler()

    cwd = os.getcwd()
    qsleep = int( os.environ.get( 'VVTEST_BATCH_SLEEP_LENGTH', 15 ) )

    uthook = utesthooks.construct_unit_testing_hook( 'batch' )

    rfile = tlist.initializeResultsFile()
    for inclf in batch.getIncludeFiles():
        tlist.addIncludeFile( inclf )

    try:
        while True:

            qid = schedule.checkstart()
            if qid != None:
                # nothing to print here because the qsubmit prints
                pass
            elif schedule.numInFlight() == 0:
                break
            else:
                time.sleep( qsleep )

            qidL,doneL = schedule.checkdone()
            
            if len(qidL) > 0:
                ids = ' '.join( [ str(qid) for qid in qidL ] )
                print3( 'Finished batch IDS:', ids )
            for t in doneL:
                ts = XstatusString( t, False, test_dir, cwd )
                print3( "Finished:", ts )

            uthook.check( schedule.numInFlight(), schedule.numPastQueue() )

            if len(doneL) > 0:
                jpct = 100 * float(schedule.numDone()) / float(numjobs)
                jdiv = 'jobs '+str(schedule.numDone())+'/'+str(numjobs)
                jflt = '(in flight '+str(schedule.numStarted())+')'
                ndone = tlist.numDone()
                ntot = tlist.numActive()
                tpct = 100 * float(ndone) / float(ntot)
                tdiv = 'tests '+str(ndone)+'/'+str(ntot)
                dt = pretty_time( time.time() - starttime )
                print3( "Progress: " + \
                        jdiv+" = %%%.1f"%jpct + ' '+jflt+', ' + \
                        tdiv+" = %%%.1f"%tpct + ', ' + \
                        'time = '+dt )

        # any remaining tests cannot be run; flush then print warnings
        NS, NF, nrL = schedule.flush()

    finally:
        tlist.writeFinished()

    tlist.inlineIncludeFiles()

    perms.set( os.path.abspath( rfile ) )

    if len(NS)+len(NF)+len(nrL) > 0:
        print3()
    if len(NS) > 0:
      print3( "*** Warning: these batch numbers did not seem to start:",
              ' '.join(NS) )
    if len(NF) > 0:
      print3( "*** Warning: these batch numbers did not seem to finish:",
              ' '.join(NF) )
    for tx,deptx in nrL:
        assert tx.hasDependency() and deptx != None
        print3( '*** Warning: analyze test skipped for "' + \
                tx.atest.getExecuteDirectory() + \
                '" due to dependency "' + deptx.atest.getExecuteDirectory() + '"' )

    print3()
    do_junit = opts.junit_filename
    printResults( tlist, test_dir, perms,
                  optD['sort_letters'], opts.results_date,
                  tohtml=True, tojunit=do_junit )
    
    elapsed = pretty_time( time.time() - starttime )
    print3( "\nFinish date:", time.ctime() + " (elapsed time "+elapsed+")" )


def pretty_time( nseconds ):
    """
    Returns a string with the given number of seconds written in an easily
    readable form.
    """
    h = int( nseconds / 3600 )
    sh = str(h)+'h'

    m = int( ( nseconds - 3600*h ) / 60 )
    sm = str(m)+'m'

    s = int( ( nseconds - 3600*h - 60*m ) )
    if h == 0 and m == 0 and s == 0: s = 1
    ss = str(s) + 's'

    if h > 0: return sh+' '+sm+' '+ss
    if m > 0: return sm+' '+ss
    return ss


def loadRunTimes( tlist, plat, opttimeout, optmult, optmaxtimeout ):
    """
    For each test, a 'runtimes' file will be read (if it exists) and the
    run time for this platform extracted.  This run time is saved in the
    test as the 'runtime' attribute.  Also, a timeout is calculated for
    each test and placed in the 'timeout' attribute.
    """
    pname = plat.getName()
    cplr = plat.getCompiler()

    cache = results.LookupCache( pname, cplr, plat.testingDirectory() )

    for t in tlist.getTests():

        # grab explicit timeout value, if the test specifies it
        tout = t.getTimeout()

        # look for a previous runtime value
        tlen,tresult = cache.getRunTime( t )

        if tlen != None:

            t.setAttr( 'runtime', int(tlen) )

            if tout == None:
                if tresult == "timeout":
                    # for tests that timed out, make timeout much larger
                    if t.hasKeyword( "long" ):
                        # only long tests get timeouts longer than an hour
                        if tlen < 60*60:
                            tout = 4*60*60
                        elif tlen < 5*24*60*60:  # even longs are capped
                            tout = 4*tlen
                    else:
                        tout = 60*60

                else:
                    # pick timeout to allow for some runtime variability
                    if tlen < 120:
                        tout = max( 120, 2*tlen )
                    elif tlen < 300:
                        tout = max( 300, 1.5*tlen )
                    elif tlen < 4*60*60:
                        tout = int( float(tlen)*1.5 )
                    else:
                        tout = int( float(tlen)*1.3 )

        else:  # no previous result

            if tout != None:
                # use the explicit timeout value as the runtime
                tlen = tout
            else:
                # with no information, the default depends on 'long' keyword
                if t.hasKeyword("long"):
                    tlen = 5*60*60  # five hours
                else:
                    tlen = 60*60  # one hour
                tout = tlen

        if opttimeout != None:
            tout = int( float(opttimeout) )
        if optmult != None:
            tout = int( float(tout) * optmult )

        if optmaxtimeout != None:
            tout = min( tout, float(optmaxtimeout) )

        t.setAttr( 'timeout', tout )

    cache = None


class Batcher:
    
    def __init__(self, toolsdir, opts, optD,
                       plat, test_dir, tlist, perms, maxjobs):
        """
        The 'tlist' is a TestList class instance.
        """
        self.toolsdir = toolsdir
        self.opts = opts
        self.optD = optD
        self.plat = plat
        self.test_dir = test_dir
        self.tlist = tlist
        self.perms = perms
        self.maxjobs = maxjobs
        self.clean_exit_marker = "queue job finished cleanly"

        # TODO: make Tzero a platform plugin thing
        self.Tzero = 21*60*60  # no timeout in batch mode is 21 hours

        # allow these values to be set by environment variable, mainly for
        # unit testing; if setting these is needed more regularly then a
        # command line option should be added
        val = int( os.environ.get( 'VVTEST_BATCH_READ_INTERVAL', 30 ) )
        self.read_interval = val
        val = int( os.environ.get( 'VVTEST_BATCH_READ_TIMEOUT', 5*60 ) )
        self.read_timeout = val

        self.qsub_testfilenames = []

        self.accountant = batchutils.BatchAccountant()
        self.namer = batchutils.BatchFileNamer( self.test_dir, testlist_name )

        self.createTestGroups()

        self.scheduler = batchutils.BatchScheduler(
                            self.test_dir, self.tlist,
                            self.accountant, self.namer,
                            self.perms, self.plat, self.maxjobs,
                            self.clean_exit_marker )

    def getScheduler(self):
        return self.scheduler

    def removeBatchDirectories(self):
        """
        """
        for d in self.namer.globBatchDirectories():
            print3( 'rm -rf '+d )
            fault_tolerant_remove( d )

    def createTestGroups(self):
        """
        """
        qlen = self.opts.batch_length
        if qlen == None:
            qlen = 30*60

        qL = []
        for np in self.tlist.getTestExecProcList():
          xL = []
          for tx in self.tlist.getTestExecList(np):
            xL.append( (tx.atest.getAttr('timeout'),tx) )
          xL.sort()
          grpL = []
          tsum = 0
          for rt,tx in xL:
            if tx.hasDependency() or tx.atest.getAttr('timeout') < 1:
              # analyze tests and those with no timeout get their own group
              qL.append( [ self.Tzero, len(qL), [tx] ] )
            else:
              if len(grpL) > 0 and tsum + rt > qlen:
                qL.append( [ tsum, len(qL), grpL ] )
                grpL = []
                tsum = 0
              grpL.append( tx )
              tsum += rt
          if len(grpL) > 0:
            qL.append( [ tsum, len(qL), grpL ] )
        
        qL.sort()
        qL.reverse()
        self.qsublists = map( lambda L: L[2], qL )

    def writeQsubScripts(self, results_suffix):
        """
        """
        self.tlist.markTestsWithDependents()

        self.removeBatchDirectories()

        commonopts = ''
        if self.opts.useenv: commonopts += ' -e'
        if self.opts.nopreclean: commonopts += ' -m'
        if self.opts.postclean: commonopts += ' -C'
        if self.opts.analyze: commonopts += ' -a'
        if self.opts.maxprocs: commonopts += ' -N '+str( self.opts.maxprocs )
        if self.optD['param_list']:
            # have to escape exclamation points
            for s in self.optD['param_list']:
                s = s.replace( '!', '\\!' )
                commonopts += ' -p "'+s+'"'
        if self.opts.perms:
            commonopts += ' --perms '+','.join( self.opts.perms )
        if config.get('configdir'):
            commonopts += ' --config='+config.get('configdir')
        if self.optD['platopt_dict']:
            for k,v in optD['platopt_dict'].items():
                if v:
                    commonopts += ' --platopt ' + k + '=' + v
                else:
                    commonopts += ' --platopt ' + k 
        for arg in config.get('testargs'):
            commonopts += ' --test-args="'+arg+'"'

        qsubids = {}  # maps batch id to max num processors for that batch
        
        qid = 0
        for qL in self.qsublists:
          self.make_queue_batch( qid, qL, qsubids, commonopts, results_suffix )
          qid += 1

        qidL = list( qsubids.keys() )
        qidL.sort()
        for i in qidL:
            incl = self.namer.getTestListName( i, relative=True )
            self.qsub_testfilenames.append( incl )

        for i in qidL:
            d = self.namer.getSubdir( i )
            self.perms.recurse( d )

        return len( qsubids )

    def getIncludeFiles(self):
        ""
        return self.qsub_testfilenames

    def make_queue_batch(self, qnumber, qlist, npD, comopts, results_suffix):
        """
        """
        qidstr = str(qnumber)

        testlistfname = self.namer.getTestListName( qidstr )

        tl = TestList.TestList( testlistfname )
        tl.setResultsSuffix( results_suffix )

        tL = []
        maxnp = 0
        qtime = 0
        for tx in qlist:
          np = int( tx.atest.getParameters().get('np', 0) )
          if np <= 0: np = 1
          maxnp = max( maxnp, np )
          tl.addTest(tx.atest)
          tL.append( tx )
          qtime += int( tx.atest.getAttr('timeout') )
        
        if qtime == 0:
          qtime = self.Tzero  # give it the "no timeout" length of time
        else:
          # allow more time in the queue than calculated
          if qtime < 60:
            qtime = 120
          elif qtime < 600:
            qtime *= 2
          else:
            qtime = int( float(qtime) * 1.3 )

        if self.opts.maxtimeout:
            qtime = min( qtime, float(self.opts.maxtimeout) )

        npD[qnumber] = maxnp
        pout = self.namer.getBatchOutputName( qnumber )
        tout = self.namer.getTestListName( qnumber ) + '.' + results_suffix

        jb = batchutils.BatchJob( maxnp, pout, tout, tL,
                                  self.read_interval, self.read_timeout )
        self.accountant.addJob( qnumber, jb )
        
        tl.stringFileWrite( include_results_suffix=True )
        
        fn = self.namer.getBatchScriptName( qidstr )
        fp = open( fn, "w" )
        
        hdr = self.plat.getQsubScriptHeader( maxnp, qtime, self.test_dir, pout )
        fp.writelines( [ hdr + '\n\n',
                         'cd ' + self.test_dir + ' || exit 1\n',
                         'echo "job start time = `date`"\n' + \
                         'echo "job time limit = ' + str(qtime) + '"\n' ] )
        
        # set the environment variables from the platform into the script
        for k,v in self.plat.getEnvironment().items():
          fp.write( 'setenv ' + k + ' "' + v  + '"\n' )
        
        # collect relevant options to pass to the qsub vvtest invocation
        taopts = '--qsub-id=' + qidstr + ' '
        taopts += comopts
        if len(qlist) == 1:
          # force a timeout for batches with only one test
          if qtime < 600: taopts += ' -T ' + str(qtime*0.90)
          else:           taopts += ' -T ' + str(qtime-120)
        
        cmd = self.toolsdir+'/vvtest ' + taopts + ' || exit 1'
        fp.writelines( [ cmd+'\n\n' ] )
        
        # echo a marker to determine when a clean batch job exit has occurred
        fp.writelines( [ 'echo "'+self.clean_exit_marker+'"\n' ] )
        
        fp.close()


def saveResults( opts, optD, tlist, plat, test_dir, inprogress=False ):
    """
    Option is
    
      --save-results
    
    which writes to the platform config testing directory (which looks first at
    the TESTING_DIRECTORY env var).  Can add
    
      --results-tag <string>
    
    which is appended to the results file name.  A date string is embedded in
    the file name, which is obtained from the date of the first test that
    ran.  But if the option

      --results-date <float or string>

    is given on the vvtest command line, then that date is used instead.
    """
    pname = plat.getName()
    cplr = plat.getCompiler()

    rtag = opts.results_tag
    
    # determine the date stamp to embed in the file name
    rdate = opts.results_date
    if rdate != None:
      try:
        assert '_' not in rdate  # recent python 3 allows underscores
        val = float(rdate)
        # assume format was seconds since epoch
        datestr = time.strftime( "%Y_%m_%d", time.localtime( val ) )
      except:
        datestr = rdate  # just take date string verbatim
    else:
      datestamp = tlist.getDateStamp( time.time() )
      datestr = time.strftime( "%Y_%m_%d", time.localtime( datestamp ) )

    L = []
    if optD['onopts']:
        for o in optD['onopts']:
            if o != cplr:
                L.append(o)
    L.sort()
    L.insert( 0, cplr )
    optstag = '+'.join(L)

    rdir = plat.testingDirectory()
    if rdir == None or not os.path.isdir(rdir):
      raise Exception( "invalid testing directory: " + str(rdir) )
    
    L = ['results',datestr,pname,optstag]
    if rtag != None: L.append(rtag)
    fname = os.path.join( rdir, '.'.join( L ) )
    
    tr = results.TestResults()
    for t in tlist.getActiveTests():
      tr.addTest(t)
    mach = os.uname()[1]
    tr.writeResults( fname, pname, cplr, mach, test_dir, inprogress )


def restartTests( opts, optD, toolsdir, test_dir, plat, rtconfig ):
    ""
    assert test_dir != None

    # this variable allows vvtest tests to run vvtest (ie, allows recursion)
    os.environ['VVTEST_TEST_ROOT'] = os.path.normpath( os.path.abspath( test_dir ) )

    path_filter = os.getcwd()
    
    qid = opts.qsub_id
    if qid == None:
        tfile = os.path.join( test_dir, testlist_name )
    else:
        # batch jobs have --qsub-id set and land here
        namer = batchutils.BatchFileNamer( test_dir, testlist_name )
        tfile = namer.getTestListName( qid )
        # prevent the run scripts from being written again
        config.set( 'refresh', False )

    tlist = TestList.TestList( tfile, rtconfig )

    tlist.readTestListIfNoTestResults()
    tlist.readTestResults()
    tlist.ensureInlinedTestResultIncludes()

    check_for_currently_running_vvtest( tlist.getResultsFilenames(), opts.force )

    if qid == None:
        loadRunTimes( tlist, plat,
                      opts.timeout, opts.multiplier, opts.maxtimeout )

    reld = computeRelativePath( os.path.abspath(test_dir), os.getcwd() )

    load_filter_and_prune_test_list( test_dir, tlist, plat, reld, opts.analyze )

    perms = make_PermissionSetter( test_dir, opts.perms )

    perms.set( os.path.abspath( tfile ) )

    if len(tlist.active) > 0:

        print3( "Running these tests:" )
        printResults( tlist, test_dir, perms,
                      optD['sort_letters'], opts.results_date,
                      short=True )
        print3()

        if opts.save_results:
            saveResults( opts, optD, tlist, plat, test_dir, inprogress=True )

        tlist.createTestExecs( test_dir, plat, config, perms )

        if not opts.batch:
            executeTestList( opts, optD, tlist, test_dir, plat, perms, tfile )

        else:
            batchTestList( opts, optD, toolsdir, tlist, test_dir, plat, perms )

        if opts.save_results:
            saveResults( opts, optD, tlist, plat, test_dir )

    else:
        print3( "\n--------- no tests to run -----------\n" )


def baselineTests( opts, optD, test_dir, plat, rtconfig, testsubdir ):
    ""
    path_filter = None
    if test_dir == None:
        test_dir = os.path.abspath( testsubdir )
    else:
        path_filter = os.getcwd()

    tfile = os.path.join( test_dir, testlist_name )

    tlist = TestList.TestList( tfile, rtconfig )

    tlist.readTestListIfNoTestResults()
    tlist.readTestResults()
    tlist.ensureInlinedTestResultIncludes()

    filter_dir = None
    if path_filter != None:
      filter_dir = computeRelativePath( test_dir, path_filter )
    
    # if the keyword expression does not include a results keyword, then
    # add the 'diff' keyword so that only diffs are rebaselined by default
    if not keyword_expr.containsResultsKeywords():
      keyword_expr.append( ['diff'], 'and' )
    
    tlist.loadAndFilter( plat.getMaxProcs(),
                         filter_dir=filter_dir,
                         baseline=True )
    
    if len(tlist.active) > 0:

        print3( "Baselining these tests:" )
        printResults( tlist, test_dir, None,
                      optD['sort_letters'], opts.results_date,
                      short=False )

        perms = make_PermissionSetter( test_dir, opts.perms )

        tlist.createTestExecs( test_dir, plat, config, perms )

        failures = False
        for tx in tlist.getTestExecList():

            if isinstance(tx, TestExec.TestExec):
                ref = tx.atest
            else:
                ref = tx

            sys.stdout.write( "baselining "+ref.getExecuteDirectory()+"..." )

            tx.start( baseline=1 )

            tm = int( os.environ.get( 'VVTEST_BASELINE_TIMEOUT', 30 ) )
            for i in range(tm):

                time.sleep(1)

                if tx.poll():
                    if tx.atest.getAttr('result') == "pass":
                        print3( "done" )
                    else:
                        failures = True
                        print3("FAILED")
                    break

            if not tx.isDone():
                tx.killJob()
                failures = True
                print3( "TIMED OUT" )

        if failures:
          print3( "\n\n !!!!!!!!!!!  THERE WERE FAILURES  !!!!!!!!!! \n\n" )

    else:
        print3( "\n--------- no tests to baseline -----------\n" )


###########################################################################

def getUserName():
    """
    Retrieves the user name associated with this process.
    """
    usr = None
    try:
        import getpass
        usr = getpass.getuser()
    except:
        usr = None
    
    if usr == None:
        try:
            uid = os.getuid()
            import pwd
            usr = pwd.getpwuid( uid )[0]
        except:
            usr = None
    
    if usr == None:
        try:
            p = os.path.expanduser( '~' )
            if p != '~':
                usr = os.path.basename( p )
        except:
            usr = None
    
    if usr == None:
        # try manually checking the environment
        for n in ['USER', 'LOGNAME', 'LNAME', 'USERNAME']:
            if os.environ.get(n,'').strip():
                usr = os.environ[n]
                break

    if usr == None:
        raise Exception( "could not determine this process's user name" )

    return usr


def computeRelativePath(d1, d2):
    """
    Compute relative path from directory d1 to directory d2.
    """
    
    assert os.path.isabs(d1)
    assert os.path.isabs(d2)
    
    d1 = os.path.normpath(d1)
    d2 = os.path.normpath(d2)
    
    list1 = d1.split( os.sep )
    list2 = d2.split( os.sep )
    
    while 1:
      try: list1.remove('')
      except: break
    while 1:
      try: list2.remove('')
      except: break
    
    i = 0
    while i < len(list1) and i < len(list2):
      if list1[i] != list2[i]:
        break
      i = i + 1
    
    p = []
    j = i
    while j < len(list1):
      p.append('..')
      j = j + 1
    
    j = i
    while j < len(list2):
      p.append(list2[j])
      j = j + 1
    
    if len(p) > 0:
      return os.path.normpath( os.sep.join(p) )
    
    return "."


def issubdir(parent_dir, subdir):
    """
    TODO: test for relative paths
    """
    lp = len(parent_dir)
    ls = len(subdir)
    if ls > lp and parent_dir + '/' == subdir[0:lp+1]:
      return subdir[lp+1:]
    return None


def print3( *args, **kwargs ):
    s = ' '.join( [ str(x) for x in args ] )
    if len(kwargs) > 0:
        s += ' ' + ' '.join( [ str(k)+'='+str(v) for k,v in kwargs.items() ] )
    sys.stdout.write( s + os.linesep )
    sys.stdout.flush()


###########################################################################

def getToolsDirectory():
    ""
    d = sys.path[0]
    if not d:                  d = os.getcwd()
    elif not os.path.isabs(d): d = os.path.abspath(d)
    return d


def check_for_bootstrap_file():
    """
    if vvtest_bootstrap.py exists in the same directory as vvtest,
    then import it (which may set os.environ variables)
    """
    try:
        import vvtest_bootstrap

    except ImportError:
        # to allow for vvtest to be a soft link to an installed vvtest area,
        # look for a bootstrap file in the directory containing the soft link
        bindir = os.path.dirname( os.path.abspath( sys.argv[0] ) )
        boot = os.path.join( bindir, 'vvtest_bootstrap.py' )
        if os.path.exists( boot ):
            sys.path.append( bindir )
            import vvtest_bootstrap


def insert_configdir_into_sys_path( toolsdir, config ):
    ""
    d1 = os.path.normpath( os.path.join( toolsdir, 'config' ) )

    d2 = config.get('configdir')
    if d2:
        d2 = os.path.normpath( d2 )

        if d1 != d2:
            sys.path.insert( 1, d1 )

        sys.path.insert( 1, d2 )

    else:
        sys.path.insert( 1, d1 )


def make_PermissionSetter( test_dir, optperms ):
    ""
    if optperms:
        perms = PermissionSetter( test_dir, optperms )
    else:
        perms = DummyPermissionSetter()

    return perms


def remove_directory_contents( path ):
    ""
    sys.stdout.write( 'rm -rf ' + path + '/* ...' )
    sys.stdout.flush()

    for f in os.listdir(path):
        df = os.path.join( path, f )
        fault_tolerant_remove( df )

    print3( 'done' )


def random_string( numchars=8 ):
    ""
    seq = string.ascii_uppercase + string.digits
    cL = [ random.choice( seq ) for _ in range(numchars) ]
    return ''.join( cL )


def fault_tolerant_remove( path, num_attempts=5 ):
    ""
    dn,fn = os.path.split( path )

    rmpath = os.path.join( dn, 'remove_'+fn + '_'+ random_string() )

    os.rename( path, rmpath )

    for i in range( num_attempts ):
        try:
            if os.path.islink( rmpath ):
                os.remove( rmpath )
            elif os.path.isdir( rmpath ):
                shutil.rmtree( rmpath )
            else:
                os.remove( rmpath )
            break
        except Exception:
            pass

        time.sleep(1)


##############################################################################

if __name__ == '__main__':

    main()
