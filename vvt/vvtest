#!/usr/bin/env python

# Copyright 2018 National Technology & Engineering Solutions of Sandia, LLC
# (NTESS). Under the terms of Contract DE-NA0003525 with NTESS, the U.S.
# Government retains certain rights in this software.

import sys
sys.dont_write_bytecode = True
sys.excepthook = sys.__excepthook__
import os
import re
import stat
import time
import signal
import shutil
import types
import glob
import shlex

version = '29'  # version number; used to check format and for information

usagepage = \
'''
  vvtest { -h | -H | --help | -v | --version }
  vvtest [OPTIONS] [directory ...]
  vvtest -i [OPTIONS]
  vvtest -g [kKpP] [directory ...]
  vvtest -b [OPTIONS]
  vvtest --extract <to_directory> [OPTIONS] [source_directory ...]

Use -H or --help to get the full man page.
'''

manpage = \
'''
NAME
      vvtest - generate and/or run a set of execution tests
      
      Version = ''' + version + '''

SYNOPSIS
      1.  vvtest { -h | -H | --help | -v | --version }
      2.  vvtest [OPTIONS] [directory ...]
      3.  vvtest -i [OPTIONS]
      4.  vvtest -g [kKpP] [directory ...]
      5.  vvtest -b [OPTIONS]
      6.  vvtest --extract <to_directory> [OPTIONS] [source_directory ...]

DESCRIPTION
      1. -h gives short usage help, while --help or -H gives long man page.
      Get the version number with -v or --version.

      2. Execute a set of tests.  The list of tests is determined by recursively
      scanning the given directories (or the current working directory if no
      directories are given).  If this is a repeat call and a test results
      directory already exists, then any new tests are merged into the
      existing list.  Also in this case, if no directory is given, a scan of
      the current working directory is not done.
      
      The tests are run in a directory whose name starts with TestResults
      and is appended with the platform (usually uname) and the -o and -O
      options.  Therefore, the -o and -O options must be specified for each
      invocation of vvtest (unless the current working directory is one of
      the TestResults directories).

      If the current working directory is a TestResults directory, then the
      original options are used.
      
      Only those tests are run that show "notrun" or "notdone" and that
      satisfy the filtering options.  However, if the -k or -K options are
      used, they take precedence.  Also, if the -R option is used, all the
      tests are run after being filtered by -kK keyword options.
      
      3. Display information on a previous test run.  The same on/off
      options must be given on the command line when the current working
      directory is not a TestResults directory.  Also, to get the results of
      a platform other than the current platform, the --plat option must be
      used (unless the current working directory is a TestResults directory).
      
      4. Generate a test list for later execution by recursively scanning the
      given directories the same as in number 2.  The test directory will be
      populated with the test scripts but no tests will be launched.

      5. Rebaseline generated files from a previous run by copying them over
      the top of the files in the directory which contain the XML test
      description.  Keywords and parameter options can be given to control
      which tests get rebaselined, including results keywords like "diff".
      If no keyword options are specified, then all tests that show a diff
      status are rebaselined, which would be equivalent to running the
      command "vvtest -b -k diff".
      
      The rebaseline option can be given when the current working directory is
      a TestResults directory with the same behavior.
      
      6. Extract tests from a test tree.  This mode copies the files from the
      test source directory into the 'to_directory' given as the argument to
      the --extract option.  Keyword filtering can be applied, and is often
      used to pull tests that can be distributed to various customers.

OPTIONS
      -o on_options
      -O off_options
            Options are arbitrary and may be used by the execution launch
            script.  Also, the TestResults directory contains the options.
      
      -k keyword
            Filter the set of tests by including those with keyword.  Multiple
            -k keyword options are logically ANDed together.  Keywords
            separated with a "/" are ORed together.  Eg, "-k key1/key2" means
            if key1 or key2 is contained in the test keyword list, then the
            test is included.
            
            Note that implicit results keywords are available:

                notrun  : test was in the test list but was not launched
                notdone : test was launched but did not finish
                fail    : test finished and returned a fail status
                diff    : test finished with a diff status
                pass    : test finished with a pass status
                timeout : test finished by running out of time

            For example, "vvtest -k diff" would run all tests that
            finished with a diff status.  If no keywords are specified,
            then all tests "notrun" or "notdone" will be run.  If a test times
            out, it will receive the "fail" and "timeout" implicit keywords.
      
      -K keyword
            Filter the set of tests by excluding those with keyword.  Multiple
            -K keyword options are logically ANDed together.  Keywords
            separated with a "/" are ORed together after applying the exclusion
            operator.  Eg, "-K key1/key2" means "(!key1) OR (!key2)" where
            "!key" means "key" is not in the test keyword list.
            
            Note that if both -k and -K options are given, the expressions are
            evaluated left to right.
      
      --keys
            Collect the tests in an existing TestResults directory if it exists,
            or scan the directories given on the command line.  Then gather all
            the keywords for each of the tests and write them to stdout.  Note
            that keyword filtering is applied before the keywords are gathered.
      
      --files
            Same as --keys except the collection of test file names is written
            instead.
      
      --sort <character(s)>
            When test results are printed to the screen, they are sorted by
            their test name by default.  Use this option to sort by other
            fields:
                    n : test name (the default)
                    x : execution directory name
                    t : test run time
                    d : execution date
                    s : test status (such as pass, fail, diff, etc)
                    r : reverse the order
            If more than one character is given, then ties on the first
            ordering are broken by subsquent orderings.  For example,
            "--sort txr" would sort first by runtime, then by execution
            directory name, and would reverse the order (so largest runtimes
            first, smallest runtimes last).

      -w    Wipe out previous test list and test results, if present.  By
            default, previous tests are only modified or appended to.
      
      -p param=value[/param2=value2...]
            Filter the set of tests by including those whose given parameter
            has the given value.  If a test does not know the parameter, it
            is filtered out.  If a value is not given, eg. "-p np=" or "-p np",
            then any test that does not have the parameter name, eg. "np", is
            filtered out.  Can also use "np<=4", "np<4", "np>4", "np!=4", etc.
            Specifications separated by '/' character are OR'ed together, and
            multiple -p/-P options are AND'ed together.
      
      -P param=value[/param2=value2...]
            Filter the set of tests by excluding those whose given parameter
            has the given value.  If a test does not know the parameter, it
            is not filtered out.  If a value is not given, eg. "-P np=" or
            "-P np", then any test that has the parameter name, eg. "np", is
            filtered out.  Can also use inequalities.  Specifications separated
            by '/' character are OR'ed together, and multiple -p/-P options
            are AND'ed together.
      
      -S param=value
            Set parameter to value.  More than one is accumulated and 'value'
            can be a space separated list.  Example: -S np=2 -S np="5 8".
      
      -x platform[/platform...]
            Include tests that would be included for the given platform name.
            Platform names separated by '/' mean OR.  This can be used to run
            the tests that would run on another platform.  Multiple -x and -X
            are allowed.  So "-x Linux -X SunOS" would run all tests that would
            normally run on Linux but not on SunOS.
      
      -X platform[/platform...]
            Exclude tests that would be included for the given platform name.
            Platform names separated by '/' mean OR.  This can be used to run
            the tests that are NOT run on another platform.
      
      -s <regular expression>, --search <regular expression>
            Search for the regular expression in the input file(s).  The files
            specified for soft link or copy are searched for the given regular
            expression.  May be repeated.  If any regular expressions match
            in any of the input files, then the test is included; otherwise it
            is filtered out.  This search is case insensitive.
      
      --include-tdd
            If a test has the keyword "TDD" in it, then it will be skipped by
            default.  Adding the --include-tdd option causes such tests to be
            included.  The acronym TDD stands for Test Driven Development,
            in which tests are added before the code is modified to make them
            work.  These tests are not "production" yet so the TDD keyword can
            be used to separate and distinguish them in testing and reporting.

      -j directory
            Passed along to the test launch scripts for use in finding the
            executables to run.  This defines the PROJECT variable available
            in the test scripts.

      --tmin <seconds>
      --tmax <seconds>
            Filter tests by previous runtime.  One or both --tmin and --tmax
            can be specified.  Tests are not run if the previous runtime is
            below the minimum or above the maximum.  Tests that do not have a
            previous runtime are not filtered out.

      --tsum <seconds>
            Filter tests by accumulated previous runtime.  Tests will be
            sorted by previous runtime (in ascending order), then accumulated
            until the sum of the runtimes is above the --tsum value.  Tests
            that do not have a previous runtime are given the value zero.

      -m    Do not overwrite existing scripts and no clean out.  By default,
            the generated test scripts are always overritten and all files
            in the test directory are removed.  Turning this option on
            will prevent a test script that exists from being overwritten
            and the test directory will not be cleaned out.
      
      --perms <permissions>
            Apply permission settings to files and directories in the test
            execution area.  Multiple --perms options can be used and/or the
            specifications can be comma separated.  If the specifcation starts
            with "g=" then the group permissions are set.  If it starts with
            "o=" then the world permissions are set.  If not one of these,
            then it must be the name of a UNIX group, and the files and
            directories have their group set.  Examples are "o=-", "g=rx,o=",
            "wg-alegra,g=rws,o=---".

      -C, --postclean
            Clean out the test directory of each test after it shows a "pass".
            Only those tests that "pass" are cleaned out after they run.  The
            execute.log file is not removed.
      
      -A    Ignore platform restrictions.  Tests can be excluded by platform
            name in the test specification file.  Using -A causes any such
            exclusions to be ignored.

      -R    Rerun tests.  If a test has been run and completes, then by default
            it will not be run again.  This option causes all such tests to be
            run again.  (This was the -F option, now deprecated.)

      --force
            Force vvtest to run.  When vvtest finishes running tests, a mark
            is placed in the testlist file.  If another vvtest execution is
            started while the first is still running, the second will refuse
            to run because it cannot find the mark.  It is possible that vvtest
            dies badly and the mark is never placed (and never will be).  So
            vvtest can be forced to run in this case by using --force.

      -M directory
            Use the given directory to actually contain the test results.
            The tests will run faster if a local disk is used to write to.
            The special value "any" will search for a scratch disk.

      --run-dir <subdir>
            Override the default test results directory naming mechanism of
            TestResults.platform.ON=...  It is the subdirectory under the
            current working directory in which all the tests will be run.

      -n integer
            Set the number of available processers to the given value.  By
            default, this number is set to the maximum number of processors
            on the platform.
      
      -N integer
            Set the maximum number of processers available on the current
            platform.  By default, the system is probed to determine this
            value.  Note that tests requiring more than this number of
            processors are not run.
      
      --plat platform_name
            Use the given platform name for the plugin and defaults.
            This can be used to specify the platform name, thus overriding
            the default platform name.  For example, you could use "--plat Linux".
      
      --platopt <optname>[=value]
            Platform options.  Some platform plugins understand special
            options.  Use this to send command line options into the platform
            plugin.  For example, use "--platopt ppn=4" to set the number
            of processors per node to an artificial value of 4.
      
      -T timeout
            Apply a timeout value in seconds to each test.  Zero means no
            timeout.  In batch mode, note that the time given to the batch
            queue for each batch job is the sum of the timeouts of each test.
      
      --timeout-multiplier mult
            Apply a float multiplier to the timeout value for each test.

      --max-timeout <seconds>
            Apply a maximum timeout value for each test and for batch jobs.
            It is the last operation performed when computing timeouts.

      -L    Do not use log files.  By default, each test output is sent to
            a log file contained in the run directory.  This option turns
            off this redirection.  This can be used to debug a single test
            for example, so that the results of the run are seen immediately.
            Note that "-n 1" can be used with the -L option to send many test
            results to stdout/stderr without interleaving the output (the
            "-n 1" prevents multiple tests from being run simultaneously).
      
      -e    Use environment variable overrides.  By default, all environment
            variables that can influence the execution of the tests are
            cleared.  This switch will allow certain environment variables
            to take precedence over the defaults determined by each platform
            plugin.
      
      -a, --analyze
            Only execute the analysis portions of the tests (and skip running
            the main code again).  This is only useful if the tests have
            already been run previously.
      
      --config <directory>
            Specify the directory containing the platform configuration and
            test helpers.  Vvtest looks for a file called platform_plugin.py
            in that directory.

            An environment variable can be used instead, VVTEST_CONFIGDIR.

      --batch, --pipeline
            Collect sets of tests in and submit them to a batch queue rather
            than executing individual tests as resources become open.  In
            order to use the queue submit capability of batch systems, the
            plugin for the platform must be set up to launch batch jobs.
            The option --pipeline is deprecated.
      
      --qsub-limit <integer>
            Used in conjunction with the --batch switch, this will limit
            the number of batch jobs submitted to the queue at any one time.
            The default is 5 concurrent batch jobs.
      
      --qsub-length=<integer>
            Used in conjunction with the --batch switch, this limits the
            number of tests that are placed into each batch set.  The sum
            of the timeouts of the tests in each set will be less than the
            given number of seconds.  The default is 30 minutes.  The longer
            the length, the more tests will go in each batch job; the shorter
            the length, the fewer.  A value of zero will force each test to
            run in a separate batch job.
      
      --check=<name>
            Activates optional sections in the test files.  The execution
            blocks in the test files may have an ifdef="CHECK_NAME" attribute,
            where the NAME portion is just upper case of <name>.  Those blocks
            are not active by default.  Using this option will cause those
            blocks to be executed as part of the test.

      --test-args=<arguments>
            Pass the given arguments to each test script invocation.  Can be
            option and/or non-option arguments.

      --save-results
            If used with the -i option, this saves test results for an
            existing run to the testing directory in a file called
            'results.<date>.<platform>.<compiler>.<tag>' where the "tag" is
            an optional, arbitrary string determined by the --results-tag
            option.  The testing directory is determined by the platform
            plugins, but can be overridden by defining the environment
            variable TESTING_DIRECTORY to the desired absolute path name.

            If used without the -i option, this causes an empty results
            file to be written at the start of the testing sequence, and a
            final results file to be written when the test sequence finishes.
      
      --results-tag=<string>
            When used with --save-results, this adds a string to the results
            file name within the testing directory.
      
      --junit=<filename>
            Writes a test summary to a file in the JUnit XML format.  Should
            be compatible with Jenkins JUnit test results plugin.  Can be used
            as part of the vvtest run, or given with the -i option.

      --vg, -vg
            Pass the -vg option to the test launch script.
'''

############################################################################

toolsdir = None
exeName = None

# the globally available option dictionary
optdict = {}

search_fnmatch = ['*.inp','*.apr','*.i']

testlist_name = 'testlist'

# a FilterExpressions.WordExpression() object filled with the command line
# -k and -K specifications
keyword_expr = None

class Configuration:
    
    defaults = { \
                 'toolsdir':None,  # the top level tools directory
                 'configdir':None,  # the configuration directory
                 'exepath':None,  # the path to the executables
                 'onopts':[],
                 'offopts':[],
                 'refresh':1,
                 'postclean':0,
                 'timeout':None,
                 'multiplier':1.0,
                 'preclean':1,
                 'analyze':0,
                 'logfile':1,
                 'testargs':[],
               }
    
    def get(self, name):
        """
        """
        return self.attrs[name]
    
    def set(self, name, value):
        """
        """
        if value == None:
          self.attrs[name] = Configuration.defaults[name]
        else:
          self.attrs[name] = value
    
    def __init__(self):
        self.attrs = {}
        for n,v in Configuration.defaults.items():
          self.attrs[n] = v


config = Configuration()


############################################################################

def XstatusString( t, add_date, test_dir, cwd ):
    """
    Returns a formatted string containing the job and its status.  If the
    add_date argument is true, it adds the date the job finished to the
    returned string.
    """
    
    if isinstance(t, TestExec.TestExec):
      ref = t.atest
      s = "%-20s " % ref.getName()
    else:
      ref = t
      s = "%-20s " % t.getName()
    
    state = ref.getAttr('state')
    if state != "notrun":
      
      if state == "done":
        result = ref.getAttr('result')
        if result == 'diff':
          s = s + "%-7s %-8s" % ("Exit", "diff")
        elif result ==  'pass':
          s = s + "%-7s %-8s" % ("Exit", "pass")
        elif result == "timeout":
          s = s + "%-7s %-8s" % ("TimeOut", 'SIGINT')
        else:
          s = s + "%-7s %-8s" % ("Exit", "fail(1)")
        
        xtime = ref.getAttr('xtime')
        if xtime >= 0: s = s + (" %-4s" % (str(xtime)+'s'))
        else:          s = s + "     "
      else:
        s = s + "%-7s %-8s     " % ("Running", "")
    
    else:
      s = s + "%-7s %-8s     " % ("NotRun", "")
    
    if add_date:
      xdate = ref.getAttr('xdate')
      if xdate > 0:
        s = s + time.strftime( " %m/%d %H:%M:%S", time.localtime(xdate) )
      else:
        s = s + "               "
    
    s += ' ' + relative_execute_directory( ref, test_dir, cwd )
    
    return s


def XstatusResult(t):
    
    if isinstance(t, TestExec.TestExec): ref = t.atest
    else:                        ref = t
    
    state = ref.getAttr('state')
    if state == "notrun" or state == "notdone":
      return state
    
    return ref.getAttr('result')


def relative_execute_directory( tst, testdir, cwd ):
    """
    Returns the test execute directory relative to the given current working
    directory.
    """
    xdir = tst.getExecuteDirectory()

    if testdir == None:
      return xdir
    
    d = os.path.join( testdir, xdir )
    sdir = issubdir( cwd, d )
    if sdir == None or sdir == "":
        return os.path.basename( xdir )
    
    return sdir


##############################################################################
#
# generation of tests

def generateTestList( dirs, plat, rtconfig ):
    """
    """
    # default scan directory is the current working directory
    if len(dirs) == 0: dirs = ['.']
    
    tlist = TestList.TestList( rtconfig )
    for d in dirs:
      if not os.path.exists(d):
        sys.stderr.write('*** ' + exeName + \
            ': error: directory does not exist: ' + d + '\n')
        sys.exit(1);
      tlist.scanDirectory( d, optdict.get('-S',None) )

    loadRunTimes( tlist, plat )

    tlist.loadAndFilter( plat.getMaxProcs() )
    
    test_dir = None
    cwd = os.getcwd()
    testdirname = testResultsDir( optdict, plat.getName() )
    test_dir = os.path.join( cwd, testdirname )

    perms = make_PermissionSetter( test_dir, optdict )
    
    createTestDir( testdirname, perms )
    writeCommandInfo( test_dir, plat, perms )
    printResults( tlist, test_dir, perms, add_date=False )
    tlist.createTestExecs( test_dir, plat, config, perms )

    tf = os.path.join( test_dir, testlist_name )
    tlist.stringFileWrite( tf )
    tlist.writeFinished()

    perms.set( os.path.abspath( tf ) )
    
    print3( "Test directory:", testdirname )


def extractTestFiles( dirs, plat, target_dir, rtconfig ):
    """
    Uses all the regular filtering mechanisms to gather tests from a test
    source area and copies the files used for each test into a separate
    directory.
    """
    # default scan directory is the current working directory
    if len(dirs) == 0: dirs = ['.']
    
    tlist = TestList.TestList( rtconfig )
    for d in dirs:
      if not os.path.exists(d):
        sys.stderr.write('*** ' + exeName + \
            ': error: directory does not exist: ' + d + '\n')
        sys.exit(1);
      tlist.scanDirectory( d, optdict.get('-S',None) )

    loadRunTimes( tlist, plat )

    tlist.loadAndFilter( plat.getMaxProcs() )
    
    if not os.path.isabs(target_dir):
      target_dir = os.path.abspath(target_dir)
    if not os.path.exists(target_dir):
      os.makedirs( target_dir )
    
    uniqD = {}
    
    def wvisit( arg, dname, dirs, files ):
        """
        copy a directory tree, but leave out version control files
        """
        for n in ['CVS','.cvsignore','.svn','.git','.gitignore']:
            while (n in dirs): dirs.remove(n)
            while (n in files): files.remove(n)
        fd = os.path.normpath( os.path.join( arg[0], dname ) )
        td = os.path.normpath( os.path.join( arg[1], dname ) )
        if not os.path.exists(td):
            os.makedirs(td)
        for f1 in files:
            f2 = os.path.join(fd,f1)
            tf = os.path.join(td,f1)
            shutil.copy2( f2, tf )
    
    for xdir,t in tlist.active.items():
      
      tname = t.getName()
      T = (tname, t.getFilename())
      
      from_dir = os.path.dirname( t.getFilename() )
      p = os.path.dirname( t.getFilepath() )
      if p: to_dir = os.path.normpath( os.path.join( target_dir, p ) )
      else: to_dir = target_dir
      
      if not os.path.exists( to_dir ):
        os.makedirs( to_dir )
      tof = os.path.join( target_dir, t.getFilepath() )
      
      if tof not in uniqD:
        uniqD[tof] = None
        try: shutil.copy2( t.getFilename(), tof )
        except IOError: pass
      
      for srcf in t.getSourceFiles():

        if os.path.exists( os.path.join( from_dir, srcf ) ):
            fL = [ srcf ]
        else:
            cwd = os.getcwd()
            try:
                os.chdir( from_dir )
                fL = glob.glob( srcf )
            except:
                fL = []
            os.chdir( cwd )

        for f in fL:
            fromf = os.path.join( from_dir, f )
            tof = os.path.join( to_dir, f )
            tod = os.path.dirname(tof)
            if tof not in uniqD:
              uniqD[tof] = None
              if not os.path.exists(tod):
                os.makedirs(tod)
              
              if os.path.isdir(fromf):
                cwd = os.getcwd()
                os.chdir(fromf)
                for root,dirs,files in os.walk( '.' ):
                    wvisit( (fromf, tof), root, dirs, files )
                os.chdir(cwd)
                
              else:
                try: shutil.copy2( fromf, tof )
                except IOError: pass


##############################################################################

def keywordInformation(tlist):
    
    if '--keys' in optdict:
      print3( "\nresults keywords: " + ' '.join( TestSpec.results_keywords ) )
      kd = {}
      for t in tlist.getActiveTests():
        for k in t.getKeywords():
          if k not in TestSpec.results_keywords and k != t.getName():
            kd[k] = None
      L = list( kd.keys() )
      L.sort()
      print3( "\ntest keywords: " )
      while len(L) > 0:
        k1 = L.pop(0)
        if len(L) > 0: k2 = L.pop(0)
        else:          k2 = ''
        if len(L) > 0: k3 = L.pop(0)
        else:          k3 = ''
        print3( "  %-20s %-20s %-20s" % (k1,k2,k3) )
    else:
      assert '--files' in optdict
      D = {}
      for t in tlist.getActiveTests():
        d = os.path.normpath( t.getFilename() )
        D[d] = None
      L = list( D.keys() )
      L.sort()
      for d in L:
        print3( d )

##############################################################################


def testResultsDir( optdict, platform_name ):
    """
    Generates and returns the directory name to hold test results, which is
    unique up to the platform and on/off options.
    """
    if '--run-dir' in optdict:
        testdirname = optdict['--run-dir']

    else:
        testdirname = 'TestResults.' + platform_name
        if '-o' in optdict:
          testdirname += '.ON=' + '_'.join( optdict['-o'] )
        if '-O' in optdict:
          testdirname += '.OFF=' + '_'.join( optdict['-O'] )
    
    return testdirname


def createTestDir( testdirname, perms ):
    """
    Create the given directory name.  If -M is given in the command line
    options, then a mirror directory is created and 'testdirname' will be
    created as a soft link pointing to the mirror directory.
    """
    if '-M' in optdict and \
            makeMirrorDirectory( optdict['-M'], testdirname, perms ):
        pass
    
    else:
        if os.path.exists( testdirname ):
            if not os.path.isdir( testdirname ):
                # replace regular file with a directory
                os.remove( testdirname )
                os.mkdir( testdirname )
        else:
            if os.path.islink( testdirname ):
                os.remove( testdirname )  # remove broken softlink
            os.mkdir( testdirname )

        perms.set( os.path.abspath( testdirname ) )


def makeMirrorDirectory( Mval, testdirname, perms ):
    """
    Create a directory in another location then soft link 'testdirname' to it.
    Returns False only if 'Mval' is the word "any" and a suitable scratch
    directory could not be found.
    """
    assert testdirname == os.path.basename( testdirname )

    if Mval == 'any':
      
        usr = getUserName()
        for d in ['/var/scratch', '/scratch', '/var/scratch1', '/scratch1', \
                  '/var/scratch2', '/scratch2', '/var/scrl1', '/gpfs1']:
            if os.path.exists(d) and os.path.isdir(d):
                ud = os.path.join( d, usr )
                if os.path.exists(ud):
                    if os.path.isdir(ud) and \
                       os.access( ud, os.X_OK ) and os.access( ud, os.W_OK ):
                        Mval = ud
                        break
                elif os.access( d, os.X_OK ) and os.access( d, os.W_OK ):
                    try:
                        os.mkdir(ud)
                    except:
                        pass
                    else:
                        Mval = ud
                        break
        
        if Mval == 'any':
            return False  # a scratch dir could not be found
        
        # include the current directory name in the mirror location
        curdir = os.path.basename( os.getcwd() )
        Mval = os.path.join( Mval, curdir )

        if not os.path.exists( Mval ):
            os.mkdir( Mval )

    else:
        Mval = os.path.abspath( Mval )
    
    if not os.path.exists( Mval ) or not os.path.isdir( Mval ) or \
       not os.access( Mval, os.X_OK ) or not os.access( Mval, os.W_OK ):
        raise Exception( "invalid or non-existent mirror directory: "+Mval )

    if os.path.samefile( Mval, os.getcwd() ):
        raise Exception( "mirror directory and current working directory " + \
                "are the same: "+Mval+' == '+os.getcwd() )

    mirdir = os.path.join( Mval, testdirname )

    if os.path.exists( mirdir ):
        if not os.path.isdir( mirdir ):
            # replace regular file with a directory
            os.remove( mirdir )
            os.mkdir( mirdir )
    else:
        if os.path.islink( mirdir ):
            os.remove( mirdir )  # remove broken softlink
        os.mkdir( mirdir )
    
    perms.set( os.path.abspath( mirdir ) )

    if os.path.islink( testdirname ):
        path = os.readlink( testdirname )
        if path != mirdir:
            os.remove( testdirname )
            os.symlink( mirdir, testdirname )

    else:
        if os.path.exists( testdirname ):
            if os.path.isdir( testdirname ):
                shutil.rmtree( testdirname )
            else:
                os.remove( testdirname )
        os.symlink( mirdir, testdirname )
    
    return True


def writeCommandInfo( test_dir, plat, perms ):
    """
    Creates the test results information file.
    """
    f = os.path.join(test_dir, 'test.cache')
    if not os.path.exists( f ):
      fp = open( f, "w" )
      fp.write( 'VERSION=' + str(version) + '\n' )
      fp.write( 'DIR=' + os.getcwd() + '\n' )
      if '--plat' in optdict:
        fp.write( 'PLATFORM=' + optdict['--plat'].strip() + '\n' )
      else:
        fp.write( 'PLATFORM=' + plat.getName() + '\n' )
      if '-p' in optdict:
        fp.write( 'PARAMETERS=' + str( optdict['-p'] ) + '\n' )
      if config.get('exepath'):
        fp.write( \
            'PROJECT=' + os.path.abspath( config.get('exepath') ) + '\n' )
      if '-o' in optdict:
        fp.write( 'ONOPTS=' + '+'.join(optdict['-o']) + '\n' )
      if '-O' in optdict:
        fp.write( 'OFFOPTS=' + '+'.join(optdict['-O']) + '\n' )
      if '-T' in optdict:
        fp.write( 'TIMEOUT=' + str(optdict['-T']).strip() + '\n' )
      if '--timeout-multiplier' in optdict:
        fp.write( 'TIMEOUT_MULTIPLIER=' + \
             str(optdict['--timeout-multiplier']).strip() + '\n' )
      if '-e' in optdict:
        fp.write( 'USE_ENV=1\n' )
      if '-A' in optdict:
        fp.write( 'ALL_PLATFORMS=1\n' )
      if '--include-tdd' in optdict:
        fp.write( 'INCLUDE_TDD=True\n' )
      if '--check' in optdict:
        fp.write( 'CHECK=' + ' '.join( optdict['--check'] ) + '\n' )
      fp.close()

    perms.set( os.path.abspath(f) )


def readCommandInfo():
    """
    Check for a file called 'test.cache' that indicates whether the
    current working directory is a TestResults directory (or subdirectory)
    then open that file for information.  The test results directory is
    returned, or None if not in a TestRestults directory.
    """
    # an environment variable is used to identify vvtest run recursion
    troot = os.environ.get( 'VVTEST_TEST_ROOT', None )

    test_cache = misc.find_vvtest_test_root_file(
                                        os.getcwd(), troot, 'test.cache' )

    if test_cache != None:
      
      if '-o' in optdict or '-O' in optdict or '-g' in optdict:
        sys.stderr.write('*** ' + exeName + \
            ': error: the -g, -o, and -O options are not allowed ' + \
            'in a TestResults directory\n')
        sys.exit(1);
      
      fp = open( test_cache, "r" )
      write_version = 0
      for line in fp.readlines():
        line = line.strip()
        kvpair = line.split( '=', 1 )
        if kvpair[0] == 'VERSION':
          write_version = int(kvpair[1])
        elif kvpair[0] == 'DIR':
          previous_run_dir = kvpair[1]
        elif kvpair[0] == 'PLATFORM':
          optdict['--plat'] = kvpair[1]
        elif kvpair[0] == 'PARAMETERS':
          L = eval( kvpair[1] )
          if '-p' in optdict: optdict['-p'].extend(L)
          else:               optdict['-p'] = L
        elif kvpair[0] == 'PROJECT':
          # do not replace if the command line contains -j
          if '-j' not in optdict:
            optdict['-j'] = kvpair[1]
            config.set( 'exepath', kvpair[1] )
        elif kvpair[0] == 'ONOPTS':
          optdict['-o'] = kvpair[1].split( '+' )
          config.set( 'onopts', optdict['-o'] )
        elif kvpair[0] == 'OFFOPTS':
          optdict['-O'] = kvpair[1].split( '+' )
          config.set( 'offopts', optdict['-O'] )
        elif kvpair[0] == 'TIMEOUT':
          # do not replace if the command line contains -T
          if '-T' not in optdict:
            optdict['-T'] = float(kvpair[1])
            config.set( 'timeout', optdict['-T'] )
        elif kvpair[0] == 'TIMEOUT_MULTIPLIER':
          if '--timeout-multiplier' not in optdict:
            optdict['--timeout-multiplier'] = float(kvpair[1])
            config.set( 'multiplier', optdict['--timeout-multiplier'] )
        elif kvpair[0] == 'USE_ENV':
          optdict['-e'] = ''
        elif kvpair[0] == 'ALL_PLATFORMS':
          optdict['-A'] = ''
        elif kvpair[0] == 'INCLUDE_TDD':
          optdict['--include-tdd'] = ''
        elif kvpair[0] == 'CHECK':
          optdict['--check'] = kvpair[1].split()
      fp.close()

    if test_cache != None:
        return os.path.dirname( test_cache )
    return None


def runTests( dirs, plat, rtconfig ):
    """
    Executes a list of tests.
    """
    # determine the directory that stores the test results then create it
    testdirname = testResultsDir( optdict, plat.getName() )
    test_dir = os.path.join( os.getcwd(), testdirname )
    tfile = os.path.join( test_dir, testlist_name )

    if os.path.exists( tfile ):
        tl = TestList.TestList()
        tl.scanFile( tfile )
        if tl.getFinishDate() == None and '--force' not in optdict:
            print3( '*** error: tests are currently running in another process' )
            print3( '    (or a previous run crashed); use --force to run anyway' )
            sys.exit(1)
        tl = None

    # this variable allows vvtest tests to run vvtest (ie, allows recursion)
    os.environ['VVTEST_TEST_ROOT'] = os.path.normpath( os.path.abspath( test_dir ) )

    perms = make_PermissionSetter( test_dir, optdict )
    
    createTestDir( testdirname, perms )
    
    if '-w' in optdict:
      print3( '/bin/rm -rf ' + testdirname + '/*' )
      for f in os.listdir(testdirname):
        df = os.path.join( testdirname, f )
        if os.path.isdir(df):
          shutil.rmtree( df, 0 )
        else:
          try: os.remove(df)
          except: pass
    
    writeCommandInfo( test_dir, plat, perms )
    
    tlist = TestList.TestList( rtconfig )

    # generate test list by scanning directories
    if len(dirs) == 0:
      dirs = ['.']
    for d in dirs:
      if not os.path.exists(d):
        sys.stderr.write('*** ' + exeName + \
            ': error: directory does not exist: ' + d + '\n')
        sys.exit(1);
      tlist.scanDirectory( d, optdict.get('-S',None) )
    
    if os.path.exists( tfile ):
        tlist.readFile( tfile )  # merge in existing test results
    
    loadRunTimes( tlist, plat )

    pruneL,cntmax = tlist.loadAndFilter( plat.getMaxProcs(),
                                         analyze_only='-a' in optdict,
                                         prune=True )
    
    if len(pruneL) > 0:
        print3()
        for pt,ct in pruneL:
            print3( '*** Warning: analyze test',
                    relative_execute_directory( pt, test_dir, os.getcwd() ),
                    'will not be run due to dependency',
                    relative_execute_directory( ct, test_dir, os.getcwd() ) )
    if cntmax > 0:
        print3()
        print3( 'Note: there were', cntmax, 'tests that exceeded the',
                'maximum number of processors - they will not be run' )

    # save the test list in the TestResults directory
    tlist.stringFileWrite( tfile )

    try:
        
        perms.set( os.path.abspath( tfile ) )
        
        if len(tlist.active) > 0:
          
          print3( "Running these tests:" )
          printResults( tlist, test_dir, perms, short=True )
          print3()

          tlist.createTestExecs( test_dir, plat, config, perms )
          
          if '--save-results' in optdict:
            saveResults( tlist, plat, test_dir, inprogress=True )

          if '--batch' not in optdict:
            executeTestList( tlist, test_dir, plat, perms, tfile )
          
          else:
            batchTestList( tlist, test_dir, plat, perms )
        
          print3( "Test directory:", testdirname )
          
          if '--save-results' in optdict:
            saveResults( tlist, plat, test_dir )
        
        else:
          print3( "\n--------- no tests to run -----------\n" )
    
    except:
        tlist.writeFinished()
        raise
    tlist.writeFinished()


def printResults( atestlist, test_dir, perms,
                  short=False, add_date=True,
                  tostdout=True, tohtml=False, tojunit='' ):
    """
    Prints a summary to the screen and also creates an HTML summary file.
    If 'short' is True, then only a handfull of tests are written to the
    screen.
    """
    rawlist = atestlist.getActiveTests( optdict.get( '--sort', '' ) )
    
    Lfail = []; Ltime = []; Ldiff = []; Lpass = []; Lnrun = []; Lndone = []
    for atest in rawlist:
        statr = XstatusResult(atest)
        if   statr == "fail":    Lfail.append(atest)
        elif statr == "timeout": Ltime.append(atest)
        elif statr == "diff":    Ldiff.append(atest)
        elif statr == "pass":    Lpass.append(atest)
        elif statr == "notrun":  Lnrun.append(atest)
        elif statr == "notdone": Lndone.append(atest)
    sumstr = str(len(Lpass)) + " pass, " + \
             str(len(Ltime)) + " timeout, " + \
             str(len(Ldiff)) + " diff, " + \
             str(len(Lfail)) + " fail, " + \
             str(len(Lnrun)) + " notrun, " + \
             str(len(Lndone)) + " notdone"
    
    if tostdout:
        cwd = os.getcwd()
        print3( "==================================================" )
        if short and len(rawlist) > 16:
            for atest in rawlist[:8]:
                print3( XstatusString( atest, add_date, test_dir, cwd ) )
            print3( "..." )
            for atest in rawlist[-8:]:
                print3( XstatusString( atest, add_date, test_dir, cwd ) )
        else:
            for atest in rawlist:
                print3( XstatusString( atest, add_date, test_dir, cwd ) )
        print3( "==================================================" )
        print3( "Summary:", sumstr )
    
    if tohtml:
        printHTMLResults( sumstr, test_dir, perms,
                          Lfail, Ltime, Ldiff, Lpass, Lnrun, Lndone )

    if tojunit:

        rdate = optdict.get( '--results-date', None )
        if rdate != None:
          try:
            datestamp = float(rdate)
          except:
            print3( '*** vvtest error: --results-date must be seconds ' + \
                    'since epoch for use with --junit option' )
            sys.exit(1)
        else:
          datestamp = atestlist.getDateStamp( time.time() )
        
        print3( "Writing", len(rawlist), "tests to JUnit file", tojunit )
        results.write_JUnit_file( test_dir, rawlist, tojunit, datestamp )


def printHTMLResults( sumstr, test_dir, perms,
                      Lfail, Ltime, Ldiff, Lpass, Lnrun, Lndone ):
    """
    Opens and writes an HTML summary file in the test directory.
    """
    
    if test_dir == ".":
      test_dir = os.getcwd()
    if not os.path.isabs(test_dir):
      test_dir = os.path.abspath(test_dir)
    
    fp = open("summary.htm","w")
    fp.write( "<html>\n<head>\n<title>Test Results</title>\n" )
    fp.write( "</head>\n<body>\n" )
    
    # a summary section
    
    fp.write( "<h1>Summary</h1>\n" )
    fp.write( "  <ul>\n" )
    fp.write( "  <li> Directory: " + test_dir + "\n" )
    fp.write( "  <li> Options: " )
    if '-o' in optdict:
      fp.write( ' -o ' + '+'.join(optdict['-o']) )
    if '-O' in optdict:
      fp.write( ' -O ' + '+'.join(optdict['-O']) )
    if '-j' in optdict:
      fp.write( ' -j ' + optdict['-j'] )
    if '--plat' in optdict:
      fp.write( ' --plat ' + optdict['--plat'] )
    if '-T' in optdict:
      fp.write( ' -T ' + str(optdict['-T']) )
    if '--timeout-multiplier' in optdict:
      fp.write( ' --timeout-multiplier ' + \
                        str(optdict['--timeout-multiplier']) )
    if '-e' in optdict:
      fp.write( ' -e ' )
    fp.write( "  </li>\n" )
    fp.write( "  <li> " + sumstr + "</li>\n" )
    fp.write( "  </ul>\n" )
    
    # segregate the tests into implicit keywords, such as fail and diff
    
    fp.write( '<h1>Tests that showed "fail"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Lfail )
    fp.write( '<h1>Tests that showed "timeout"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Ltime )
    fp.write( '<h1>Tests that showed "diff"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Ldiff )
    fp.write( '<h1>Tests that showed "notdone"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Lndone )
    fp.write( '<h1>Tests that showed "pass"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Lpass )
    fp.write( '<h1>Tests that showed "notrun"</h1>\n' )
    writeHTMLTestList( fp, test_dir, Lnrun )
    
    fp.write( "</body>\n</html>\n" )
    fp.close()

    perms.set( os.path.abspath( 'summary.htm' ) )


def writeHTMLTestList( fp, test_dir, tlist ):
    """
    Used by printHTMLResults().  Writes the HTML for a list of tests to the
    HTML summary file.
    """
    cwd = os.getcwd()
    
    fp.write( '  <ul>\n' )
    
    for atest in tlist:
      
      fp.write( '  <li><code>' + XstatusString(atest, 1, test_dir, cwd) + '</code>\n' )
      
      if isinstance(atest, TestExec.TestExec): ref = atest.atest
      else:                            ref = atest
      
      tdir = os.path.join( test_dir, ref.getExecuteDirectory() )
      assert cwd == tdir[:len(cwd)]
      reltdir = tdir[len(cwd)+1:]
      
      fp.write( "<ul>\n" )
      thome = atest.getRootpath()
      xfile = os.path.join( thome, atest.getFilepath() )
      fp.write( '  <li>XML: <a href="file://' + xfile + '" ' + \
                       'type="text/plain">' + xfile + "</a></li>\n" )
      fp.write( '  <li>Parameters:<code>' )
      for (k,v) in atest.getParameters().items():
        fp.write( ' ' + k + '=' + v )
      fp.write( '</code></li>\n' )
      fp.write( '  <li>Keywords: <code>' + ' '.join(atest.getKeywords()) + \
                 ' ' + ' '.join( atest.getResultsKeywords() ) + \
                 '</code></li>\n' )
      fp.write( '  <li>Status: <code>' + XstatusString(atest, 1, test_dir, cwd) + \
                 '</code></li>\n' )
      fp.write( '  <li> Files:' )
      if os.path.exists(reltdir):
        for f in os.listdir(reltdir):
          fp.write( ' <a href="file:' + os.path.join(reltdir,f) + \
                    '" type="text/plain">' + f + '</a>' )
      fp.write( '</li>\n' )
      fp.write( "</ul>\n" )
      fp.write( "</li>\n" )
    fp.write( '  </ul>\n' )


def executeTestList( tlist, test_dir, plat, perms, tfile ):
    """
    """
    plat.display()
    starttime = time.time()
    print3( "Start time:", time.ctime() )
    
    # execute tests

    perms.set( os.path.abspath( tfile ) )

    cwd = os.getcwd()

    while True:

        tnext = tlist.popNext( plat )

        if tnext != None:
            print3( 'Starting:',
                    relative_execute_directory( tnext.atest, test_dir, cwd ) )
            tnext.start()
            tlist.AppendTestResult( tnext.atest )
        
        elif tlist.numRunning() == 0:
            break

        else:
            time.sleep(1)

        showprogress = False
        for tx in list( tlist.getRunning() ):
            if tx.poll():
                xs = XstatusString( tx, False, test_dir, cwd )
                print3( "Finished:", xs )
                tlist.testDone( tx )
                showprogress = True
      
        unit_test_hook( 'tests', tlist.numRunning(), tlist.numDone() )

        if showprogress:
            ndone = tlist.numDone()
            ntot = tlist.numActive()
            pct = 100 * float(ndone) / float(ntot)
            div = str(ndone)+'/'+str(ntot)
            dt = pretty_time( time.time() - starttime )
            print3( "Progress: " + div+" = %%%.1f"%pct + ', time = '+dt )
    
    # any remaining tests cannot run, so print warnings
    tL = tlist.popRemaining()
    if len(tL) > 0:
        print3()
    for tx in tL:
        deptx = tx.getBlockingDependency()
        assert tx.hasDependency() and deptx != None
        print3( '*** Warning: analyze test skipped for "' + \
                tx.atest.getExecuteDirectory() + \
                '" due to dependency "' + deptx.atest.getExecuteDirectory() + '"' )
    
    do_html = ( '--qsub-id' not in optdict )
    do_junit = ''
    if '--qsub-id' not in optdict and '--junit' in optdict:
        do_junit = optdict['--junit']
    print3()
    printResults( tlist, test_dir, perms, tohtml=do_html, tojunit=do_junit )
    
    elapsed = pretty_time( time.time() - starttime )
    print3( "\nFinish date:", time.ctime() + " (elapsed time "+elapsed+")" )


def batchTestList( tlist, test_dir, plat, perms ):
    """
    The 'tlist' is a TestList class instance.
    """
    assert '--qsub-id' not in optdict

    qsublimit = optdict.get( '--qsub-limit', plat.getDefaultQsubLimit() )
    
    batch = Batcher( plat, test_dir, tlist, perms, qsublimit )
    
    plat.display()
    starttime = time.time()
    print3( "Start time:", time.ctime() )

    # write testlist files for each qsub
    numjobs = batch.writeQsubScripts()

    print3( 'Total number of batch jobs: ' + str(numjobs) + \
            ', maximum concurrent jobs: ' + str(qsublimit) )

    if '-g' in optdict:
      return
    
    schedule = batch.getScheduler()

    cwd = os.getcwd()
    qsleep = int( os.environ.get( 'VVTEST_BATCH_SLEEP_LENGTH', 15 ) )
    
    while True:

        qid = schedule.checkstart()
        if qid != None:
            # nothing to print here because the qsubmit prints
            pass
        elif schedule.numInFlight() == 0:
            break
        else:
            time.sleep( qsleep )

        qidL,doneL = schedule.checkdone()
        
        if len(qidL) > 0:
            ids = ' '.join( [ str(qid) for qid in qidL ] )
            print3( 'Finished batch IDS:', ids )
        for t in doneL:
            ts = XstatusString( t, False, test_dir, cwd )
            print3( "Finished:", ts )
        
        unit_test_hook( 'batch', schedule.numInFlight(), schedule.numDone() )

        if len(doneL) > 0:
            jpct = 100 * float(schedule.numDone()) / float(numjobs)
            jdiv = 'jobs '+str(schedule.numDone())+'/'+str(numjobs)
            jflt = '(in flight '+str(schedule.numStarted())+')'
            ndone = tlist.numDone()
            ntot = tlist.numActive()
            tpct = 100 * float(ndone) / float(ntot)
            tdiv = 'tests '+str(ndone)+'/'+str(ntot)
            dt = pretty_time( time.time() - starttime )
            print3( "Progress: " + \
                    jdiv+" = %%%.1f"%jpct + ' '+jflt+', ' + \
                    tdiv+" = %%%.1f"%tpct + ', ' + \
                    'time = '+dt )

    # any remaining tests cannot be run; flush then print warnings
    NS, NF, nrL = schedule.flush()
    if len(NS)+len(NF)+len(nrL) > 0:
        print3()
    if len(NS) > 0:
      print3( "*** Warning: these batch numbers did not seem to start:",
              ' '.join(NS) )
    if len(NF) > 0:
      print3( "*** Warning: these batch numbers did not seem to finish:",
              ' '.join(NF) )
    for tx,deptx in nrL:
        assert tx.hasDependency() and deptx != None
        print3( '*** Warning: analyze test skipped for "' + \
                tx.atest.getExecuteDirectory() + \
                '" due to dependency "' + deptx.atest.getExecuteDirectory() + '"' )
    
    print3()
    do_junit = optdict.get( '--junit', '' )
    printResults( tlist, test_dir, perms, tohtml=True, tojunit=do_junit )
    
    elapsed = pretty_time( time.time() - starttime )
    print3( "\nFinish date:", time.ctime() + " (elapsed time "+elapsed+")" )


def pretty_time( nseconds ):
    """
    Returns a string with the given number of seconds written in an easily
    readable form.
    """
    h = int( nseconds / 3600 )
    sh = str(h)+'h'

    m = int( ( nseconds - 3600*h ) / 60 )
    sm = str(m)+'m'

    s = int( ( nseconds - 3600*h - 60*m ) )
    if h == 0 and m == 0 and s == 0: s = 1
    ss = str(s) + 's'

    if h > 0: return sh+' '+sm+' '+ss
    if m > 0: return sm+' '+ss
    return ss


def unit_test_hook( loop, numrun, numdone ):
    """
    The VVTEST_INTERRUPT_COUNT and VVTEST_INTERRUPT_BATCH environment variables
    can be used to interrupt the execution sequence for unit testing.
    """
    if loop == 'tests':
        snum = os.environ.get( 'VVTEST_INTERRUPT_COUNT', '' )
        if snum:
            sqid = os.environ.get( 'VVTEST_INTERRUPT_QID', '' )
            qok = True
            if sqid and '--qsub-id' in optdict:
                assert type( optdict['--qsub-id'] ) == type('')
                # if the QID is defined and equal to this batch job's queue id
                # then the interrupt will take place; if it is defined but
                # not equal to this job's queue id then no interrupt will occur
                if sqid != optdict['--qsub-id']:
                    qok = False
            if qok and numdone >= int(snum):
                sig = os.environ.get( 'VVTEST_SIGNAL', 'SIGINT' )
                sig = eval( 'signal.'+sig )
                os.kill( os.getpid(), sig )
                time.sleep(60)
                sys.exit(1)
    elif loop == 'batch':
        snum = os.environ.get( 'VVTEST_INTERRUPT_BATCH', '' )
        if snum:
            if numdone >= int(snum):
                sig = os.environ.get( 'VVTEST_SIGNAL_BATCH', 'SIGINT' )
                sig = eval( 'signal.'+sig )
                os.kill( os.getpid(), sig )
                time.sleep(60)
                sys.exit(1)


def loadRunTimes( tlist, plat ):
    """
    For each test, a 'runtimes' file will be read (if it exists) and the
    run time for this platform extracted.  This run time is saved in the
    test as the 'runtime' attribute.  Also, a timeout is calculated for
    each test and placed in the 'timeout' attribute.
    """
    pname = plat.getName()
    cplr = plat.getCompiler()

    cache = results.LookupCache( pname, cplr, plat.testingDirectory() )

    for t in tlist.getTests():

        # grab explicit timeout value, if the test specifies it
        tout = t.getTimeout()

        # look for a previous runtime value
        tlen,tresult = cache.getRunTime( t )

        if tlen != None:

            t.setAttr( 'runtime', int(tlen) )

            if tout == None:
                if tresult == "timeout":
                    # for tests that timed out, make timeout much larger
                    if t.hasKeyword( "long" ):
                        # only long tests get timeouts longer than an hour
                        if tlen < 60*60:
                            tout = 4*60*60
                        elif tlen < 5*24*60*60:  # even longs are capped
                            tout = 4*tlen
                    else:
                        tout = 60*60

                else:
                    # pick timeout to allow for some runtime variability
                    if tlen < 120:
                        tout = max( 120, 2*tlen )
                    elif tlen < 300:
                        tout = max( 300, 1.5*tlen )
                    elif tlen < 4*60*60:
                        tout = int( float(tlen)*1.5 )
                    else:
                        tout = int( float(tlen)*1.3 )

        else:  # no previous result

            if tout != None:
                # use the explicit timeout value as the runtime
                tlen = tout
            else:
                # with no information, the default depends on 'long' keyword
                if t.hasKeyword("long"):
                    tlen = 5*60*60  # five hours
                else:
                    tlen = 60*60  # one hour
                tout = tlen

        tout = int( optdict.get( '-T', tout ) )
        tout = int( float(tout) * optdict.get('--timeout-multiplier',1.0) )

        mx =optdict.get( '--max-timeout', 0 )
        if mx > 0:
            tout = min( tout, mx )

        t.setAttr( 'timeout', tout )

    cache = None


class Batcher:
    
    def __init__(self, plat, test_dir, tlist, perms, maxjobs):
        """
        The 'tlist' is a TestList class instance.
        """
        self.plat = plat
        self.test_dir = test_dir
        self.tlist = tlist
        self.perms = perms
        self.maxjobs = maxjobs
        self.clean_exit_marker = "queue job finished cleanly"

        # TODO: make Tzero a platform plugin thing
        self.Tzero = 21*60*60  # no timeout in batch mode is 21 hours

        # allow these values to be set by environment variable, mainly for
        # unit testing; if setting these is needed more regularly then a
        # command line option should be added
        val = int( os.environ.get( 'VVTEST_BATCH_READ_INTERVAL', 30 ) )
        self.read_interval = val
        val = int( os.environ.get( 'VVTEST_BATCH_READ_TIMEOUT', 5*60 ) )
        self.read_timeout = val
        
        self.accountant = batchutils.BatchAccountant()
        self.namer = batchutils.BatchFileNamer( self.test_dir, testlist_name )

        self.createTestGroups()

        self.scheduler = batchutils.BatchScheduler(
                            self.test_dir, self.tlist,
                            self.accountant, self.namer,
                            self.perms, self.plat, self.maxjobs,
                            self.clean_exit_marker )

    def getScheduler(self):
        return self.scheduler

    def removeBatchDirectories(self):
        """
        """
        for d in self.namer.globBatchDirectories():
            print3( 'rm -rf '+d )
            shutil.rmtree( d )

    def createTestGroups(self):
        """
        """
        qlen = optdict.get( '--qsub-length', 30*60 )
        qL = []
        for np in self.tlist.getTestExecProcList():
          xL = []
          for tx in self.tlist.getTestExecList(np):
            xL.append( (tx.atest.getAttr('timeout'),tx) )
          xL.sort()
          grpL = []
          tsum = 0
          for rt,tx in xL:
            if tx.hasDependency() or tx.atest.getAttr('timeout') < 1:
              # analyze tests and those with no timeout get their own group
              qL.append( [ self.Tzero, len(qL), [tx] ] )
            else:
              if len(grpL) > 0 and tsum + rt > qlen:
                qL.append( [ tsum, len(qL), grpL ] )
                grpL = []
                tsum = 0
              grpL.append( tx )
              tsum += rt
          if len(grpL) > 0:
            qL.append( [ tsum, len(qL), grpL ] )
        
        qL.sort()
        qL.reverse()
        self.qsublists = map( lambda L: L[2], qL )

    def writeQsubScripts(self):
        """
        """
        self.removeBatchDirectories()

        commonopts = ''
        if '-e' in optdict: commonopts += ' -e'
        if '-m' in optdict: commonopts += ' -m'
        if '-C' in optdict: commonopts += ' -C'
        if '-a' in optdict: commonopts += ' -a'
        if '-N' in optdict: commonopts += ' -N '+str( optdict['-N'] )
        for s in optdict.get('-p',[]):
            s = s.replace( '!', '\\!' )  # have to escape exclamation points
            commonopts += ' -p "'+s+'"'
        if '--perms' in optdict:
            val = ','.join( optdict['--perms'] )
            commonopts += ' --perms '+val
        if config.get('configdir'):
            commonopts += ' --config='+config.get('configdir')
        for k,v in optdict.get('--platopt',{}).items():
            commonopts += ' --platopt ' + k + '=' + v
        for arg in config.get('testargs'):
            commonopts += ' --test-args="'+arg+'"'

        qsubids = {}  # maps batch id to max num processors for that batch
        
        qid = 0
        for qL in self.qsublists:
          self.make_queue_batch( qid, qL, qsubids, commonopts )
          qid += 1
        
        qidL = list( qsubids.keys() )
        qidL.sort()
        for i in qidL:
          incl = self.namer.getTestListName( i, relative=True )
          self.tlist.AddIncludeFile( incl )
        
        for i in qidL:
            d = self.namer.getSubdir( i )
            self.perms.recurse( d )

        return len( qsubids )
    
    def make_queue_batch(self, qnumber, qlist, npD, comopts):
        """
        """
        qidstr = str(qnumber)
        
        tl = TestList.TestList()
        tL = []
        maxnp = 0
        qtime = 0
        for tx in qlist:
          np = int( tx.atest.getParameters().get('np', 0) )
          if np <= 0: np = 1
          maxnp = max( maxnp, np )
          tl.addTest(tx.atest)
          tL.append( tx )
          qtime += int( tx.atest.getAttr('timeout') )
        
        if qtime == 0:
          qtime = self.Tzero  # give it the "no timeout" length of time
        else:
          # allow more time in the queue than calculated
          if qtime < 60:
            qtime = 120
          elif qtime < 600:
            qtime *= 2
          else:
            qtime = int( float(qtime) * 1.3 )

        mx =optdict.get( '--max-timeout', 0 )
        if mx > 0:
            qtime = min( qtime, mx )
        
        npD[qnumber] = maxnp
        pout = self.namer.getBatchOutputName( qnumber )
        tout = self.namer.getTestListName( qnumber )

        jb = batchutils.BatchJob( maxnp, pout, tout, tL,
                                  self.read_interval, self.read_timeout )
        self.accountant.addJob( qnumber, jb )
        
        fn = self.namer.getTestListName( qidstr )
        tl.stringFileWrite( fn )
        tl.writeDependencies( self.tlist.getTests() )
        tl.writeFinished()
        
        fn = self.namer.getBatchScriptName( qidstr )
        fp = open( fn, "w" )
        
        hdr = self.plat.getQsubScriptHeader( maxnp, qtime, self.test_dir, pout )
        fp.writelines( [ hdr + '\n\n',
                         'cd ' + self.test_dir + ' || exit 1\n',
                         'echo "job start time = `date`"\n' + \
                         'echo "job time limit = ' + str(qtime) + '"\n' ] )
        
        # set the environment variables from the platform into the script
        for k,v in self.plat.getEnvironment().items():
          fp.write( 'setenv ' + k + ' "' + v  + '"\n' )
        
        # collect relevant options to pass to the qsub vvtest invocation
        taopts = '--qsub-id=' + qidstr + ' '
        taopts += comopts
        if len(qlist) == 1:
          # force a timeout for batches with only one test
          if qtime < 600: taopts += ' -T ' + str(qtime*0.90)
          else:           taopts += ' -T ' + str(qtime-120)
        
        cmd = toolsdir+'/vvtest ' + taopts + ' || exit 1'
        fp.writelines( [ cmd+'\n\n' ] )
        
        # echo a marker to determine when a clean batch job exit has occurred
        fp.writelines( [ 'echo "'+self.clean_exit_marker+'"\n' ] )
        
        fp.close()


def saveResults( tlist, plat, test_dir, inprogress=False ):
    """
    Option is
    
      --save-results
    
    which writes to the platform config testing directory (which looks first at
    the TESTING_DIRECTORY env var).  Can add
    
      --results-tag <string>
    
    which is appended to the results file name.  A date string is embedded in
    the file name, which is obtained from the date of the first test that
    ran.  But if the option

      --results-date <float or string>

    is given on the vvtest command line, then that date is used instead.
    """
    pname = plat.getName()
    cplr = plat.getCompiler()

    rtag = optdict.get( '--results-tag', None )
    
    # determine the date stamp to embed in the file name
    rdate = optdict.get( '--results-date', None )
    if rdate != None:
      try:
        assert '_' not in rdate  # recent python 3 allows underscores
        val = float(rdate)
        # assume format was seconds since epoch
        datestr = time.strftime( "%Y_%m_%d", time.localtime( val ) )
      except:
        datestr = rdate  # just take date string verbatim
    else:
      datestamp = tlist.getDateStamp( time.time() )
      datestr = time.strftime( "%Y_%m_%d", time.localtime( datestamp ) )

    L = []
    for o in optdict.get('-o',[]):
      if o != cplr:
        L.append(o)
    L.sort()
    L.insert( 0, cplr )
    optstag = '+'.join(L)
    
    rdir = plat.testingDirectory()
    if rdir == None or not os.path.isdir(rdir):
      raise Exception( "invalid testing directory: " + str(rdir) )
    
    L = ['results',datestr,pname,optstag]
    if rtag != None: L.append(rtag)
    fname = os.path.join( rdir, '.'.join( L ) )
    
    tr = results.TestResults()
    for t in tlist.getActiveTests():
      tr.addTest(t)
    mach = os.uname()[1]
    tr.writeResults( fname, pname, cplr, mach, test_dir, inprogress )


def restartTests( test_dir, plat, rtconfig ):
    ""
    assert test_dir != None

    # this variable allows vvtest tests to run vvtest (ie, allows recursion)
    os.environ['VVTEST_TEST_ROOT'] = os.path.normpath( os.path.abspath( test_dir ) )

    path_filter = os.getcwd()
    
    qid = optdict.get( '--qsub-id', None )
    if qid == None:
        tfile = os.path.join( test_dir, testlist_name )
    else:
        # batch jobs have --qsub-id set and land here
        namer = batchutils.BatchFileNamer( test_dir, testlist_name )
        tfile = namer.getTestListName( qid )
        # prevent the run scripts from being written again
        config.set( 'refresh', False )

    tlist = TestList.TestList( rtconfig )
    tlist.readFile( tfile )

    if tlist.getFinishDate() == None and '--force' not in optdict:
        print3( '*** error: tests are currently running in another process' )
        print3( '    (or a previous run crashed); use --force to run anyway' )
        sys.exit(1)

    if qid == None:
        loadRunTimes( tlist, plat )

    reld = computeRelativePath( os.path.abspath(test_dir), os.getcwd() )
    
    pruneL,cntmax = tlist.loadAndFilter( plat.getMaxProcs(),
                                         filter_dir=reld,
                                         analyze_only='-a' in optdict,
                                         prune=True )
    
    if len(pruneL) > 0:
        print3()
        for pt,ct in pruneL:
            print3( '*** Warning: analyze test',
                    relative_execute_directory( pt, test_dir, os.getcwd() ),
                    'will not be run due to dependency',
                    relative_execute_directory( ct, test_dir, os.getcwd() ) )
    if cntmax > 0:
        print3()
        print3( 'Note: there were', cntmax, 'tests that exceeded the',
                'maximum number of processors - they will not be run' )

    perms = make_PermissionSetter( test_dir, optdict )

    tlist.stringFileWrite( tfile )

    try:
        
        perms.set( os.path.abspath( tfile ) )
        
        if len(tlist.active) > 0:
          
          print3( "Running these tests:" )
          printResults( tlist, test_dir, perms, short=True )
          print3()
          
          if '--save-results' in optdict:
            saveResults( tlist, plat, test_dir, inprogress=True )
          
          if '--batch' not in optdict:
            tlist.createTestExecs( test_dir, plat, config, perms )
            executeTestList( tlist, test_dir, plat, perms, tfile )
          
          else:
            tlist.createTestExecs( test_dir, plat, config, perms )
            batchTestList( tlist, test_dir, plat, perms )
        
          if '--save-results' in optdict:
            saveResults( tlist, plat, test_dir )
        
        else:
          print3( "\n--------- no tests to run -----------\n" )
    
    except:
        tlist.writeFinished()
        raise
    tlist.writeFinished()


def baselineTests( test_dir, plat, rtconfig ):
    
    cwd = os.getcwd()
    
    path_filter = None
    if test_dir == None:
      test_dir = os.path.join( cwd, testResultsDir( optdict, plat.getName() ) )
    else:
      path_filter = os.getcwd()
    
    tfile = os.path.join( test_dir, testlist_name )

    tlist = TestList.TestList( rtconfig )
    tlist.readFile( tfile )
    
    filter_dir = None
    if path_filter != None:
      filter_dir = computeRelativePath( test_dir, path_filter )
    
    # if the keyword expression does not include a results keyword, then
    # add the 'diff' keyword so that only diffs are rebaselined by default
    if not keyword_expr.containsResultsKeywords():
      keyword_expr.append( ['diff'], 'and' )
    
    tlist.loadAndFilter( plat.getMaxProcs(),
                         filter_dir=filter_dir,
                         baseline=True )
    
    if len(tlist.active) > 0:
      
      print3( "Baselining these tests:" )
      printResults( tlist, test_dir, None, short=False )

      perms = make_PermissionSetter( test_dir, optdict )
      
      tlist.createTestExecs( test_dir, plat, config, perms )
      
      failures = 0
      for tx in tlist.getTestExecList():
        if isinstance(tx, TestExec.TestExec): ref = tx.atest
        else:                         ref = tx
        sys.stdout.write("baselining " + ref.getExecuteDirectory() + "...")
        sys.stdout.flush()
        tx.start( baseline=1 )
        for i in range(30):
          time.sleep(1)
          if tx.poll():
            if tx.atest.getAttr('result') == "pass":
              sys.stdout.write("done\n")
              sys.stdout.flush()
            else:
              failures = 1
              sys.stdout.write("FAILED\n")
              sys.stdout.flush()
            break
        if not tx.isDone():
          tx.killJob()
          failures = 1
          sys.stdout.write("TIMED OUT\n")
          sys.stdout.flush()
      
      if failures:
        print3( "\n\n !!!!!!!!!!!!!!  THERE WERE FAILURES  !!!!!!!!!!!!! \n\n" )
    
    else:
      print3( "\n--------- no tests to baseline -----------\n" )


###########################################################################

def getUserName():
    """
    Retrieves the user name associated with this process.
    """
    usr = None
    try:
        import getpass
        usr = getpass.getuser()
    except:
        usr = None
    
    if usr == None:
        try:
            uid = os.getuid()
            import pwd
            usr = pwd.getpwuid( uid )[0]
        except:
            usr = None
    
    if usr == None:
        try:
            p = os.path.expanduser( '~' )
            if p != '~':
                usr = os.path.basename( p )
        except:
            usr = None
    
    if usr == None:
        # try manually checking the environment
        for n in ['USER', 'LOGNAME', 'LNAME', 'USERNAME']:
            if os.environ.get(n,'').strip():
                usr = os.environ[n]
                break

    if usr == None:
        raise Exception( "could not determine this process's user name" )

    return usr


def computeRelativePath(d1, d2):
    """
    Compute relative path from directory d1 to directory d2.
    """
    
    assert os.path.isabs(d1)
    assert os.path.isabs(d2)
    
    d1 = os.path.normpath(d1)
    d2 = os.path.normpath(d2)
    
    list1 = d1.split( os.sep )
    list2 = d2.split( os.sep )
    
    while 1:
      try: list1.remove('')
      except: break
    while 1:
      try: list2.remove('')
      except: break
    
    i = 0
    while i < len(list1) and i < len(list2):
      if list1[i] != list2[i]:
        break
      i = i + 1
    
    p = []
    j = i
    while j < len(list1):
      p.append('..')
      j = j + 1
    
    j = i
    while j < len(list2):
      p.append(list2[j])
      j = j + 1
    
    if len(p) > 0:
      return os.path.normpath( os.sep.join(p) )
    
    return "."


def issubdir(parent_dir, subdir):
    """
    TODO: test for relative paths
    """
    lp = len(parent_dir)
    ls = len(subdir)
    if ls > lp and parent_dir + '/' == subdir[0:lp+1]:
      return subdir[lp+1:]
    return None


def print3( *args, **kwargs ):
    s = ' '.join( [ str(x) for x in args ] )
    if len(kwargs) > 0:
        s += ' ' + ' '.join( [ str(k)+'='+str(v) for k,v in kwargs.items() ] )
    sys.stdout.write( s + os.linesep )
    sys.stdout.flush()


###########################################################################
#
# main

def parseOptionsAndGenerateOptionDictionary():
    """
    Generates a dictionary of the options.  Multiple values for an option are
    appended to a list.  Checks are performed for the correct usage of the
    options.  Options that are expected to only have one value are entered
    as a single value, rather than a list with one element.
    """
    argL = []
    for s in sys.argv[1:]:
      if s in ['-h']:
        optdict['-h'] = ''
        return []
      elif s in ['-H','-help','--help']:
        optdict['-H'] = ''
        return []
      elif s =='-vg':
        argL.append( '--vg' )
      else:
        argL.append(s)
    
    singles = 'hHvgGk:K:p:P:S:o:O:wAj:FRM:n:N:ibT:Lmeas:x:X:C'
    doubles = [ 'version','plat=','platopt=',
                'keys', 'files', 'batch', 'pipeline',
                'qsub-id=', 'qsub-limit=', 'qsub-length=',
                'html', 'junit=', 'check=', 'analyze', 'cdir=',
                'search=', 'extract=', 'include-tdd',
                'timeout-multiplier=', 'max-timeout=', 'postclean',
                'vg', 'config=', 'perms=', 'sort=', 'force',
                'save-results', 'results-tag=','results-date=',
                'tmin=', 'tmax=', 'tsum=',
                'test-args=', 'run-dir=',
              ]
    import getopt
    try: opts, dirs = getopt.getopt( argL, singles, longopts=doubles )
    except getopt.error:
        e = sys.exc_info()[1]
        sys.stderr.write('*** ' + exeName + ': error: ' + str(e) + '\n')
        sys.exit(1)
    
    for optpair in opts:
        n = optpair[0]

        if   n == '--analyze': n = '-a'
        elif n == '--search' : n = '-s'
        elif n == '--postclean': n = '-C'
        elif n == '-F': n = '-R'  # -F is deprecated, use -R instead
        elif n == '--pipeline': n = '--batch'  # --pipeline is deprecated

        if n in ['--results-tag','--results-date']:
            optdict[n] = optpair[1]
        elif n == '--junit':
            optdict[n] = os.path.abspath( optpair[1] )
        elif n not in optdict:
            optdict[n] = [optpair[1]]
        else:
            optdict[n].append(optpair[1])
    
    if '-h' in optdict: return []
    if '-H' in optdict: return []
    
    if '-v' in optdict or '--version' in optdict:
      print3( version )
      sys.exit(0)
    
    if '-G' in optdict:
      # the -g and -G options changed into the same thing (now just -g)
      optdict['-g'] = None
    
    if '-g' in optdict and ( '--plat' in optdict or \
                             '-i' in optdict or \
                             '-b' in optdict or \
                             '-n' in optdict or \
                             '-m' in optdict or \
                             '-L' in optdict ):
      sys.stderr.write('*** ' + exeName + \
            ': error: the only options allowed with -g are -oOMfkKpPS\n')
      sys.exit(1)
    
    if '-i' in optdict and ( '-L' in optdict or \
                             '-g' in optdict or \
                             '-R' in optdict or \
                             '-w' in optdict or \
                             '-S' in optdict or \
                             '-b' in optdict ):
      sys.stderr.write('*** ' + exeName + \
            ': error: the options -fLgbRSF cannot be used with -i\n')
      sys.exit(1)
    
    if '-b' in optdict and ( '-A' in optdict or \
                             '-g' in optdict or \
                             '-w' in optdict or \
                             '-S' in optdict or \
                             '-M' in optdict ):
      sys.stderr.write('*** ' + exeName + \
            ': error: the options -glfMARS cannot be used with -b\n')
      sys.exit(1)
    
    # parse -k & -K options into two convenient objects
    
    keywL = []
    if '-k' in optdict:
      keywL.extend( optdict['-k'] )
    
    # change -K into -k expressions by using the '!' operator
    for s in optdict.get('-K',[]):
      bangL = map( lambda k: '!'+k, s.split('/') )
      keywL.append( '/'.join( bangL ) )
    
    if len(keywL) > 0:
      keyword_expr.append( keywL, 'and' )
    
    # check validity of -p & -P options and combine them
    try:
      pf = FilterExpressions.ParamFilter( optdict.get('-p',None) )
    except ValueError:
      e = sys.exc_info()[1]
      sys.stderr.write('*** ' + exeName + \
            ': error: in -p specification: ' + str(e) + '\n')
      sys.exit(1)
    try:
      pf = FilterExpressions.ParamFilter( optdict.get('-P',None) )
    except ValueError:
      e = sys.exc_info()[1]
      sys.stderr.write('*** ' + exeName + \
            ': error: in -P specification: ' + str(e) + '\n')
      sys.exit(1)
    
    if '-P' in optdict:
      # convert -P values into -p values
      for s in optdict['-P']:
        s = s.strip()
        if s:
          orL = []
          for k in s.split('/'):
            k = k.strip()
            if k:
              orL.append( '!' + k )
          if '-p' not in optdict:
            optdict['-p'] = []
          optdict['-p'].append( '/'.join( orL ) )
      del optdict['-P']
    
    if '-X' in optdict:
      for s in optdict['-X']:
        s = s.strip()
        if s:
          orL = []
          for p in s.split('/'):
            p = p.strip()
            if p:
              orL.append( '!' + p )
          if '-x' not in optdict:
            optdict['-x'] = []
          optdict['-x'].append( '/'.join( orL ) )
    
    if '-s' in optdict:
      for pat in optdict['-s']:
        try:
          c = re.compile(pat)
        except:
          sys.stderr.write( '*** ' + exeName + ': error: -s option ' + \
                            'specified an invalid regular expression: ' + pat )
          sys.exit(1)
    
    if '-S' in optdict:
      # -S should be a list of param=value strings, which we convert into
      # a dictionary of param name -> list of values
      sdict = {}
      for s in optdict['-S']:
        L = s.split( '=', 1 )
        if len(L) < 2 or not L[0] or not L[1]:
          sys.stderr.write('*** ' + exeName + \
              ': error: -S option values must be "param=value"\n')
          sys.exit(1)
        if L[0] in sdict: sdict[L[0]].extend( L[1].split() )
        else:             sdict[L[0]] = L[1].split()
      optdict['-S'] = sdict
    
    if '--sort' in optdict:
        val = ''.join( [ s.strip() for s in optdict['--sort'] ] )
        for c in val:
            if c not in 'nxtdsr':
                sys.stderr.write('*** ' + exeName + \
                    ': error: invalid --sort character: ' + c + '\n')
                sys.exit(1)
        optdict[ '--sort' ] = val

    if '--plat' in optdict:
      if len(optdict['--plat']) > 1:
        sys.stderr.write('*** ' + exeName + \
              ': error: only one --plat option allowed\n')
        sys.exit(1)
      optdict['--plat'] = optdict['--plat'][0]
    
    if '--platopt' in optdict:
      # create a dictionary storing option->value pairs
      popts = {}
      for po in optdict['--platopt']:
        tmp = po.split( '=', 1 )
        if len(tmp) == 1: popts[ po ] = ''
        else:             popts[ tmp[0] ] = tmp[1]
      # reset the platopt option with the dictionary
      optdict['--platopt'] = popts
    
    if '--qsub-id' in optdict:
      assert len( optdict['--qsub-id'] ) == 1
      optdict['--qsub-id'] = optdict['--qsub-id'][0]
    
    if '--qsub-limit' in optdict:
      if len(optdict['--qsub-limit']) > 1:
        sys.stderr.write('*** ' + exeName + \
              ': error: only one --qsub-limit option allowed\n')
        sys.exit(1)
      try: val = int(optdict['--qsub-limit'][0])
      except:
          sys.stderr.write('*** ' + exeName + \
            ': error: the --qsub-limit option must be followed by an integer\n')
          sys.exit(1)
      if val <= 0:
          sys.stderr.write('*** ' + exeName + \
            ': error: the --qsub-limit value must be a positive integer\n')
          sys.exit(1)
      optdict['--qsub-limit'] = val
    
    if '--qsub-length' in optdict:
      if len(optdict['--qsub-length']) > 1:
        sys.stderr.write('*** ' + exeName + \
              ': error: only one --qsub-length option allowed\n')
        sys.exit(1)
      try: val = int(optdict['--qsub-length'][0])
      except:
          sys.stderr.write('*** ' + exeName + \
            ': error: the --qsub-length option must be followed by an integer\n')
          sys.exit(1)
      if val < 0:
          sys.stderr.write('*** ' + exeName + \
            ': error: the --qsub-length value cannot be negative\n')
          sys.exit(1)
      optdict['--qsub-length'] = val
    
    # clean up option strings
    
    if '-o' in optdict:
      on = {}
      for o1 in optdict.get('-o',[]):
        for o2 in o1.split( '+' ):
          for o3 in o2.split():
            on[o3] = ''
      on = list( on.keys() )
      on.sort()
      optdict['-o'] = on
      config.set( 'onopts', on )
    
    if '-O' in optdict:
      off = {}
      for o1 in optdict.get('-O'):
        for o2 in o1.split( '+' ):
          for o3 in o2.split():
            off[o3] = ''
      off = list( off.keys() )
      off.sort()
      optdict['-O'] = off
      config.set( 'offopts', off )
    
    if '-j' in optdict and len(optdict['-j']) > 1:
      sys.stderr.write('*** ' + exeName + \
            ': error: only one -j option allowed\n')
      sys.exit(1)
    
    if '-M' in optdict and len(optdict['-M']) > 1:
      sys.stderr.write('*** ' + exeName + \
            ': error: only one -M option allowed\n')
      sys.exit(1)

    if '--run-dir' in optdict:
        d = optdict['--run-dir'][0]
        if d != os.path.basename(d):
            sys.stderr.write('*** ' + exeName + \
                ': error: --run-dir value can only be a single path segment\n')
            sys.exit(1)
        optdict['--run-dir'] = d

    if '-n' in optdict:
      if len(optdict['-n']) > 1:
          sys.stderr.write('*** ' + exeName + \
                ': error: only one -n option is allowed\n')
          sys.exit(1)
      try: val = int(optdict['-n'][0])
      except:
          sys.stderr.write('*** ' + exeName + \
                ': error: the -n option must be followd by an integer\n')
          sys.exit(1)
      if val <= 0:
        sys.stderr.write('*** ' + exeName + \
              ': error: the -n option must specify a positive integer\n')
        sys.exit(1);
     
    if '-N' in optdict:
      if len(optdict['-N']) > 1:
          sys.stderr.write('*** ' + exeName + \
                ': error: only one -N option is allowed\n')
          sys.exit(1)
      try: val = int(optdict['-N'][0])
      except:
          sys.stderr.write('*** ' + exeName + \
                ': error: the -N option must be followd by an integer\n')
          sys.exit(1)
      if val <= 0:
        sys.stderr.write('*** ' + exeName + \
              ': error: the -N option must specify a positive integer\n')
        sys.exit(1);
     
    if '-T' in optdict:
      if len(optdict['-T']) > 1:
          sys.stderr.write('*** ' + exeName + \
                ': error: only one -T option is allowed\n')
          sys.exit(1)
      try: val = float(optdict['-T'][0])
      except:
          sys.stderr.write('*** ' + exeName + \
                ': error: the -T option must be followd by a number\n')
          sys.exit(1);
      if val < 0.0: optdict['-T'] = 0.0
      else:         optdict['-T'] = val
    
    if '--timeout-multiplier' in optdict:
      if len(optdict['--timeout-multiplier']) > 1:
          sys.stderr.write('*** ' + exeName + \
                ': error: only one --timeout-multiplier option is allowed\n')
          sys.exit(1)
      try: val = float(optdict['--timeout-multiplier'][0])
      except:
          sys.stderr.write('*** ' + exeName + \
               ': error: the --timeout-multiplier option must be ' + \
               'followd by a number\n')
          sys.exit(1)
      if val <= 0.0:
        sys.stderr.write('*** ' + exeName + \
             ': error: the --timeout-multiplier option must be ' + \
             'followd by a positive number\n')
        sys.exit(1)
      optdict['--timeout-multiplier'] = val

    if '--max-timeout' in optdict:
        try:
            val = float( optdict['--max-timeout'][-1] )
            assert val > 0
        except:
            sys.stderr.write('*** ' + exeName + \
               ': error: the --max-timeout option must be a positive number\n' )
            sys.exit(1)
        optdict['--max-timeout'] = val

    if '--tmin' in optdict:
        try:
            mn = float( optdict['--tmin'][-1] )
        except:
            sys.stderr.write('*** ' + exeName + \
                ': error: the --tmin option must be a number\n' )
            sys.exit(1)
        optdict['--tmin'] = mn
    if '--tmax' in optdict:
        try:
            mx = float( optdict['--tmax'][-1] )
        except:
            sys.stderr.write('*** ' + exeName + \
                ': error: the --tmax option must be a number\n' )
            sys.exit(1)
        optdict['--tmax'] = mx
    if '--tsum' in optdict:
        try:
            sm = float( optdict['--tsum'][-1] )
        except:
            sys.stderr.write('*** ' + exeName + \
                ': error: the --tsum option must be a number\n' )
            sys.exit(1)
        optdict['--tsum'] = sm
     
    # collapse the list of length one for single argument options
    
    if '-j' in optdict:
      d = optdict['-j'][0]
      if not os.path.exists(d):
        sys.stderr.write('*** ' + exeName + ': error: project directory ' + \
                         'does not exist: ' + d + os.linesep )
        sys.exit(1)
      d = os.path.abspath(d)
      optdict['-j'] = d
      config.set( 'exepath', d )
    
    if '-M' in optdict:
      optdict['-M'] = optdict['-M'][0]
    
    if '-n' in optdict:
      optdict['-n'] = int( optdict['-n'][0] )
    if '-N' in optdict:
      optdict['-N'] = int( optdict['-N'][0] )
    
    if '--extract' in optdict:
      optdict['--extract'] = optdict['--extract'][-1]

    if '--config' in optdict:
        config.set( 'configdir', os.path.abspath( optdict['--config'][-1] ) )
    else:
        d = os.getenv( 'VVTEST_CONFIGDIR' )
        if d == None:
            d = os.path.join( config.get('toolsdir'), 'config' )
        config.set( 'configdir', os.path.abspath(d) )

    config.set( 'refresh', '-m' not in optdict )
    config.set( 'postclean', '-C' in optdict )
    if '-T' in optdict:
      config.set( 'timeout', optdict['-T'] )
    if '--timeout-multiplier' in optdict:
      config.set( 'multiplier', optdict['--timeout-multiplier'] )
    config.set( 'preclean', '-m' not in optdict )
    config.set( 'analyze', '-a' in optdict )
    config.set( 'logfile', '-L' not in optdict )

    if '--test-args' in optdict:
        argL = []
        for args in optdict.get( '--test-args', [] ):
            argL.extend( shlex.split( args ) )
        config.set( 'testargs', argL )

    return dirs


def getExeDirAndName():
    d = sys.path[0]
    if not d:                  d = os.getcwd()
    elif not os.path.isabs(d): d = os.path.abspath(d)
    n = os.path.basename( sys.argv[0] )
    return d, n


def check_for_bootstrap_file():
    """
    if vvtest_bootstrap.py exists in the same directory as vvtest,
    then import it (which may set os.environ variables)
    """
    try:
        import vvtest_bootstrap

    except ImportError:
        # to allow for vvtest to be a soft link to an installed vvtest area,
        # look for a bootstrap file in the directory containing the soft link
        bindir = os.path.dirname( os.path.abspath( sys.argv[0] ) )
        boot = os.path.join( bindir, 'vvtest_bootstrap.py' )
        if os.path.exists( boot ):
            sys.path.append( bindir )
            import vvtest_bootstrap


def insert_configdir_into_sys_path( toolsdir, config ):
    ""
    d1 = os.path.normpath( os.path.join( toolsdir, 'config' ) )

    d2 = config.get('configdir')
    if d2:
        d2 = os.path.normpath( d2 )

        if d1 != d2:
            sys.path.insert( 1, d1 )

        sys.path.insert( 1, d2 )

    else:
        sys.path.insert( 1, d1 )


def make_PermissionSetter( test_dir, optdict ):
    ""
    if '--perms' in optdict:
        perms = PermissionSetter( test_dir, optdict['--perms'] )
    else:
        perms = DummyPermissionSetter()

    return perms


##############################################################################


if __name__ == '__main__':
  
  # get directory containing this script and the script name itself
  toolsdir, exeName = getExeDirAndName()
  
  import libvvtest.vvplatform as vvplatform
  import libvvtest.misc as misc
  import libvvtest.TestSpec as TestSpec
  import libvvtest.TestExec as TestExec
  import libvvtest.TestList as TestList
  import libvvtest.batchutils as batchutils
  import libvvtest.TestSpecCreator as TestSpecCreator
  import libvvtest.FilterExpressions as FilterExpressions
  from libvvtest.RuntimeConfig import RuntimeConfig
  import results
  from libvvtest.PermissionSetter import DummyPermissionSetter
  from libvvtest.PermissionSetter import PermissionSetter

  keyword_expr = FilterExpressions.WordExpression()

  check_for_bootstrap_file()

  # set this before options are parsed
  config.set( 'toolsdir', toolsdir )
  
  # this creates the global "optdict" dictionary and returns non-option args
  dirs = parseOptionsAndGenerateOptionDictionary()
  
  # do this after options are parsed
  insert_configdir_into_sys_path( toolsdir, config )
  
  # the help option quits immediately
  if '-h' in optdict:
    print3( usagepage )
    sys.exit(0)
  if '-H' in optdict:
    print3( manpage )
    sys.exit(0)
  
  # 'test_dir' non-None only if the CWD is in a TestResults.* directory
  test_dir = readCommandInfo()
  
  if '--check' in optdict:
    for n in optdict['--check']:
      os.environ[ 'CHECK_' + n.upper() ] = ''
  
  if '--vg' in optdict:
    os.environ[ 'CHECK_VALGRIND' ] = ''
  
  if test_dir and '-S' in optdict:
    print3( "*** error: cannot use -S in a TestResults directory" )
    sys.exit(1)
  
  if test_dir and '-g' in optdict:
    print3( "*** error: cannot use -g in a TestResults directory" )
    sys.exit(1)
  
  # construct platform
  plat = vvplatform.construct_Platform( toolsdir, optdict )

  results_dir = testResultsDir( optdict, plat.getName() )
  
  rtconfig = RuntimeConfig( \
                param_expr_list=optdict.get('-p', None),
                keyword_expr=keyword_expr,
                option_list=optdict.get('-o', []) + [plat.getCompiler()],
                platform_name=plat.getName(),
                ignore_platforms='-A' in optdict,
                platform_expr_list=optdict.get('-x', None),
                search_file_globs=search_fnmatch,
                search_patterns=optdict.get('-s',None),
                include_tdd='--include-tdd' in optdict,
                runtime_range=[optdict.get('--tmin',None),
                               optdict.get('--tmax',None)],
                runtime_sum=optdict.get( '--tsum', None ) )
  if '--qsub-id' in optdict:
    rtconfig.setAttr( 'include_all', True )
  
  info = ( '-i' in optdict or \
           '--files' in optdict or \
           '--keys' in optdict )
  
  if '-g' in optdict and not info:
    assert test_dir == None
    generateTestList( dirs, plat, rtconfig )
  
  elif info:
    
    tlist = TestList.TestList( rtconfig )
    if test_dir != None:
      tfile = os.path.join( test_dir, testlist_name )
      tlist.readFile( tfile )
      tlist.loadAndFilter( plat.getMaxProcs(),
                    filter_dir=computeRelativePath( test_dir, os.getcwd() ) )
    elif os.path.exists( results_dir ):
      test_dir = os.path.join( os.getcwd(), results_dir )
      tfile = os.path.join( test_dir, testlist_name )
      rtconfig.setAttr( 'include_tdd', True )  # always include tdd with -i
      tlist.readFile( tfile )
      tlist.loadAndFilter( plat.getMaxProcs() )
    elif '--keys' in optdict or '--files' in optdict:
      if len(dirs) == 0:
        dirs = ['.']
      for d in dirs:
        if os.path.exists(d):
          tlist.scanDirectory( d, optdict.get('-S',None) )
      tlist.loadAndFilter( plat.getMaxProcs(), prune=True )
    else:
      print3( "*** warning: previous TestResults directory not found" )
      test_dir = os.path.join( os.getcwd(), results_dir )
      tlist.loadAndFilter( plat.getMaxProcs(), prune=True )
    
    if '--keys' in optdict or '--files' in optdict:
        keywordInformation(tlist)
    elif '--save-results' in optdict:
        saveResults( tlist, plat, test_dir )
    else:
        do_stdout = True
        do_html = ( '--html' in optdict )
        do_junit = optdict.get('--junit','')
        if do_html or do_junit:
            do_stdout = False
        printResults( tlist, test_dir, None,
                      tostdout=do_stdout, tohtml=do_html, tojunit=do_junit )
 
  elif '-b' in optdict:
    
    if '-R' in optdict or '-w' in optdict:
      print3( "*** error: cannot use -R or -w with -b (baseline)" )
      sys.exit(1)
    
    baselineTests( test_dir, plat, rtconfig )
  
  elif '--extract' in optdict:
    extractTestFiles( dirs, plat, optdict['--extract'], rtconfig )
  
  else:
    
    # if no results keywords are specified, then add -k notrun/notdone
    if not keyword_expr.containsResultsKeywords() and \
       '-w' not in optdict and '-R' not in optdict:
      keyword_expr.append( ['notrun/notdone'], 'and' )
    
    if test_dir != None:
      restartTests( test_dir, plat, rtconfig )
    else:
      runTests( dirs, plat, rtconfig )
