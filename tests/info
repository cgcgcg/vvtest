#!/usr/bin/env python

# Copyright 2018 National Technology & Engineering Solutions of Sandia, LLC
# (NTESS). Under the terms of Contract DE-NA0003525 with NTESS, the U.S.
# Government retains certain rights in this software.

#RUNTEST: vvtest

import sys
sys.dont_write_bytecode = True
sys.excepthook = sys.__excepthook__
import os
from os.path import basename, abspath
import time

import vvtestutils as vtu
import testutils as util
from testutils import print3

from libvvtest.filtering import TestFilter
from libvvtest.FilterExpressions import WordExpression


class info_tests( vtu.vvtestTestCase ):

    def test_obtaining_keywords_and_test_files(self):
        ""
        util.writefile( "param.xml", """
            <rtest name="param">
              <parameterize hello="world mars"/>
              <execute>
                sleep 1
                echo "parm=$hello"
              </execute>
            </rtest>""" )
        util.writefile( "FailTest.xml", """
            <rtest name="FailTest">
              <execute>
                sleep 1
                echo "Exiting with failure status"
                exit 1
              </execute>
            </rtest>""")
        util.writefile( "DiffTest.xml", """
            <rtest name="DiffTest">
              <execute>
                sleep 1
                echo "Exiting with diff status"
                set have_diff = yes
              </execute>
            </rtest>""")
        time.sleep(1)

        vrun = vtu.runvvtest()
        vrun.assertCounts( total=4, npass=2, diff=1, fail=1 )
        tdir = vrun.resultsDir()

        vrun = vtu.runvvtest( '-i -v' )
        vrun.assertCounts( total=4, npass=2, diff=1, fail=1 )

        # cd into the run directory and check the -i output
        vrun = vtu.runvvtest( '-i -v', chdir=tdir )
        vrun.assertCounts( total=4, npass=2, diff=1, fail=1 )

        vtu.remove_results()

        # again but with a build option

        vrun = vtu.runvvtest( '-o dbg' )
        vrun.assertCounts( total=4, npass=2, diff=1, fail=1 )
        tdir = vrun.resultsDir()

        vrun = vtu.runvvtest( '-i -v', chdir=tdir )
        vrun.assertCounts( total=4, npass=2, diff=1, fail=1 )

        vtu.remove_results()

        util.writefile( 'keys.xml', """
              <rtest name="keys">
                <keywords> hello world </keywords>
              </rtest>
            """ )
        util.writefile( 'sdir/skeys.xml', """
              <rtest name="skeys">
                <keywords> mars jupiter fast </keywords>
              </rtest>
            """ )
        time.sleep(1)

        vrun = vtu.runvvtest( '--keys' )
        assert_keywords( vrun, ['fast', 'hello', 'jupiter', 'mars', 'world'] )

        vrun = vtu.runvvtest( '--files' )
        assert_files( vrun, [ 'DiffTest.xml', 'FailTest.xml', 'keys.xml',
                              'param.xml', 'sdir/skeys.xml' ] )

        vrun = vtu.runvvtest( '-k fast --files' )
        assert_files( vrun, [ 'sdir/skeys.xml'] )

        vrun = vtu.runvvtest( '-K fast -K medium -K long --files' )
        assert_files( vrun, [ 'DiffTest.xml', 'FailTest.xml',
                              'keys.xml', 'param.xml'] )

        vrun = vtu.runvvtest()
        tdir = vrun.resultsDir()

        vrun = vtu.runvvtest( '-R --keys' )
        assert_keywords( vrun, ['fast', 'hello', 'jupiter', 'mars', 'world'] )

        vrun = vtu.runvvtest( '-i --keys' )
        assert_keywords( vrun, ['fast', 'hello', 'jupiter', 'mars', 'world'] )

        vrun = vtu.runvvtest( '-i --files' )
        assert_files( vrun, [ 'DiffTest.xml', 'FailTest.xml', 'keys.xml',
                              'param.xml', 'sdir/skeys.xml'] )

        vrun = vtu.runvvtest( '-R --keys', chdir=tdir )
        assert_keywords( vrun, ['fast', 'hello', 'jupiter', 'mars', 'world'] )

        vrun = vtu.runvvtest( '-i --keys', chdir=tdir )
        assert_keywords( vrun, ['fast', 'hello', 'jupiter', 'mars', 'world'] )

        vrun = vtu.runvvtest( '-i --files', chdir=tdir )
        assert_files( vrun, [ 'DiffTest.xml', 'FailTest.xml', 'keys.xml',
                              'param.xml', 'sdir/skeys.xml'] )

        vtu.runvvtest( '-i -v', chdir=tdir ).assertCounts( total=6 )
        vrun = vtu.runvvtest( '-i -v', chdir=tdir+'/sdir' )
        vrun.assertCounts( total=1 )
        assert vrun.countTestLines( ' skeys' ) == 1

        vrun = vtu.runvvtest( '-R --keys', chdir=tdir+'/sdir' )
        assert_keywords( vrun, ['fast', 'jupiter', 'mars'] )

        vrun = vtu.runvvtest( '-i --keys', chdir=tdir+'/sdir' )
        assert_keywords( vrun, ['fast', 'jupiter', 'mars'] )

        vrun = vtu.runvvtest( '-i --files', chdir=tdir+'/sdir' )
        assert_files( vrun, [ 'sdir/skeys.xml'] )

    def test_command_line_keyword_dump_does_not_include_parameters(self):
        ""
        util.writefile( 'keys.vvt', """
              #VVT: keywords = jupiter mars
              #VVT: parameterize : np = 1 4
              pass
            """ )
        time.sleep(1)

        vrun = vtu.runvvtest( '--keys' )
        assert_keywords( vrun, ['jupiter', 'mars'] )

    def test_sorting_the_test_listing(self):
        ""
        util.writefile( "one/zebra.xml", """
            <rtest name="zebra">
              <execute>
                sleep 1
                exit 1
              </execute>
            </rtest>""" )
        util.writefile( "one/marmot.xml", """
            <rtest name="marmot">
              <execute>
                sleep 3
                set have_diff=yes
              </execute>
            </rtest>""" )
        util.writefile( "two/ardvark.xml", """
            <rtest name="ardvark">
              <execute>
                sleep 5
              </execute>
            </rtest>""" )
        util.writefile( "two/otter.xml", """
            <rtest name="otter">
              <execute>
                sleep 20
              </execute>
            </rtest>""" )
        time.sleep(1)

        vrun = vtu.runvvtest( '-T 10 -n 1' )
        vrun.assertCounts( total=4, npass=1, diff=1, fail=1, timeout=1 )

        finL = vrun.grepLines( 'Finished: ' )
        assert len(finL) == 4

        tdir = vrun.resultsDir()

        vrun = vtu.runvvtest( '-i -v', chdir=tdir )
        assert ordered_testids( vrun ) == ['marmot','zebra','ardvark','otter']

        vrun = vtu.runvvtest( '-i -v --sort nx', chdir=tdir )
        assert ordered_testids( vrun ) == ['ardvark','marmot','otter','zebra']

        vrun = vtu.runvvtest( '-i -v --sort x', chdir=tdir )
        assert ordered_testids( vrun ) == ['marmot','zebra','ardvark','otter']

        vrun = vtu.runvvtest( '-i -v --sort t', chdir=tdir )
        assert ordered_testids( vrun ) == ['zebra','marmot','ardvark','otter']

        vrun = vtu.runvvtest( '-i -v --sort sr', chdir=tdir )
        assert ordered_testids( vrun ) == ['otter','ardvark','zebra','marmot']

        vrun = vtu.runvvtest( '-i -v --sort d', chdir=tdir )
        assert ordered_testids( vrun ) == self.sorted_by_date_executed( vrun )

    def sorted_by_date_executed(self, vrun):
        ""
        dateL = [ [ vrun.startDate('one/zebra'),   'zebra'   ],
                  [ vrun.startDate('one/marmot'),  'marmot'  ],
                  [ vrun.startDate('two/ardvark'), 'ardvark' ],
                  [ vrun.startDate('two/otter'),   'otter' ] ]
        dateL.sort()
        return [ L[1] for L in dateL ]

    def test_sorting_with_near_duplicate_tests(self):
        ""
        util.writefile( 'dupname.vvt', """
            #VVT: parameterize : np = 1 4
            pass
            """ )
        time.sleep(1)

        vrun = vtu.runvvtest()
        vrun.assertCounts( total=2, npass=2 )
        tdir = vrun.resultsDir()

        vrun = vtu.runvvtest( '-i -v --sort n', chdir=tdir )
        assert ordered_testids( vrun ) == ['dupname', 'dupname']

    def test_info_mode_should_not_apply_default_max_procs_filter(self):
        ""
        util.writefile( 'config/platform_plugin.py', """
            def initialize( plat ):
                plat.setattr( 'maxprocs', 4 )
            """ )
        util.writefile( 'procs.vvt', """
            #VVT: parameterize : np = 1 8
            """ )
        time.sleep(1)

        vrun = vtu.runvvtest( "-N 8 --config config" )
        vrun.assertCounts( total=2, npass=2 )
        tdir = vrun.resultsDir()

        vrun = vtu.runvvtest( "-i -v -N 4 --config config" )
        vrun.assertCounts( total=1, npass=1 )

        vrun = vtu.runvvtest( "-i -v --config config" )
        vrun.assertCounts( total=2, npass=2 )

        vrun = vtu.runvvtest( "-i -v --config", abspath('config'), chdir=tdir )
        vrun.assertCounts( total=2, npass=2 )

    def test_info_mode_should_not_apply_default_platform_filter(self):
        ""
        util.writefile( 'config1/idplatform.py', """
            def platform( opts ):
                return 'XBox'
            """ )
        util.writefile( 'config2/idplatform.py', """
            def platform( opts ):
                return 'PlayStation'
            """ )
        util.writefile( 'xbox.vvt', """
            #VVT: enable (platforms="XBox")
            """ )
        util.writefile( 'ps.vvt', """
            #VVT: enable (platforms="PlayStation")
            """ )
        util.writefile( 'both.vvt', """
            #VVT: enable (platforms="XBox or PlayStation")
            """ )
        time.sleep(1)

        vrun = vtu.runvvtest( "--config config1", addplatform=False )
        vrun.assertCounts( total=2, npass=2 )
        assert vrun.getTestIds() == ['both','xbox']
        tdir = vrun.resultsDir()

        vrun = vtu.runvvtest( "-i -v --config config1", addplatform=False )
        vrun.assertCounts( total=2, npass=2 )
        assert vrun.getTestIds() == ['both','xbox']

        vrun = vtu.runvvtest( "-i -v --config", abspath('config1'),
                              chdir=tdir, addplatform=False )
        vrun.assertCounts( total=2, npass=2 )
        assert vrun.getTestIds() == ['both','xbox']

        # a new default platform means a new TestResults directory
        vrun = vtu.runvvtest( "-i -v --config config2", addplatform=False )
        vrun.assertCounts( total=0 )

        # a new default platform has no effect with -i
        vrun = vtu.runvvtest( "-i -v --config", abspath('config2'),
                              chdir=tdir, addplatform=False )
        vrun.assertCounts( total=2, npass=2 )
        assert vrun.getTestIds() == ['both','xbox']

        # a new specified platform means a new TestResults directory
        vrun = vtu.runvvtest( "-i -v --plat PlayStation --config config1",
                              addplatform=False )
        vrun.assertCounts( total=0 )

        # a new specified platform has no effect with -i in restart mode
        vrun = vtu.runvvtest( "-i -v --plat PlayStation --config",
                              abspath('config1'), chdir=tdir, addplatform=False )
        vrun.assertCounts( total=2, npass=2 )
        assert vrun.getTestIds() == ['both','xbox']

        # same thing but different default platform
        vrun = vtu.runvvtest( "-i -v --plat PlayStation --config",
                              abspath('config2'), chdir=tdir, addplatform=False )
        vrun.assertCounts( total=2, npass=2 )
        assert vrun.getTestIds() == ['both','xbox']

        vtu.remove_results()

        # run with non-default platform
        vrun = vtu.runvvtest( "--plat PlayStation --config config1",
                              addplatform=False )
        vrun.assertCounts( total=2, npass=2 )
        assert vrun.getTestIds() == ['both','ps']
        tdir = vrun.resultsDir()

        # leaving off --plat PlayStation in restart mode should not matter
        vrun = vtu.runvvtest( "-i -v --config", abspath('config1'),
                              chdir=tdir, addplatform=False )
        vrun.assertCounts( total=2, npass=2 )
        assert vrun.getTestIds() == ['both','ps']

    def test_using_the_files_option_with_a_multi_test_xml_file(self):
        ""
        util.writefile( "multi.xml", """
            <rtest name="zebra">
              <rtest name="horse"/>
              <keywords testname="zebra"> stripes </keywords>
              <keywords testname="horse"> patchwork </keywords>
              <execute> echo "hello world" </execute>
            </rtest>""" )
        time.sleep(1)

        vrun = vtu.runvvtest( '--files' )
        assert_files( vrun, [ 'multi.xml'] )

        vrun = vtu.runvvtest( '--files -k stripes' )
        assert_files( vrun, [ 'multi.xml'] )

        vrun = vtu.runvvtest( '--files -k patchwork' )
        assert_files( vrun, [ 'multi.xml'] )

        vrun = vtu.runvvtest( '--files -k stripes -k patchwork' )
        assert_files( vrun, [] )

    def test_using_the_files_option_with_a_multi_test_script_file(self):
        ""
        util.writescript( "multi.vvt", """
            #!/bin/sh
            #VVT: name = zebra
            #VVT: name = horse
            #VVT: keywords (testname=zebra) : stripes
            #VVT: keywords (testname=horse) : patchwork
            echo "hello world"
            """ )
        time.sleep(1)

        vrun = vtu.runvvtest( '--files' )
        assert_files( vrun, [ 'multi.vvt'] )

        vrun = vtu.runvvtest( '--files -k stripes' )
        assert_files( vrun, [ 'multi.vvt'] )

        vrun = vtu.runvvtest( '--files -k patchwork' )
        assert_files( vrun, [ 'multi.vvt'] )

        vrun = vtu.runvvtest( '--files -k stripes -k patchwork' )
        assert_files( vrun, [] )

    def test_using_files_option_should_apply_max_processors_filtering(self):
        ""
        maxprocs = 4

        util.writescript( 'atest.vvt', """
            #!"""+sys.executable+"""
            #VVT: parameterize : np=1
            import vvtest_util as vvt
            print ( 'executing test, np='+str(vvt.np) )
            """ )
        util.writescript( 'btest.vvt', """
            #!"""+sys.executable+"""
            #VVT: parameterize : np="""+str(maxprocs)+"""
            import vvtest_util as vvt
            print ( 'executing test, np='+str(vvt.np) )
            """ )
        util.writescript( 'ctest.vvt', """
            #!"""+sys.executable+"""
            #VVT: parameterize : np="""+str(maxprocs+1)+"""
            import vvtest_util as vvt
            print ( 'executing test, np='+str(vvt.np) )
            """ )
        util.writescript( 'dtest.vvt', """
            #!"""+sys.executable+"""
            #VVT: parameterize : np="""+str(maxprocs+2)+"""
            import vvtest_util as vvt
            print ( 'executing test, np='+str(vvt.np) )
            """ )

        vrun = vtu.runvvtest( '-N '+str(maxprocs)+' --files' )
        assert_files( vrun, [ 'atest.vvt', 'btest.vvt' ] )

    def test_that_complete_info_from_previous_runs_is_shown(self):
        ""
        util.writefile( 'foo.vvt', """
            pass
            """ )
        util.writefile( 'bar.vvt', """
            pass
            """ )
        time.sleep(1)

        vrun = vtu.runvvtest()
        vrun.assertCounts( total=2, npass=2 )
        assert vrun.getTestIds() == [ 'bar', 'foo' ]
        rdir = vrun.resultsDir()

        vrun = vtu.runvvtest( '-R -k foo' )
        vrun.assertCounts( total=1, npass=1 )
        assert vrun.getTestIds() == [ 'foo' ]

        vrun = vtu.runvvtest( '-i -v' )
        vrun.assertCounts( total=2, npass=2 )
        assert vrun.getTestIds() == [ 'bar', 'foo' ]

        vrun = vtu.runvvtest( '-i -v -k bar' )
        vrun.assertCounts( total=1, npass=1 )
        assert vrun.getTestIds() == [ 'bar' ]

        vrun = vtu.runvvtest( '-i -v', chdir=rdir )
        vrun.assertCounts( total=2, npass=2 )
        assert vrun.getTestIds() == [ 'bar', 'foo' ]

        vrun = vtu.runvvtest( '-i -v -k bar', chdir=rdir )
        vrun.assertCounts( total=1, npass=1 )
        assert vrun.getTestIds() == [ 'bar' ]

    def test_that_filtering_records_skips(self):
        ""
        tcase = vtu.make_fake_TestCase()

        tf = TestFilter( None, None )
        tf.checkEnabled( tcase )
        assert len( tf.getSkipped() ) == 0
        tcase.getSpec().setEnabled( False ) #magic
        tf.checkEnabled( tcase )
        tL = list( tf.getSkipped() )
        assert len(tL) == 1 and id( tL[0] ) == id( tcase )

        tf = TestFilter( None, None )
        xdir = tcase.getSpec().getExecuteDirectory()
        tf.checkSubdirectory( tcase, xdir )
        assert len( list( tf.getSkipped() ) ) == 0
        tf.checkSubdirectory( tcase, 'foobar.np=4' )
        tL = list( tf.getSkipped() )
        assert len(tL) == 1 and id( tL[0] ) == id( tcase )

        tf = TestFilter( vtu.make_RuntimeConfig( 'atari', [] ), None )
        tf.checkPlatform( tcase )
        assert len( list( tf.getSkipped() ) ) == 0
        tcase.getSpec().addEnablePlatformExpression( WordExpression( 'XBox' ) )
        tf.checkPlatform( tcase )
        tL = list( tf.getSkipped() )
        assert len(tL) == 1 and id( tL[0] ) == id( tcase )

        rtconfig = vtu.make_RuntimeConfig( 'atari', [] )
        tf = TestFilter( rtconfig, None )
        tf.checkKeywords( tcase )
        assert len( list( tf.getSkipped() ) ) == 0
        rtconfig.setKeywordExpression( WordExpression( 'not key1' ) )
        tf.checkKeywords( tcase )
        tL = list( tf.getSkipped() )
        assert len(tL) == 1 and id( tL[0] ) == id( tcase )

    def test_that_command_line_info_ignores_its_own_filtering(self):
        """
        When using "vvtest -i -K pass", for example, the skips shown in the
        summary output should NOT include the tests filtered out by "-K pass".
        """
        util.writefile( 'blue.vvt', """
            #VVT: keywords = velvet underground
            """ )
        util.writefile( 'subdir/pink.vvt', """
            #VVT: keywords = velvet uptown
            import sys, vvtest_util
            sys.exit( vvtest_util.diff_exit_status )
            """ )
        util.writefile( 'gold.vvt', """
            #VVT: keywords = nugget underground
            """ )
        time.sleep(1)

        vrun = vtu.runvvtest( '-k velvet' )
        vrun.assertCounts( total=2, npass=1, diff=1 )
        tdir = vrun.resultsDir()

        vrun = vtu.runvvtest( '-i -vv' )
        vrun.assertCounts( total=3, npass=1, diff=1, skip=1 )

        vrun = vtu.runvvtest( '-i -K pass -vv' )
        vrun.assertCounts( total=2, diff=1, skip=1 )
        vrun.getTestIds() == ['gold','pink']

        vrun = vtu.runvvtest( '-i -k underground -vv', chdir=tdir )
        vrun.assertCounts( total=2, npass=1, skip=1 )
        vrun.getTestIds() == ['blue','gold']

        vrun = vtu.runvvtest( '-i -vv', chdir=tdir+'/subdir' )
        vrun.assertCounts( total=1, diff=1 )
        vrun.getTestIds() == ['pink']


############################################################################

def assert_keywords( vrun, keywordlist ):
    ""
    kL = extract_keywords( vrun.out )
    assert kL == keywordlist


def assert_files( vrun, relative_filenames ):
    ""
    fL = extract_files( vrun.out, os.getcwd() )
    assert fL == relative_filenames


def ordered_testids( vrun ):
    ""
    nameL = []
    for line in vtu.extract_testlines( vrun.out ):
        xdir = line.split()[-1]
        bname = basename( xdir )
        nameL.append( bname.split('.')[0] )
    return nameL


def extract_keywords( out ):
    ""
    start = False
    kL = []
    for line in out.split( '\n' ):
        if start:
            kL.extend( line.split() )
        elif line.strip()[:14] == 'test keywords:':
            start = True

    D = {}
    for k in kL:
        D[k] = None

    L = list( D.keys() )
    L.sort()

    return L


def extract_files( out, rootd ):
    ""
    D = {}
    for line in out.split( '\n' ):
        line = line.strip()
        if line and line[0] == '/':
            D[line] = None
    fL = list( D.keys() )

    lend = len(rootd)
    for i in range(len(fL)):
        fL[i] = fL[i][lend+1:]

    fL.sort()

    return fL


############################################################################

util.run_test_cases( sys.argv, sys.modules[__name__] )
